{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Improving Model Quality With TensorFlow Model Analysis","text":""},{"location":"#introduction","title":"Introduction","text":"<p>As you tweak your model during development, you need to check whether your changes are improving your model. Just checking accuracy may not be enough. For example, if you have a classifier for a problem in which 95% of your instances are positive, you may be able to improve accuracy by simply always predicting positive, but you won't have a very robust classifier.</p>"},{"location":"#overview","title":"Overview","text":"<p>The goal of TensorFlow Model Analysis is to provide a mechanism for model evaluation in TFX. TensorFlow Model Analysis allows you to perform model evaluations in the TFX pipeline, and view resultant metrics and plots in a Jupyter notebook. Specifically, it can provide:</p> <ul> <li>Metrics computed on entire training and holdout     dataset, as well as next-day evaluations</li> <li>Tracking metrics over time</li> <li>Model quality performance on different feature slices</li> <li>Model validation for ensuring that     model's maintain consistent performance</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Try our TFMA tutorial.</p> <p>Check out our github page for details on the supported metrics and plots and associated notebook visualizations.</p> <p>See the installation and getting started guides for information and examples on how to get set up in a standalone pipeline. Recall that TFMA is also used within the Evaluator component in TFX, so these resources will be useful for getting started in TFX as well.</p>"},{"location":"architecture/","title":"Tensorflow Model Analysis Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>The TensorFlow Model Analysis (TFMA) pipeline is depicted as follows:</p> <p></p> <p>The pipeline is made up of four main components:</p> <ul> <li>Read Inputs</li> <li>Extraction</li> <li>Evaluation</li> <li>Write Results</li> </ul> <p>These components make use of two primary types: <code>tfma.Extracts</code> and <code>tfma.evaluators.Evaluation</code>. The type <code>tfma.Extracts</code> represents data that is extracted during pipeline processing and may correspond to one or more examples for the model. <code>tfma.evaluators.Evaluation</code> represents the output from evaluating the extracts at various points during the process of extraction. In order to provide a flexible API, these types are just dicts where the keys are defined (reserved for use) by different implementations. The types are defined as follows:</p> <pre><code># Extracts represent data extracted during pipeline processing.\n# For example, the PredictExtractor stores the data for the\n# features, labels, and predictions under the keys \"features\",\n# \"labels\", and \"predictions\".\nExtracts = Dict[Text, Any]\n\n# Evaluation represents the output from evaluating extracts at\n# particular point in the pipeline. The evaluation outputs are\n# keyed by their associated output type. For example, the metric / plot\n# dictionaries from evaluating metrics and plots will be stored under\n# \"metrics\" and \"plots\" respectively.\nEvaluation = Dict[Text, beam.pvalue.PCollection]\n</code></pre> <p>Note that <code>tfma.Extracts</code> are never written out directly they must always go through an evaluator to produce a <code>tfma.evaluators.Evaluation</code> that is then written out. Also note that <code>tfma.Extracts</code> are dicts that are stored in a <code>beam.pvalue.PCollection</code> (i.e. <code>beam.PTransform</code>s take as input <code>beam.pvalue.PCollection[tfma.Extracts]</code>) whereas a <code>tfma.evaluators.Evaluation</code> is a dict whose values are <code>beam.pvalue.PCollection</code>s (i.e. <code>beam.PTransform</code>s take the dict itself as the argument for the <code>beam.value.PCollection</code> input). In other words the <code>tfma.evaluators.Evaluation</code> is used at pipeline construction time, but the <code>tfma.Extracts</code> are used at pipeline runtime.</p>"},{"location":"architecture/#read-inputs","title":"Read Inputs","text":"<p>The <code>ReadInputs</code> stage is made up of a transform that takes raw inputs (tf.train.Example, CSV, ...) and converts them into extracts. Today the extracts are represented as raw input bytes stored under <code>tfma.INPUT_KEY</code>, however the extracts can be in any form that is compatible with the extraction pipeline -- meaning that it creates <code>tfma.Extracts</code> as output, and that those extracts are compatible with downstream extractors. It is up to the different extractors to clearly document what they require.</p>"},{"location":"architecture/#extraction","title":"Extraction","text":"<p>The extraction process is a list of <code>beam.PTransform</code>s that are run in series. The extractors take <code>tfma.Extracts</code> as input and return <code>tfma.Extracts</code> as output. The proto-typical extractor is the <code>tfma.extractors.PredictExtractor</code> which uses the input extract produced by the read inputs transform and runs it through a model to produce predictions extracts. Customized extractors can be inserted at any point provided their transforms conform to the <code>tfma.Extracts</code> in and <code>tfma.Extracts</code> out API. An extractor is defined as follows:</p> <pre><code># An Extractor is a PTransform that takes Extracts as input and returns\n# Extracts as output. A typical example is a PredictExtractor that receives\n# an 'input' placeholder for input and adds additional 'predictions' extracts.\nExtractor = NamedTuple('Extractor', [\n    ('stage_name', Text),\n    ('ptransform', beam.PTransform)])  # Extracts -&gt; Extracts\n</code></pre>"},{"location":"architecture/#inputextractor","title":"InputExtractor","text":"<p>The <code>tfma.extractors.InputExtractor</code> is used to extract raw features, raw labels, and raw example weights from <code>tf.train.Example</code> records for using in metrics slicing and computations. By default the values are stored under the extract keys <code>features</code>, <code>labels</code>, and <code>example_weights</code> respectively. Single-output model labels and example weights are stored directly as <code>np.ndarray</code> values. Multi-output model labels and example weights are stored as dicts of <code>np.ndarray</code> values (keyed by output name). If multi-model evaluation is performed the labels and example weights will be further embedded within another dict (keyed by model name).</p>"},{"location":"architecture/#predictextractor","title":"PredictExtractor","text":"<p>The <code>tfma.extractors.PredictExtractor</code> runs model predictions and stores them under the key <code>predictions</code> in the <code>tfma.Extracts</code> dict. Single-output model predictions are stored directly as the predicted output values. Multi-output model predictions are stored as a dict of output values (keyed by output name).  If multi-model evaluation is performed the prediction will be further embedded within another dict (keyed by model name). The actual output value used depends on the model (e.g. TF estimator's return outputs in the form of a dict whereas keras returns <code>np.ndarray</code> values).</p>"},{"location":"architecture/#slicekeyextractor","title":"SliceKeyExtractor","text":"<p>The <code>tfma.extractors.SliceKeyExtractor</code> uses the slicing spec to determine which slices apply to each example input based on the extracted features and adds the coresponding slicing values to the extracts for later use by the evaluators.</p>"},{"location":"architecture/#evaluation","title":"Evaluation","text":"<p>Evaluation is the process of taking an extract and evaluating it. While it is common to perform evaluation at the end of the extraction pipeline, there are use-cases that require evaluation earlier in the extraction process. As such evaluators are associated with the extractors whose output they should be evaluated against. An evaluator is defined as follows:</p> <pre><code># An evaluator is a PTransform that takes Extracts as input and\n# produces an Evaluation as output. A typical example of an evaluator\n# is the MetricsAndPlotsEvaluator that takes the 'features', 'labels',\n# and 'predictions' extracts from the PredictExtractor and evaluates\n# them using post export metrics to produce metrics and plots dictionaries.\nEvaluator = NamedTuple('Evaluator', [\n  ('stage_name', Text),\n  ('run_after', Text),              # Extractor.stage_name\n  ('ptransform', beam.PTransform)]) # Extracts -&gt; Evaluation\n</code></pre> <p>Notice that an evaluator is a <code>beam.PTransform</code> that takes <code>tfma.Extracts</code> as inputs. There is nothing stopping an implementation from performing additional transformations on the extracts as part of the evaluation process. Unlike extractors that must return a <code>tfma.Extracts</code> dict, there are no restrictions on the types of outputs an evaluator can produce though most evaluators also return a dict (e.g. of metric names and values).</p>"},{"location":"architecture/#metricsandplotsevaluator","title":"MetricsAndPlotsEvaluator","text":"<p>The <code>tfma.evaluators.MetricsAndPlotsEvaluator</code> takes <code>features</code>, <code>labels</code>, and <code>predictions</code> as input, runs them through <code>tfma.slicer.FanoutSlices</code> to group them by slices, and then performs metrics and plots computations. It produces outputs in the form of dictionaries of metrics and plots keys and values (these are later converted to serialized protos for output by <code>tfma.writers.MetricsAndPlotsWriter</code>).</p>"},{"location":"architecture/#write-results","title":"Write Results","text":"<p>The <code>WriteResults</code> stage is where the evaluation output gets written out to disk. WriteResults uses writers to write out the data based on the output keys. For example, an <code>tfma.evaluators.Evaluation</code> may contain keys for <code>metrics</code> and <code>plots</code>. These would then be associated with the metrics and plots dictionaries called 'metrics' and 'plots'. The writers specify how to write out each file:</p> <pre><code># A writer is a PTransform that takes evaluation output as input and\n# serializes the associated PCollections of data to a sink.\nWriter = NamedTuple('Writer', [\n  ('stage_name', Text),\n  ('ptransform', beam.PTransform)])    # Evaluation -&gt; PDone\n</code></pre>"},{"location":"architecture/#metricsandplotswriter","title":"MetricsAndPlotsWriter","text":"<p>We provide a <code>tfma.writers.MetricsAndPlotsWriter</code> that converts the metrics and plots dictionaries to serialized protos and writes them to disk.</p> <p>If you wish to use a different serialization format, you can create a custom writer and use that instead. Since the <code>tfma.evaluators.Evaluation</code> passed to the writers contains the output for all of the evaluators combined, a <code>tfma.writers.Write</code> helper transform is provided that writers can use in their <code>ptransform</code> implementations to select the appropriate <code>beam.PCollection</code>s based on an output key (see below for an example).</p>"},{"location":"architecture/#customization","title":"Customization","text":"<p>The <code>tfma.run_model_analysis</code> method takes <code>extractors</code>, <code>evaluators</code>, and <code>writers</code> arguments for customing the extractors, evaluators, and writers used by the pipeline. If no arguments are provided then <code>tfma.default_extractors</code>, <code>tfma.default_evaluators</code>, and <code>tfma.default_writers</code> are used by default.</p>"},{"location":"architecture/#custom-extractors","title":"Custom Extractors","text":"<p>To create a custom extractor, create a <code>tfma.extractors.Extractor</code> type that wraps a <code>beam.PTransform</code> taking <code>tfma.Extracts</code> as input and returning <code>tfma.Extracts</code> as output. Examples of extractors are available under <code>tfma.extractors</code>.</p>"},{"location":"architecture/#custom-evaluators","title":"Custom Evaluators","text":"<p>To create a custom evaluator, create a <code>tfma.evaluators.Evaluator</code> type that wraps a <code>beam.PTransform</code> taking <code>tfma.Extracts</code> as input and returning <code>tfma.evaluators.Evaluation</code> as output. A very basic evaluator might just take the incoming <code>tfma.Extracts</code> and output them for storing in a table. This is exactly what the <code>tfma.evaluators.AnalysisTableEvaluator</code> does. A more complicated evaluator might perform additional processing and data aggregation. See the <code>tfma.evaluators.MetricsAndPlotsEvaluator</code> as an example.</p> <p>Note that the <code>tfma.evaluators.MetricsAndPlotsEvaluator</code> itself can be customized to support custom metrics (see metrics for more details).</p>"},{"location":"architecture/#custom-writers","title":"Custom Writers","text":"<p>To create a custom writer, create a <code>tfma.writers.Writer</code> type that wraps a <code>beam.PTransform</code> taking <code>tfma.evaluators.Evaluation</code> as input and returning <code>beam.pvalue.PDone</code> as output. The following is a basic example of a writer for writing out TFRecords containing metrics:</p> <pre><code>tfma.writers.Writer(\n  stage_name='WriteTFRecord(%s)' % tfma.METRICS_KEY,\n  ptransform=tfma.writers.Write(\n    key=tfma.METRICS_KEY,\n    ptransform=beam.io.WriteToTFRecord(file_path_prefix=output_file))\n</code></pre> <p>A writer's inputs depend on the output of the associated evaluator. For the above example, the output is a serialized proto produced by the <code>tfma.evaluators.MetricsAndPlotsEvaluator</code>. A writer for the <code>tfma.evaluators.AnalysisTableEvaluator</code> would be responsible for writing out a <code>beam.pvalue.PCollection</code> of <code>tfma.Extracts</code>.</p> <p>Note that a writer is associated with the output of an evaluator via the output key used (e.g. <code>tfma.METRICS_KEY</code>, <code>tfma.ANALYSIS_KEY</code>, etc).</p>"},{"location":"architecture/#step-by-step-example","title":"Step by Step Example","text":"<p>The following is an example of the steps involved in the extraction and evaluation pipeline when both the <code>tfma.evaluators.MetricsAndPlotsEvaluator</code> and <code>tfma.evaluators.AnalysisTableEvaluator</code> are used:</p> <pre><code>run_model_analysis(\n    ...\n    extractors=[\n        tfma.extractors.InputExtractor(...),\n        tfma.extractors.PredictExtractor(...),\n        tfma.extractors.SliceKeyExtrator(...)\n    ],\n    evaluators=[\n        tfma.evaluators.MetricsAndPlotsEvaluator(...),\n        tfma.evaluators.AnalysisTableEvaluator(...)\n    ])\n</code></pre> <p><code>ReadInputs</code></p> <pre><code># Out\nExtracts {\n  'input': bytes                 # CSV, Proto, ...\n}\n</code></pre> <p><code>ExtractAndEvaluate</code></p> <ul> <li><code>tfma.extractors.InputExtractor</code></li> </ul> <pre><code># In:  ReadInputs Extracts\n# Out:\nExtracts {\n  'input': bytes                    # CSV, Proto, ...\n  'features': tensor_like           # Raw features\n  'labels': tensor_like             # Labels\n  'example_weights': tensor_like    # Example weights\n}\n</code></pre> <ul> <li><code>tfma.extractors.PredictExtractor</code></li> </ul> <pre><code># In:  InputExtractor Extracts\n# Out:\nExtracts {\n  'input': bytes                    # CSV, Proto, ...\n  'features': tensor_like           # Raw features\n  'labels': tensor_like             # Labels\n  'example_weights': tensor_like    # Example weights\n  'predictions': tensor_like        # Predictions\n}\n</code></pre> <ul> <li><code>tfma.extractors.SliceKeyExtractor</code></li> </ul> <pre><code># In: PredictExtractor Extracts\n# Out:\nExtracts {\n  'features': tensor_like           # Raw features\n  'labels': tensor_like             # Labels\n  'example_weights': tensor_like    # Example weights\n  'predictions': tensor_like        # Predictions\n  'slice_key': Tuple[bytes...]      # Slice\n}\n</code></pre> <ul> <li><code>tfma.evaluators.MetricsAndPlotsEvaluator</code> (run_after: <code>SLICE_KEY_EXTRACTOR_STAGE_NAME</code>)</li> </ul> <pre><code># In: SliceKeyExtractor Extracts\n# Out:\nEvaluation {\n  'metrics': PCollection[Tuple[slicer.SliceKeyType, Dict[Text, Any]]]  # Tuples of (slice key, dictionary from metric key to metric values)\n  'plots': PCollection[Tuple[slicer.SliceKeyType, Dict[Text, Any]]]  # Tuples of (slice key, dictionary from plot key to plot values)\n}\n</code></pre> <ul> <li><code>tfma.evaluators.AnalysisTableEvaluator</code> (run_after: <code>LAST_EXTRACTOR_STAGE_NAME</code>)</li> </ul> <pre><code># In: SliceKeyExtractor Extracts\n# Out:\nEvaluation {\n  'analysis': PCollection[Extracts] # Final Extracts\n}\n</code></pre> <p><code>WriteResults</code></p> <pre><code># In:\nEvaluation {\n  'metrics': PCollection[Tuple[slicer.SliceKeyType, Dict[Text, Any]]]  # Tuples of (slice key, dictionary from metric key to metric values)\n  'plots': PCollection[Tuple[slicer.SliceKeyType, Dict[Text, Any]]]  # Tuples of (slice key, dictionary from plot key to plot values)\n  'analysis': PCollection[Extracts] # Final Extracts\n}\n# Out: metrics, plots, and analysis files\n</code></pre>"},{"location":"eval_saved_model/","title":"Configuring an Eval Saved Model","text":"<p>TensorFlow Model Analysis (TFMA) can export a model's evaluation graph to a special <code>SavedModel</code> called <code>EvalSavedModel</code>. (Note that the evaluation graph is used and not the graph for training or inference.) The <code>EvalSavedModel</code> contains additional information that allows TFMA to compute the same evaluation metrics defined in the model in a distributed manner over a large amount of data and user-defined slices.</p>"},{"location":"eval_saved_model/#modify-an-existing-model","title":"Modify an existing model","text":"<p>To use an existing model with TFMA, first modify the model to export the <code>EvalSavedModel</code>. This is done by adding a call to <code>tfma.export.export_eval_savedmodel</code> and is similar to <code>estimator.export_savedmodel</code>. For example:</p> <pre><code># Define, train and export your estimator as usual\nestimator = tf.estimator.DNNClassifier(...)\nestimator.train(...)\nestimator.export_savedmodel(...)\n\n# Also export the EvalSavedModel\ntfma.export.export_eval_savedmodel(\n  estimator=estimator, export_dir_base=export_dir,\n  eval_input_receiver_fn=eval_input_receiver_fn)\n</code></pre> <p><code>eval_input_receiver_fn</code> must be defined and is similar to the <code>serving_input_receiver_fn</code> for <code>estimator.export_savedmodel</code>. Like <code>serving_input_receiver_fn</code>, the <code>eval_input_receiver_fn</code> function defines an input placeholder example, parses the features from the example, and returns the parsed features. It parses and returns the label.</p> <p>The following snippet defines an example <code>eval_input_receiver_fn</code>:</p> <pre><code>country = tf.feature_column.categorical_column_with_hash('country', 100)\nlanguage = tf.feature_column.categorical_column_with_hash('language', 100)\nage = tf.feature_column.numeric_column('age')\nlabel = tf.feature_column.numeric_column('label')\n\ndef eval_input_receiver_fn():\n  serialized_tf_example = tf.compat.v1.placeholder(\n      dtype=tf.string, shape=[None], name='input_example_placeholder')\n\n  # This *must* be a dictionary containing a single key 'examples', which\n  # points to the input placeholder.\n  receiver_tensors = {'examples': serialized_tf_example}\n\n  feature_spec =  tf.feature_column.make_parse_example_spec(\n      [country, language, age, label])\n  features = tf.io.parse_example(serialized_tf_example, feature_spec)\n\n  return tfma.export.EvalInputReceiver(\n    features=features,\n    receiver_tensors=receiver_tensors,\n    labels=features['label'])\n</code></pre> <p>In this example you can see that:</p> <ul> <li><code>labels</code> can also be a dictionary. Useful for a multi-headed model.</li> <li>The <code>eval_input_receiver_fn</code> function will, most likely, be the same as your     <code>serving_input_receiver_fn</code> function. But, in some cases, you may want to     define additional features for slicing. For example, you introduce an     <code>age_category</code> feature which divides the <code>age</code> feature into multiple     buckets. You can then slice on this feature in TFMA to help understand how     your model's performance differs across different age categories.</li> </ul>"},{"location":"eval_saved_model/#adding-post-export-metrics","title":"Adding Post Export Metrics","text":"<p>Additional metrics that are not included in the model can be aded using <code>add_metrics_callbacks</code>. For more details, see the Python help for <code>run_model_analysis</code>.</p>"},{"location":"eval_saved_model/#end-to-end-examples","title":"End-to-end examples","text":"<p>Try the extensive end-to-end example featuring TensorFlow Transform for feature preprocessing, TensorFlow Estimators for training, TensorFlow Model Analysis and Jupyter for evaluation, and TensorFlow Serving for serving.</p>"},{"location":"eval_saved_model/#adding-a-custom-post-export-metric","title":"Adding a Custom Post Export Metric","text":"<p>If you want to add your own custom post export metric in TFMA, please checkout the documentation here.</p>"},{"location":"faq/","title":"Tensorflow Model Analysis Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#is-an-evalsavedmodel-still-required","title":"Is an EvalSavedModel still required?","text":"<p>Previously TFMA required all metrics to be stored within a tensorflow graph using a special <code>EvalSavedModel</code>. Now, metrics can be computed outside of the TF graph using <code>beam.CombineFn</code> implementations.</p> <p>Some of the main differences are:</p> <ul> <li>An <code>EvalSavedModel</code> requires a special export from the trainer whereas a     serving model can be used without any changes required to the training code.</li> <li>When an <code>EvalSavedModel</code> is used, any metrics added at training time are     automatically available at evaluation time. Without an <code>EvalSavedModel</code>     these metrics must be re-added.<ul> <li>The exception to this rule is if a keras model is used the metrics can     also be added automatically because keras saves the metric information     along side of the saved model.</li> </ul> </li> </ul> <p>NOTE: There are some metrics that are only supported using combiners (e.g. multi-class/multi-label plots, aggregated multi-clas/multi-label metrics, etc).</p>"},{"location":"faq/#can-tfma-work-with-both-in-graph-metrics-and-external-metrics","title":"Can TFMA work with both in-graph metrics and external metrics?","text":"<p>TFMA allows a hybrid approach to be used where some metrics can be computed in-graph where as others can be computed outside. If you currently have an <code>EvalSavedModel</code> then you can continue to use it.</p> <p>There are two cases:</p> <ol> <li>Use TFMA <code>EvalSavedModel</code> for both feature extraction and metric     computations but also add additional combiner-based metrics. In this case     you would get all the in-graph metrics from the <code>EvalSavedModel</code> along with     any additional metrics from the combiner-based that might not have been     previously supported.</li> <li>Use TFMA <code>EvalSavedModel</code> for feature/prediction extraction but use     combiner-based metrics for all metrics computations. This mode is useful if     there are feature transformations present in the <code>EvalSavedModel</code> that you     would like to use for slicing, but prefer to perform all metric computations     outside the graph.</li> </ol>"},{"location":"faq/#setup","title":"Setup","text":""},{"location":"faq/#what-model-types-are-supported","title":"What model types are supported?","text":"<p>TFMA supports keras models, models based on generic TF2 signature APIs, as well TF estimator based models (although depending on the use case the estimator based models may require an <code>EvalSavedModel</code> to be used).</p> <p>See get started guide for the full list of model types supported and any restrictions.</p>"},{"location":"faq/#how-do-i-setup-tfma-to-work-with-a-native-keras-based-model","title":"How do I setup TFMA to work with a native keras based model?","text":"<p>The following is an example config for a keras model based on the following assumptions:</p> <ul> <li>Saved model is for serving and uses the signature name <code>serving_default</code>     (this can be changed using <code>model_specs[0].signature_name</code>).</li> <li>Built in metrics from <code>model.compile(...)</code> should be evaluated (this can be     disabled via <code>options.include_default_metric</code> within the     tfma.EvalConfig).</li> </ul> <pre><code>from google.protobuf import text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    label_key: \"&lt;label-key&gt;\"\n    example_weight_key: \"&lt;example-weight-key&gt;\"\n  }\n  metrics_specs {\n    # Add metrics here. For example:\n    #  metrics { class_name: \"ConfusionMatrixPlot\" }\n    #  metrics { class_name: \"CalibrationPlot\" }\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n</code></pre> <p>See metrics for more information about other types of metrics that can be configured.</p>"},{"location":"faq/#how-do-i-setup-tfma-to-work-with-a-generic-tf2-signatures-based-model","title":"How do I setup TFMA to work with a generic TF2 signatures based model?","text":"<p>The following is an example config for a generic TF2 model. Below, <code>signature_name</code> is the name of the specific signature that should be used for evaluation.</p> <pre><code>from google.protobuf import text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    signature_name: \"&lt;signature-name&gt;\"\n    label_key: \"&lt;label-key&gt;\"\n    example_weight_key: \"&lt;example-weight-key&gt;\"\n  }\n  metrics_specs {\n    # Add metrics here. For example:\n    #  metrics { class_name: \"BinaryCrossentropy\" }\n    #  metrics { class_name: \"ConfusionMatrixPlot\" }\n    #  metrics { class_name: \"CalibrationPlot\" }\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n</code></pre> <p>See metrics for more information about other types of metrics that can be configured.</p>"},{"location":"faq/#how-do-i-setup-tfma-to-work-with-an-estimator-based-model","title":"How do I setup TFMA to work with an estimator based model?","text":"<p>In this case there are three choices.</p> <p>Option1: Use Serving Model</p> <p>If this option is used then any metrics added during training will NOT be included in the evaluation.</p> <p>The following is an example config assuming <code>serving_default</code> is the signature name used:</p> <pre><code>from google.protobuf import text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    label_key: \"&lt;label-key&gt;\"\n    example_weight_key: \"&lt;example-weight-key&gt;\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n</code></pre> <p>See metrics for more information about other types of metrics that can be configured.</p> <p>Option2: Use EvalSavedModel along with additional combiner-based metrics</p> <p>In this case, use <code>EvalSavedModel</code> for both feature / prediction extraction and evaluation and also add additional combiner based metrics.</p> <p>The following is an example config:</p> <pre><code>from google.protobuf import text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    signature_name: \"eval\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n</code></pre> <p>See metrics for more information about other types of metrics that can be configured and EvalSavedModel for more information about setting up the EvalSavedModel.</p> <p>Option3: Use EvalSavedModel Model only for Feature / Prediction Extraction</p> <p>Similar to option(2), but only use <code>EvalSavedModel</code> for feature / prediction extraction. This option is useful if only external metrics are desired, but there are feature transformations that you would like to slice on. Similar to option (1) any metrics added during training will NOT be included in the evaluation.</p> <p>In this case the config is the same as above only <code>include_default_metrics</code> is disabled.</p> <pre><code>from google.protobuf import text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    signature_name: \"eval\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n  options {\n    include_default_metrics { value: false }\n  }\n\"\"\", tfma.EvalConfig())\n</code></pre> <p>See metrics for more information about other types of metrics that can be configured and EvalSavedModel for more information about setting up the EvalSavedModel.</p>"},{"location":"faq/#how-do-i-setup-tfma-to-work-with-a-keras-model-to-estimator-based-model","title":"How do I setup TFMA to work with a keras model-to-estimator based model?","text":"<p>The keras <code>model_to_estimator</code> setup is similar to the estimator confiugration. However there are a few differences specific to how model to estimator works. In particular, the model-to-esimtator returns its outputs in the form of a dict where the dict key is the name of the last output layer in the associated keras model (if no name is provided, keras will choose a default name for you such as <code>dense_1</code> or <code>output_1</code>). From a TFMA perspective, this behavior is similar to what would be output for a multi-output model even though the model to estimator may only be for a single model. To account for this difference, an additional step is required to setup the output name. However, the same three options apply as estimator.</p> <p>The following is an example of the changes required to an estimator based config:</p> <pre><code>from google.protobuf import text_format\n\nconfig = text_format.Parse(\"\"\"\n  ... as for estimator ...\n  metrics_specs {\n    output_names: [\"&lt;keras-output-layer&gt;\"]\n    # Add metrics here.\n  }\n  ... as for estimator ...\n\"\"\", tfma.EvalConfig())\n</code></pre>"},{"location":"faq/#how-do-i-setup-tfma-to-work-with-pre-calculated-ie-model-agnostic-predictions-tfrecord-and-tfexample","title":"How do I setup TFMA to work with pre-calculated (i.e. model-agnostic) predictions? (<code>TFRecord</code> and <code>tf.Example</code>)","text":"<p>In order to configure TFMA to work with pre-calculated predictions, the default <code>tfma.PredictExtractor</code> must be disabled and the <code>tfma.InputExtractor</code> must be configured to parse the predictions along with the other input features. This is accomplished by configuring a <code>tfma.ModelSpec</code> with the name of the feature key used for the predictions alongside of the labels and weights.</p> <p>The following is an example setup:</p> <pre><code>from google.protobuf import text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    prediction_key: \"&lt;prediction-key&gt;\"\n    label_key: \"&lt;label-key&gt;\"\n    example_weight_key: \"&lt;example-weight-key&gt;\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n</code></pre> <p>See metrics for more information about metrics that can be configured.</p> <p>Note that altough a <code>tfma.ModelSpec</code> is being configured a model is not actually being used (i.e. there is no <code>tfma.EvalSharedModel</code>). The call to run model analysis might look as follows:</p> <pre><code>eval_result = tfma.run_model_analysis(\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location=\"/path/to/file/containing/tfrecords\",\n    output_path=\"/path/for/metrics_for_slice_proto\")\n</code></pre>"},{"location":"faq/#how-do-i-setup-tfma-to-work-with-pre-calculated-ie-model-agnostic-predictions-pddataframe","title":"How do I setup TFMA to work with pre-calculated (i.e. model-agnostic) predictions? (<code>pd.DataFrame</code>)","text":"<p>For small datasets that can fit in memory, an alternative to a <code>TFRecord</code> is a <code>pandas.DataFrame</code>s. TFMA can operate on <code>pandas.DataFrame</code>s using the <code>tfma.analyze_raw_data</code> API. For an explanation of <code>tfma.MetricsSpec</code> and <code>tfma.SlicingSpec</code>, see the setup guide. See metrics for more information about metrics that can be configured.</p> <p>The following is an example setup:</p> <pre><code># Run in a Jupyter Notebook.\n\ndf_data = ...  # your pd.DataFrame\n\neval_config = text_format.Parse(\"\"\"\n  model_specs {\n    label_key: 'label'\n    prediction_key: 'prediction'\n  }\n  metrics_specs {\n    metrics { class_name: \"AUC\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  slicing_specs {}\n  slicing_specs {\n    feature_keys: 'language'\n  }\n\"\"\", config.EvalConfig())\n\neval_result = tfma.analyze_raw_data(df_data, eval_config)\n\ntfma.view.render_slicing_metrics(eval_result)\n</code></pre>"},{"location":"faq/#metrics","title":"Metrics","text":""},{"location":"faq/#what-types-of-metrics-are-supported","title":"What types of metrics are supported?","text":"<p>TFMA supports a wide variety of metrics including:</p> <ul> <li>regression metrics</li> <li>binary classification metrics</li> <li>multi-class/multi-label classification metrics</li> <li>micro average / macro average metrics</li> <li>query / ranking based metrics</li> </ul>"},{"location":"faq/#are-metrics-from-multi-output-models-supported","title":"Are metrics from multi-output models supported?","text":"<p>Yes. See metrics guide for more details.</p>"},{"location":"faq/#are-metrics-from-multiple-models-supported","title":"Are metrics from multiple-models supported?","text":"<p>Yes. See metrics guide for more details.</p>"},{"location":"faq/#can-the-metric-settings-name-etc-be-customized","title":"Can the metric settings (name, etc) be customized?","text":"<p>Yes. Metrics settings can be customized (e.g. setting specific thresholds, etc) by adding <code>config</code> settings to the metric configuration. See metrics guide has more details.</p>"},{"location":"faq/#are-custom-metrics-supported","title":"Are custom metrics supported?","text":"<p>Yes. Either by writing a custom <code>tf.keras.metrics.Metric</code> implementation or by writing a custom <code>beam.CombineFn</code> implementation. The metrics guide has more details.</p>"},{"location":"faq/#what-types-of-metrics-are-not-supported","title":"What types of metrics are not supported?","text":"<p>As long as your metric can be calculated using a <code>beam.CombineFn</code>, there are no restrictions on the types of metrics that can be computed based on <code>tfma.metrics.Metric</code>. If working with a metric derived from <code>tf.keras.metrics.Metric</code> then the following criteria must be satisfied:</p> <ul> <li>It should be possible to compute sufficient statistics for the metric on     each example independently, then combine these sufficient statistics by     adding them across all the examples, and determine the metric value solely     from these sufficient statistics.</li> <li>For example, for accuracy the sufficient statistics are \"total correct\" and     \"total examples\". It\u2019s possible to compute these two numbers for individual     examples, and add them up for a group of examples to get the right values     for those examples. The final accuracy can be computed used \"total correct /     total examples\".</li> </ul>"},{"location":"faq/#add-ons","title":"Add-ons","text":""},{"location":"faq/#can-i-use-tfma-to-evaluate-fairness-or-bias-in-my-model","title":"Can I use TFMA to evaluate fairness or bias in my model?","text":"<p>TFMA includes a FairnessIndicators add-on that provides post-export metrics for evaluating the effects of unintended bias in classification models.</p>"},{"location":"faq/#customization","title":"Customization","text":""},{"location":"faq/#what-if-i-need-more-customization","title":"What if I need more customization?","text":"<p>TFMA is very flexibile and allows you to customize almost all parts of the pipeline using custom <code>Extractors</code>, <code>Evaluators</code>, and/or <code>Writers</code>. These abstrations are discusssed in more detail in the architecture document.</p>"},{"location":"faq/#troubleshooting-debugging-and-getting-help","title":"Troubleshooting, debugging, and getting help","text":""},{"location":"faq/#why-dont-multiclassconfusionmatrix-metrics-match-binarized-confusionmatrix-metrics","title":"Why don't MultiClassConfusionMatrix metrics match binarized ConfusionMatrix metrics","text":"<p>These are actually different calculations. Binarization performs a comparison for each class ID independently (i.e. the prediction for each class is compared separately against the thresholds provided). In this case it is possible for two or more classes to all indicate that they matched the prediction because their predicted value was greater than the threshold (this will be even more apparant at lower thresholds). In the case of the multiclass confusion matrix, there is still only one true predicted value and it either matches the actual value or it doesn't. The threshold is only used to force a prediction to match no class if it is less than the threshold. The higher the threshold the harder for a binarized class's prediction to match. Likewise the lower the threshold the easier it is for a binarized class's predictions to match. The means that at thresholds &gt; 0.5 the binarized values and the multiclass matrix values will be closer aligned and at thresholds &lt; 0.5 they will be farther apart.</p> <p>For example, let's say we have 10 classes where class 2 was predicted with a probability of 0.8, but the actual class was class 1 which had a probability of 0.15. If you binarize on class 1 and use a threshold of 0.1, then class 1 will be considered correct (0.15 &gt; 0.1) so it will be counted as a TP, However, for the multiclass case, class 2 will be considered correct (0.8 &gt; 0.1) and since class 1 was the actual, this will be counted as a FN. Because at lower thresholds more values will be considered positives, in general there will be higher TP and FP counts for binarized confusion matrix than for the multiclass confusion matrix, and similarly lower TN and FN.</p> <p>The following is an example of observed differences between MultiClassConfusionMatrixAtThresholds and the corresponding counts from binarization of one of the classes.</p> <p></p>"},{"location":"faq/#why-do-my-precision1-and-recall1-metrics-have-the-same-value","title":"Why do my precision@1 and recall@1 metrics have the same value?","text":"<p>At a top k value of 1 precision and recall are the same thing. Precision is equal to <code>TP / (TP + FP)</code> and recall is equal to <code>TP / (TP + FN)</code>. The top prediction is always positive and will either match or not match the label. In other words, with <code>N</code> examples, <code>TP + FP = N</code>. However, if the label doesn't match the top prediction, then this also implies a non-top k prediction was matched and with top k set to 1, all non-top 1 predictions will be 0. This implies FN must be <code>(N - TP)</code> or <code>N = TP + FN</code>. The end result is <code>precision@1 = TP / N = recall@1</code>. Note that this only applies when there is a single label per example, not for multi-label.</p>"},{"location":"faq/#why-are-my-mean_label-and-mean_prediction-metrics-always-05","title":"Why are my mean_label and mean_prediction metrics always 0.5?","text":"<p>This is most likely caused because the metrics are configured for a binary classification problem, but the model is outputing probabilities for both of the classes instead of just one. This is common when tensorflow's classification API is used. The solution is to choose the class that you would like the predictions to be based on and then binarize on that class. For example:</p> <pre><code>eval_config = text_format.Parse(\"\"\"\n  ...\n  metrics_specs {\n    binarize { class_ids: { values: [0] } }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    ...\n  }\n  ...\n\"\"\", config.EvalConfig())\n</code></pre> <p>Note: This applies to all metrics not just <code>mean_label</code> and <code>mean_prediction</code>.</p>"},{"location":"faq/#how-to-interpret-the-multilabelconfusionmatrixplot","title":"How to interpret the MultiLabelConfusionMatrixPlot?","text":"<p>Given a particular label, the <code>MultiLabelConfusionMatrixPlot</code> (and associated <code>MultiLabelConfusionMatrix</code>) can be used to compare the outcomes of other labels and their predictions when the chosen label was actually true. For example, let's say that we have three classes <code>bird</code>, <code>plane</code>, and <code>superman</code> and we are classifying pictures to indicate if they contain one or more of any of these classes. The <code>MultiLabelConfusionMatrix</code> will compute the cartesian product of each actual class against each other class (called the predicted class). Note that while the pairing is <code>(actual, predicted)</code>, the <code>predicted</code> class does not necessarily imply a positive prediction, it merely represents the predicted column in the actual vs predicted matrix. For example, let's say we have computed the following matrices:</p> <pre><code>   (bird, bird)         -&gt;    { tp: 6, fp: 0, fn: 2, tn: 0}\n   (bird, plane)        -&gt;    { tp: 2, fp: 2, fn: 2, tn: 2}\n   (bird, superman)     -&gt;    { tp: 1, fp: 1, fn: 4, tn: 2}\n   (plane, bird)        -&gt;    { tp: 3, fp: 1, fn: 1, tn: 3}\n   (plane, plane)       -&gt;    { tp: 4, fp: 0, fn: 4, tn: 0}\n   (plane, superman)    -&gt;    { tp: 1, fp: 3, fn: 3, tn: 1}\n   (superman, bird)     -&gt;    { tp: 3, fp: 2, fn: 2, tn: 2}\n   (superman, plane)    -&gt;    { tp: 2, fp: 3, fn: 2, tn: 2}\n   (superman, superman) -&gt;    { tp: 4, fp: 0, fn: 5, tn: 0}\n\n   num_examples: 20\n</code></pre> <p>The <code>MultiLabelConfusionMatrixPlot</code> has three ways to display this data. In all cases the way to read the table is row by row from the perspective of the actual class.</p> <p>Note: Do NOT expect the counts to add up to the number of examples, these counts are done across labels and since this is a multi-label problem, there will be double counting.</p> <p>1) Total Prediction Count</p> <p>In this case, for a given row (i.e. actual class) what were the <code>TP + FP</code> counts for the other classes. For the counts above, our display would be as follows:</p> <pre><code>            | Predicted bird | Predicted plane | Predicted superman\n</code></pre> <p>--------------- | -------------- | --------------- | ------------------ Actual bird     | 6              | 4               | 2 Actual plane    | 4              | 4               | 4 Actual superman | 5              | 5               | 4</p> <p>When the pictures actually contained a <code>bird</code> we correctly predicted 6 of them. At the same time we also predicted <code>plane</code> (either correctly or wrongly) 4 times and <code>superman</code> (either correctly or wrongly) 2 times.</p> <p>2) Incorrect Prediction Count</p> <p>In this case, for a given row (i.e. actual class) what were the <code>FP</code> counts for the other classes. For the counts above, our display would be as follows:</p> <pre><code>            | Predicted bird | Predicted plane | Predicted superman\n</code></pre> <p>--------------- | -------------- | --------------- | ------------------ Actual bird     | 0              | 2               | 1 Actual plane    | 1              | 0               | 3 Actual superman | 2              | 3               | 0</p> <p>When the pictures actually contained a <code>bird</code> we incorrectly predicted <code>plane</code> 2 times and <code>superman</code> 1 times.</p> <p>3) False Negative Count</p> <p>In this case, for a given row (i.e. actual class) what were the <code>FN</code> counts for the other classes. For the counts above, our display would be as follows:</p> <pre><code>            | Predicted bird | Predicted plane | Predicted superman\n</code></pre> <p>--------------- | -------------- | --------------- | ------------------ Actual bird     | 2              | 2               | 4 Actual plane    | 1              | 4               | 3 Actual superman | 2              | 2               | 5</p> <p>When the pictures actually contained a <code>bird</code> we failed to predict it 2 times. At the same time, we failed to predict <code>plane</code> 2 times and <code>superman</code> 4 times.</p>"},{"location":"faq/#why-do-i-get-an-error-about-prediction-key-not-found","title":"Why do I get an error about prediction key not found?","text":"<p>Some model's output their prediction in the form of a dictionary. For example, a TF estimator for binary classification problem outputs a dictionary containing <code>probabilities</code>, <code>class_ids</code>, etc. In most cases TFMA has defaults for finding commomly used key names such as <code>predictions</code>, <code>probabilities</code>, etc. However, if your model is very customized it may output keys under names not known by TFMA. In theses cases a <code>prediciton_key</code> setting must be added to the <code>tfma.ModelSpec</code> to identify the name of the key the output is stored under.</p>"},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#getting-started-with-tensorflow-model-analysis","title":"Getting Started with TensorFlow Model Analysis","text":""},{"location":"get_started/#overview","title":"Overview","text":"<p>TensorFlow Model Analysis (TFMA) is a library for performing model evaluation.</p> <ul> <li>For: Machine Learning Engineers or Data Scientists</li> <li>who: want to analyze and understand their TensorFlow models</li> <li>it is: a standalone library or component of a TFX pipeline</li> <li>that: evaluates models on large amounts of data in a distributed manner     on the same metrics defined in training. These metrics are compared over     slices of data, and visualized in Jupyter or Colab notebooks.</li> <li>unlike: some model introspection tools like tensorboard that offer model     introspection</li> </ul> <p>TFMA performs its computations in a distributed manner over large amounts of data using Apache Beam. The following sections describe how to setup a basic TFMA evaluation pipeline. See architecture more details on the underlying implementation.</p> <p>If you just want to jump in and get started, check out our colab notebook.</p> <p>This page can also be viewed from tensorflow.org.</p>"},{"location":"get_started/#model-types-supported","title":"Model Types Supported","text":"<p>TFMA is designed to support tensorflow based models, but can be easily extended to support other frameworks as well. Historically, TFMA required an <code>EvalSavedModel</code> be created to use TFMA, but the latest version of TFMA supports multiple types of models depending on the user's needs. Setting up an EvalSavedModel should only be required if a <code>tf.estimator</code> based model is used and custom training time metrics are required.</p> <p>Note that because TFMA now runs based on the serving model, TFMA will no longer automatically evaluate metrics added at training time. The exception to this case is if a keras model is used since keras saves the metrics used alongside the saved model. However, if this is a hard requirement, the latest TFMA is backwards compatible such that an <code>EvalSavedModel</code> can still be run in a TFMA pipeline.</p> <p>The following table summarizes the models supported by default:</p> Model Type Training Time Metrics Post Training Metrics TF2 (keras) Y* Y TF2 (generic) N/A Y EvalSavedModel (estimator) Y Y None (pd.DataFrame, etc) N/A Y <ul> <li>Training Time metrics refers to metrics defined at training time and saved     with the model (either TFMA EvalSavedModel or keras saved model). Post     training metrics refers to metrics added via <code>tfma.MetricConfig</code>.</li> <li>Generic TF2 models are custom models that export signatures that can be used     for inference and are not based on either keras or estimator.</li> </ul> <p>Note: Only training time metrics added via model.compile (not model.add_metric) are currently supported for keras.</p> <p>Note: When supported, training time metrics are enabled by default. However, they can be disabled via the <code>include_default_metrics</code> setting in <code>tfma.Options</code>.</p> <p>Note: To run with an <code>EvalSavedModel</code>, just set <code>signature_name: \"eval\"</code> in the model spec.</p> <p>See FAQ for more information on how to setup and configure these different model types.</p>"},{"location":"get_started/#setup","title":"Setup","text":"<p>Before running an evaluation, a small amount of setup is required. First, a <code>tfma.EvalConfig</code> object must be defined that provides specifications for the model, metrics, and slices that are to be evaluated. Second a <code>tfma.EvalSharedModel</code> needs to be created that points to the actual model (or models) to be used during the evaluation. Once these have been defined, evaluation is performed by calling <code>tfma.run_model_analysis</code> with an appropriate dataset. For more details, see the setup guide.</p> <p>If running within a TFX pipeline, see the TFX guide for how to configure TFMA to run as a TFX Evaluator component.</p>"},{"location":"get_started/#examples","title":"Examples","text":""},{"location":"get_started/#single-model-evaluation","title":"Single Model Evaluation","text":"<p>The following uses <code>tfma.run_model_analysis</code> to perform evaluation on a serving model. For an explanation of the different settings needed see the setup guide.</p> <p>Note: To run with an <code>EvalSavedModel</code>, just set <code>signature_name: \"eval\"</code> in the <code>model_spec</code>.</p> <p>Note: this uses Beam's local runner which is mainly for local, small-scale experimentation.</p> <pre><code># Run in a Jupyter Notebook.\nfrom google.protobuf import text_format\n\neval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # This assumes a serving model with a \"serving_default\" signature.\n    label_key: \"label\"\n    example_weight_key: \"weight\"\n  }\n  ## Post export metric information\n  metrics_specs {\n    # This adds AUC as a post training metric. If the model has built in\n    # training metrics which also contains AUC, this metric will replace it.\n    metrics { class_name: \"AUC\" }\n    # ... other post training metrics ...\n\n    # Plots are also configured here...\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"age\"]\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path='/path/to/saved/model', eval_config=eval_config)\n\neval_result = tfma.run_model_analysis(\n    eval_shared_model=eval_shared_model,\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location='/path/to/file/containing/tfrecords',\n    output_path='/path/for/output')\n\ntfma.view.render_slicing_metrics(eval_result)\n</code></pre> <p>For distributed evaluation, construct an Apache Beam pipeline using a distributed runner. In the pipeline, use the <code>tfma.ExtractEvaluateAndWriteResults</code> for evaluation and to write out the results. The results can be loaded for visualization using <code>tfma.load_eval_result</code>.</p> <p>For example:</p> <pre><code># To run the pipeline.\nfrom google.protobuf import text_format\nfrom tfx_bsl.tfxio import tf_example_record\n\neval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # This assumes a serving model with a \"serving_default\" signature.\n    label_key: \"label\"\n    example_weight_key: \"weight\"\n  }\n  ## Post export metric information\n  metrics_specs {\n    # This adds AUC and as a post training metric. If the model has built in\n    # training metrics which also contains AUC, this metric will replace it.\n    metrics { class_name: \"AUC\" }\n    # ... other post training metrics ...\n\n    # Plots are also configured here...\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"age\"]\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path='/path/to/saved/model', eval_config=eval_config)\n\noutput_path = '/path/for/output'\n\ntfx_io = tf_example_record.TFExampleRecord(\n    file_pattern=data_location, raw_record_column_name=tfma.ARROW_INPUT_COLUMN)\n\nwith beam.Pipeline(runner=...) as p:\n  _ = (p\n       # You can change the source as appropriate, e.g. read from BigQuery.\n       # This assumes your data is a TFRecords file containing records in the\n       # tf.train.Example format. If using EvalSavedModel then use the following\n       # instead: 'ReadData' &gt;&gt; beam.io.ReadFromTFRecord(file_pattern=...)\n       | 'ReadData' &gt;&gt; tfx_io.BeamSource()\n       | 'ExtractEvaluateAndWriteResults' &gt;&gt;\n       tfma.ExtractEvaluateAndWriteResults(\n            eval_shared_model=eval_shared_model,\n            eval_config=eval_config,\n            output_path=output_path))\n\n# To load and visualize results.\n# Note that this code should be run in a Jupyter Notebook.\nresult = tfma.load_eval_result(output_path)\ntfma.view.render_slicing_metrics(result)\n</code></pre>"},{"location":"get_started/#model-validation","title":"Model Validation","text":"<p>To perform model validation against a candiate and baseline, update the config to include a threshold setting and pass two models to <code>tfma.run_model_analysis</code>.</p> <p>For example:</p> <pre><code># Run in a Jupyter Notebook.\nfrom google.protobuf import text_format\n\neval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # This assumes a serving model with a \"serving_default\" signature.\n    label_key: \"label\"\n    example_weight_key: \"weight\"\n  }\n  ## Post export metric information\n  metrics_specs {\n    # This adds AUC and as a post training metric. If the model has built in\n    # training metrics which also contains AUC, this metric will replace it.\n    metrics {\n      class_name: \"AUC\"\n      threshold {\n        value_threshold {\n          lower_bound { value: 0.9 }\n        }\n        change_threshold {\n          direction: HIGHER_IS_BETTER\n          absolute { value: -1e-10 }\n        }\n      }\n    }\n    # ... other post training metrics ...\n\n    # Plots are also configured here...\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"age\"]\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_models = [\n  tfma.default_eval_shared_model(\n      model_name=tfma.CANDIDATE_KEY,\n      eval_saved_model_path='/path/to/saved/candiate/model',\n      eval_config=eval_config),\n  tfma.default_eval_shared_model(\n      model_name=tfma.BASELINE_KEY,\n      eval_saved_model_path='/path/to/saved/baseline/model',\n      eval_config=eval_config),\n]\n\noutput_path = '/path/for/output'\n\neval_result = tfma.run_model_analysis(\n    eval_shared_models,\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location='/path/to/file/containing/tfrecords',\n    output_path=output_path)\n\ntfma.view.render_slicing_metrics(eval_result)\ntfma.load_validation_result(output_path)\n</code></pre>"},{"location":"get_started/#visualization","title":"Visualization","text":"<p>TFMA evaluation results can be visualized in a Jupyter notebook using the frontend components included in TFMA. For example:</p> <p>.</p>"},{"location":"get_started/#more-information","title":"More Information","text":"<ul> <li>Install</li> <li>Setup</li> <li>Metrics and Plots</li> <li>Model Validations</li> <li>Visualizations</li> <li>Architecture</li> <li>FAQ</li> <li>API Reference</li> </ul>"},{"location":"install/","title":"TensorFlow Model Analysis","text":"<p>TensorFlow Model Analysis (TFMA) is a library for evaluating TensorFlow models. It allows users to evaluate their models on large amounts of data in a distributed manner, using the same metrics defined in their trainer. These metrics can be computed over different slices of data and visualized in Jupyter notebooks.</p> <p></p> <p>Caution: TFMA may introduce backwards incompatible changes before version 1.0.</p>"},{"location":"install/#installation","title":"Installation","text":"<p>The recommended way to install TFMA is using the PyPI package:</p> <pre>\npip install tensorflow-model-analysis\n</pre>"},{"location":"install/#build-tfma-from-source","title":"Build TFMA from source","text":"<p>To build from source follow the following steps:</p> <p>Install the protoc as per the link mentioned: protoc</p> <p>Create a virtual environment by running the commands</p> <pre><code>python3 -m venv &lt;virtualenv_name&gt;\nsource &lt;virtualenv_name&gt;/bin/activate\npip3 install setuptools wheel\ngit clone https://github.com/tensorflow/model-analysis.git\ncd model-analysis\npython3 setup.py bdist_wheel\n</code></pre> <p>This will build the TFMA wheel in the dist directory. To install the wheel from dist directory run the commands</p> <pre><code>cd dist\npip3 install tensorflow_model_analysis-&lt;version&gt;-py3-none-any.whl\n</code></pre>"},{"location":"install/#nightly-packages","title":"Nightly Packages","text":"<p>TFMA also hosts nightly packages at https://pypi-nightly.tensorflow.org on Google Cloud. To install the latest nightly package, please use the following command:</p> <pre>\npip install -i https://pypi-nightly.tensorflow.org/simple tensorflow-model-analysis\n</pre> <p>This will install the nightly packages for the major dependencies of TFMA such as TensorFlow Metadata (TFMD), TFX Basic Shared Libraries (TFX-BSL).</p> <p>Note: These nightly packages are unstable and breakages are likely to happen. The fix could often take a week or more depending on the complexity involved.</p> <p>Currently, TFMA requires that TensorFlow is installed but does not have an explicit dependency on the TensorFlow PyPI package. See the TensorFlow install guides for instructions.</p> <p>To enable TFMA visualization in Jupyter Notebook:</p> <pre>\n  <code>jupyter nbextension enable --py widgetsnbextension</code>\n  <code>jupyter nbextension enable --py tensorflow_model_analysis</code>\n</pre> <p>Note: If Jupyter notebook is already installed in your home directory, add <code>--user</code> to these commands. If Jupyter is installed as root, or using a virtual environment, the parameter <code>--sys-prefix</code> might be required.</p>"},{"location":"install/#jupyter-lab","title":"Jupyter Lab","text":"<p>As of writing, because of https://github.com/pypa/pip/issues/9187, <code>pip install</code> might never finish. In that case, you should revert pip to version 19 instead of 20: <code>pip install \"pip&lt;20\"</code>.</p> <p>Using a JupyterLab extension requires installing dependencies on the command line. You can do this within the console in the JupyterLab UI or on the command line. This includes separately installing any pip package dependencies and JupyterLab labextension plugin dependencies, and the version numbers must be compatible.</p> <p>The examples below use 0.27.0. Check available versions below to use the latest.</p>"},{"location":"install/#jupyter-lab-12x","title":"Jupyter Lab 1.2.x","text":"<pre><code>pip install tensorflow_model_analysis==0.27.0\n\njupyter labextension install tensorflow_model_analysis@0.27.0\n\njupyter labextension install @jupyter-widgets/jupyterlab-manager@1.1\n</code></pre>"},{"location":"install/#jupyter-lab-2","title":"Jupyter Lab 2","text":"<pre><code>pip install tensorflow_model_analysis==0.27.0\n\njupyter labextension install tensorflow_model_analysis@0.27.0\n\njupyter labextension install @jupyter-widgets/jupyterlab-manager@2\n</code></pre>"},{"location":"install/#troubleshooting","title":"Troubleshooting","text":"<p>Check pip packages:</p> <pre><code>pip list\n</code></pre> <p>Check extensions:</p> <pre><code>jupyter labextension list\n</code></pre>"},{"location":"install/#notable-dependencies","title":"Notable Dependencies","text":"<p>TensorFlow is required.</p> <p>Apache Beam is required; it's the way that efficient distributed computation is supported. By default, Apache Beam runs in local mode but can also run in distributed mode using Google Cloud Dataflow and other Apache Beam runners.</p> <p>Apache Arrow is also required. TFMA uses Arrow to represent data internally in order to make use of vectorized numpy functions.</p>"},{"location":"install/#getting-started","title":"Getting Started","text":"<p>For instructions on using TFMA, see the get started guide.</p>"},{"location":"install/#compatible-versions","title":"Compatible Versions","text":"<p>The following table is the TFMA package versions that are compatible with each other. This is determined by our testing framework, but other untested combinations may also work.</p> tensorflow-model-analysis apache-beam[gcp] pyarrow tensorflow tensorflow-metadata tfx-bsl GitHub master 2.65.0 10.0.1 nightly (2.x) 1.17.1 1.17.1 0.48.0 2.65.0 10.0.1 2.17 1.17.1 1.17.1 0.47.1 2.60.0 10.0.1 2.16 1.16.1 1.16.1 0.47.0 2.60.0 10.0.1 2.16 1.16.1 1.16.1 0.46.0 2.47.0 10.0.0 2.15 1.15.0 1.15.1 0.45.0 2.47.0 10.0.0 2.13 1.14.0 1.14.0 0.44.0 2.40.0 6.0.0 2.12 1.13.1 1.13.0 0.43.0 2.40.0 6.0.0 2.11 1.12.0 1.12.0 0.42.0 2.40.0 6.0.0 1.15 / 2.10 1.11.0 1.11.0 0.41.1 2.40.0 6.0.0 1.15 / 2.9 1.10.0 1.10.1 0.41.0 2.40.0 6.0.0 1.15 / 2.9 1.10.0 1.10.1 0.40.0 2.38.0 5.0.0 1.15 / 2.9 1.9.0 1.9.0 0.39.0 2.38.0 5.0.0 1.15 / 2.8 1.8.0 1.8.0 0.38.0 2.36.0 5.0.0 1.15 / 2.8 1.7.0 1.7.0 0.37.0 2.35.0 5.0.0 1.15 / 2.7 1.6.0 1.6.0 0.36.0 2.34.0 5.0.0 1.15 / 2.7 1.5.0 1.5.0 0.35.0 2.33.0 5.0.0 1.15 / 2.6 1.4.0 1.4.0 0.34.1 2.32.0 2.0.0 1.15 / 2.6 1.2.0 1.3.0 0.34.0 2.31.0 2.0.0 1.15 / 2.6 1.2.0 1.3.1 0.33.0 2.31.0 2.0.0 1.15 / 2.5 1.2.0 1.2.0 0.32.1 2.29.0 2.0.0 1.15 / 2.5 1.1.0 1.1.1 0.32.0 2.29.0 2.0.0 1.15 / 2.5 1.1.0 1.1.0 0.31.0 2.29.0 2.0.0 1.15 / 2.5 1.0.0 1.0.0 0.30.0 2.28.0 2.0.0 1.15 / 2.4 0.30.0 0.30.0 0.29.0 2.28.0 2.0.0 1.15 / 2.4 0.29.0 0.29.0 0.28.0 2.28.0 2.0.0 1.15 / 2.4 0.28.0 0.28.0 0.27.0 2.27.0 2.0.0 1.15 / 2.4 0.27.0 0.27.0 0.26.1 2.28.0 0.17.0 1.15 / 2.3 0.26.0 0.26.0 0.26.0 2.25.0 0.17.0 1.15 / 2.3 0.26.0 0.26.0 0.25.0 2.25.0 0.17.0 1.15 / 2.3 0.25.0 0.25.0 0.24.3 2.24.0 0.17.0 1.15 / 2.3 0.24.0 0.24.1 0.24.2 2.23.0 0.17.0 1.15 / 2.3 0.24.0 0.24.0 0.24.1 2.23.0 0.17.0 1.15 / 2.3 0.24.0 0.24.0 0.24.0 2.23.0 0.17.0 1.15 / 2.3 0.24.0 0.24.0 0.23.0 2.23.0 0.17.0 1.15 / 2.3 0.23.0 0.23.0 0.22.2 2.20.0 0.16.0 1.15 / 2.2 0.22.2 0.22.0 0.22.1 2.20.0 0.16.0 1.15 / 2.2 0.22.2 0.22.0 0.22.0 2.20.0 0.16.0 1.15 / 2.2 0.22.0 0.22.0 0.21.6 2.19.0 0.15.0 1.15 / 2.1 0.21.0 0.21.3 0.21.5 2.19.0 0.15.0 1.15 / 2.1 0.21.0 0.21.3 0.21.4 2.19.0 0.15.0 1.15 / 2.1 0.21.0 0.21.3 0.21.3 2.17.0 0.15.0 1.15 / 2.1 0.21.0 0.21.0 0.21.2 2.17.0 0.15.0 1.15 / 2.1 0.21.0 0.21.0 0.21.1 2.17.0 0.15.0 1.15 / 2.1 0.21.0 0.21.0 0.21.0 2.17.0 0.15.0 1.15 / 2.1 0.21.0 0.21.0 0.15.4 2.16.0 0.15.0 1.15 / 2.0 n/a 0.15.1 0.15.3 2.16.0 0.15.0 1.15 / 2.0 n/a 0.15.1 0.15.2 2.16.0 0.15.0 1.15 / 2.0 n/a 0.15.1 0.15.1 2.16.0 0.15.0 1.15 / 2.0 n/a 0.15.0 0.15.0 2.16.0 0.15.0 1.15 n/a n/a 0.14.0 2.14.0 n/a 1.14 n/a n/a 0.13.1 2.11.0 n/a 1.13 n/a n/a 0.13.0 2.11.0 n/a 1.13 n/a n/a 0.12.1 2.10.0 n/a 1.12 n/a n/a 0.12.0 2.10.0 n/a 1.12 n/a n/a 0.11.0 2.8.0 n/a 1.11 n/a n/a 0.9.2 2.6.0 n/a 1.9 n/a n/a 0.9.1 2.6.0 n/a 1.10 n/a n/a 0.9.0 2.5.0 n/a 1.9 n/a n/a 0.6.0 2.4.0 n/a 1.6 n/a n/a"},{"location":"install/#questions","title":"Questions","text":"<p>Please direct any questions about working with TFMA to Stack Overflow using the tensorflow-model-analysis tag.</p>"},{"location":"metrics/","title":"Tensorflow Model Analysis Metrics and Plots","text":""},{"location":"metrics/#overview","title":"Overview","text":"<p>TFMA supports the following metrics and plots:</p> <ul> <li>Standard keras metrics     (<code>tf.keras.metrics.*</code>)<ul> <li>Note that you do not need a keras model to use keras metrics. Metrics     are computed outside of the graph in beam using the metrics classes     directly.</li> </ul> </li> <li> <p>Standard TFMA metrics and plots     (<code>tfma.metrics.*</code>)</p> </li> <li> <p>Custom keras metrics (metrics derived from     <code>tf.keras.metrics.Metric</code>)</p> </li> <li> <p>Custom TFMA metrics (metrics derived from     <code>tfma.metrics.Metric</code>)     using custom beam combiners or metrics derived from other metrics).</p> </li> </ul> <p>NOTE: In TFMA, plots and metrics are both defined under the metrics library. By convention the classes related to plots end in <code>Plot</code>.</p> <p>TFMA also provides built-in support for converting binary classification metrics for use with multi-class/multi-label problems:</p> <ul> <li>Binarization based on class ID, top K, etc.</li> <li>Aggregated metrics based on micro averaging, macro averaging, etc.</li> </ul> <p>TFMA also provides built-in support for query/ranking based metrics where the examples are grouped by a query key automatically in the pipeline.</p> <p>Combined there are over 50+ standard metrics and plots available for a variety of problems including regression, binary classification, multi-class/multi-label classification, ranking, etc.</p>"},{"location":"metrics/#configuration","title":"Configuration","text":"<p>There are two ways to configure metrics in TFMA: (1) using the <code>tfma.MetricsSpec</code> or (2) by creating instances of <code>tf.keras.metrics.*</code> and/or <code>tfma.metrics.*</code> classes in python and using <code>tfma.metrics.specs_from_metrics</code> to convert them to a list of <code>tfma.MetricsSpec</code>.</p> <p>The following sections describe example configurations for different types of machine learning problems.</p>"},{"location":"metrics/#regression-metrics","title":"Regression Metrics","text":"<p>The following is an example configuration setup for a regression problem. Consult the <code>tf.keras.metrics.*</code> and <code>tfma.metrics.*</code> modules for possible additional metrics supported.</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"MeanSquaredError\" }\n    metrics { class_name: \"Accuracy\" }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics {\n      class_name: \"CalibrationPlot\"\n      config: '\"min_value\": 0, \"max_value\": 10'\n    }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    tfma.metrics.ExampleCount(name='example_count'),\n    tf.keras.metrics.MeanSquaredError(name='mse'),\n    tf.keras.metrics.Accuracy(name='accuracy'),\n    tfma.metrics.MeanLabel(name='mean_label'),\n    tfma.metrics.MeanPrediction(name='mean_prediction'),\n    tfma.metrics.Calibration(name='calibration'),\n    tfma.metrics.CalibrationPlot(\n        name='calibration', min_value=0, max_value=10)\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n</code></pre> <p>Note that this setup is also avaliable by calling <code>tfma.metrics.default_regression_specs</code>.</p>"},{"location":"metrics/#binary-classification-metrics","title":"Binary Classification Metrics","text":"<p>The following is an example configuration setup for a binary classification problem. Consult the <code>tf.keras.metrics.*</code> and <code>tfma.metrics.*</code> modules for possible additional metrics supported.</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"BinaryCrossentropy\" }\n    metrics { class_name: \"BinaryAccuracy\" }\n    metrics { class_name: \"AUC\" }\n    metrics { class_name: \"AUCPrecisionRecall\" }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    metrics { class_name: \"CalibrationPlot\" }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    tfma.metrics.ExampleCount(name='example_count'),\n    tf.keras.metrics.BinaryCrossentropy(name='binary_crossentropy'),\n    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    tf.keras.metrics.AUC(\n        name='auc_precision_recall', curve='PR', num_thresholds=10000),\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall'),\n    tfma.metrics.MeanLabel(name='mean_label'),\n    tfma.metrics.MeanPrediction(name='mean_prediction'),\n    tfma.metrics.Calibration(name='calibration'),\n    tfma.metrics.ConfusionMatrixPlot(name='confusion_matrix_plot'),\n    tfma.metrics.CalibrationPlot(name='calibration_plot')\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n</code></pre> <p>Note that this setup is also avaliable by calling <code>tfma.metrics.default_binary_classification_specs</code>.</p>"},{"location":"metrics/#multi-classmulti-label-classification-metrics","title":"Multi-class/Multi-label Classification Metrics","text":"<p>The following is an example configuration setup for a multi-class classification problem. Consult the <code>tf.keras.metrics.*</code> and <code>tfma.metrics.*</code> modules for possible additional metrics supported.</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"SparseCategoricalCrossentropy\" }\n    metrics { class_name: \"SparseCategoricalAccuracy\" }\n    metrics { class_name: \"Precision\" config: '\"top_k\": 1' }\n    metrics { class_name: \"Precision\" config: '\"top_k\": 3' }\n    metrics { class_name: \"Recall\" config: '\"top_k\": 1' }\n    metrics { class_name: \"Recall\" config: '\"top_k\": 3' }\n    metrics { class_name: \"MultiClassConfusionMatrixPlot\" }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>Note: For multi-label there is <code>MultiLabelConfusionMatrixPlot</code> instead of <code>MultiClassConfusionMatrixPlot</code></p> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    tfma.metrics.ExampleCount(name='example_count'),\n    tf.keras.metrics.SparseCategoricalCrossentropy(\n        name='sparse_categorical_crossentropy'),\n    tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n    tf.keras.metrics.Precision(name='precision', top_k=1),\n    tf.keras.metrics.Precision(name='precision', top_k=3),\n    tf.keras.metrics.Recall(name='recall', top_k=1),\n    tf.keras.metrics.Recall(name='recall', top_k=3),\n    tfma.metrics.MultiClassConfusionMatrixPlot(\n        name='multi_class_confusion_matrix_plot'),\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n</code></pre> <p>Note that this setup is also avaliable by calling <code>tfma.metrics.default_multi_class_classification_specs</code>.</p>"},{"location":"metrics/#multi-classmulti-label-binarized-metrics","title":"Multi-class/Multi-label Binarized Metrics","text":"<p>Multi-class/multi-label metrics can be binarized to produce metrics per class, per top_k, etc using the <code>tfma.BinarizationOptions</code>. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    binarize: { class_ids: { values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] } }\n    // Metrics to binarize\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    // Metrics to binarize\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, binarize=tfma.BinarizationOptions(\n        class_ids={'values': [0,1,2,3,4,5,6,7,8,9]}))\n</code></pre>"},{"location":"metrics/#multi-classmulti-label-aggregate-metrics","title":"Multi-class/Multi-label Aggregate Metrics","text":"<p>Multi-class/multi-label metrics can be aggregated to produce a single aggregated value for a binary classification metric by using <code>tfma.AggregationOptions</code>.</p> <p>Note that aggregation settings are independent of binarization settings so you can use both <code>tfma.AggregationOptions</code> and <code>tfma.BinarizationOptions</code> at the same time.</p>"},{"location":"metrics/#micro-average","title":"Micro Average","text":"<p>Micro averaging can be performed by using the <code>micro_average</code> option within <code>tfma.AggregationOptions</code>. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: { micro_average: true }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, aggregate=tfma.AggregationOptions(micro_average=True))\n</code></pre> <p>Micro averaging also supports setting <code>top_k</code> where only the top k values are used in the computation. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: {\n      micro_average: true\n      top_k_list: { values: [1, 3] }\n    }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics,\n    aggregate=tfma.AggregationOptions(micro_average=True,\n                                      top_k_list={'values': [1, 3]}))\n</code></pre>"},{"location":"metrics/#macro-weighted-macro-average","title":"Macro / Weighted Macro Average","text":"<p>Macro averaging can be performed by using the <code>macro_average</code> or <code>weighted_macro_average</code> options within <code>tfma.AggregationOptions</code>. Unless <code>top_k</code> settings are used, macro requires setting the <code>class_weights</code> in order to know which classes to compute the average for. If a <code>class_weight</code> is not provided then 0.0 is assumed. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: {\n      macro_average: true\n      class_weights: { key: 0 value: 1.0 }\n      class_weights: { key: 1 value: 1.0 }\n      class_weights: { key: 2 value: 1.0 }\n      class_weights: { key: 3 value: 1.0 }\n      class_weights: { key: 4 value: 1.0 }\n      class_weights: { key: 5 value: 1.0 }\n      class_weights: { key: 6 value: 1.0 }\n      class_weights: { key: 7 value: 1.0 }\n      class_weights: { key: 8 value: 1.0 }\n      class_weights: { key: 9 value: 1.0 }\n    }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics,\n    aggregate=tfma.AggregationOptions(\n        macro_average=True, class_weights={i: 1.0 for i in range(10)}))\n</code></pre> <p>Like micro averaging, macro averaging also supports setting <code>top_k</code> where only the top k values are used in the computation. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: {\n      macro_average: true\n      top_k_list: { values: [1, 3] }\n    }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics,\n    aggregate=tfma.AggregationOptions(macro_average=True,\n                                      top_k_list={'values': [1, 3]}))\n</code></pre>"},{"location":"metrics/#query-ranking-based-metrics","title":"Query / Ranking Based Metrics","text":"<p>Query/ranking based metrics are enabled by specifying the <code>query_key</code> option in the metrics specs. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    query_key: \"doc_id\"\n    metrics {\n      class_name: \"NDCG\"\n      config: '\"gain_key\": \"gain\", \"top_k_list\": [1, 2]'\n    }\n    metrics { class_name: \"MinLabelPosition\" }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>This same setup can be created using the following python code:</p> <pre><code>metrics = [\n    tfma.metrics.NDCG(name='ndcg', gain_key='gain', top_k_list=[1, 2]),\n    tfma.metrics.MinLabelPosition(name='min_label_position')\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics, query_key='doc_id')\n</code></pre>"},{"location":"metrics/#multi-model-evaluation-metrics","title":"Multi-model Evaluation Metrics","text":"<p>TFMA supports evaluating multiple models at the same time. When multi-model evaluation is performed, metrics will be calculated for each model. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    # no model_names means all models\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>If metrics need to be computed for a subset of models, set <code>model_names</code> in the <code>metric_specs</code>. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    model_names: [\"my-model1\"]\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>The <code>specs_from_metrics</code> API also supports passing model names:</p> <pre><code>metrics = [\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, model_names=['my-model1'])\n</code></pre>"},{"location":"metrics/#model-comparison-metrics","title":"Model Comparison Metrics","text":"<p>TFMA supports evaluating comparison metrics for a candidate model against a baseline model. A simple way to setup the candidate and baseline model pair is to pass along a eval_shared_model with the proper model names (tfma.BASELINE_KEY and tfma.CANDIDATE_KEY):</p> <pre><code>eval_config = text_format.Parse(\"\"\"\n  model_specs {\n    # ... model_spec without names ...\n  }\n  metrics_spec {\n    # ... metrics ...\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_models = [\n  tfma.default_eval_shared_model(\n      model_name=tfma.CANDIDATE_KEY,\n      eval_saved_model_path='/path/to/saved/candidate/model',\n      eval_config=eval_config),\n  tfma.default_eval_shared_model(\n      model_name=tfma.BASELINE_KEY,\n      eval_saved_model_path='/path/to/saved/baseline/model',\n      eval_config=eval_config),\n]\n\neval_result = tfma.run_model_analysis(\n    eval_shared_models,\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location=\"/path/to/file/containing/tfrecords\",\n    output_path=\"/path/for/output\")\n</code></pre> <p>Comparison metrics are computed automatically for all of the diff-able metrics (currently only scalar value metrics such as accuracy and AUC).</p>"},{"location":"metrics/#multi-output-model-metrics","title":"Multi-output Model Metrics","text":"<p>TFMA supports evaluating metrics on models that have different outputs. Multi-output models store their output predictions in the form of a dict keyed by output name. When multi-output model's are used, the names of the outputs associated with a set of metrics must be specified in the <code>output_names</code> section of the MetricsSpec. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    output_names: [\"my-output\"]\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>The <code>specs_from_metrics</code> API also supports passing output names:</p> <pre><code>metrics = [\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, output_names=['my-output'])\n</code></pre>"},{"location":"metrics/#customizing-metric-settings","title":"Customizing Metric Settings","text":"<p>TFMA allows customizing of the settings that are used with different metrics. For example you might want to change the name, set thresholds, etc. This is done by adding a <code>config</code> section to the metric config. The config is specified using the JSON string version of the parameters that would be passed to the metrics <code>__init__</code> method (for ease of use the leading and trailing '{' and '}' brackets may be omitted). For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics {\n      class_name: \"ConfusionMatrixAtThresholds\"\n      config: '\"thresholds\": [0.3, 0.5, 0.8]'\n    }\n  }\n\"\"\", tfma.MetricsSpec()).metrics_specs\n</code></pre> <p>This customization is of course also supported directly:</p> <pre><code>metrics = [\n   tfma.metrics.ConfusionMatrixAtThresholds(thresholds=[0.3, 0.5, 0.8]),\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n</code></pre> <p>NOTE: It is advisable to set the default number of thresholds used with AUC, etc to 10000 because this is the default value used by the underlying histogram calcuation which is shared between multiple metric implementations.</p>"},{"location":"metrics/#outputs","title":"Outputs","text":"<p>The output of a metric evaluation is a series of metric keys/values and/or plot keys/values based on the configuration used.</p>"},{"location":"metrics/#metric-keys","title":"Metric Keys","text":"<p>MetricKeys are defined using a structured key type. This key uniquely identifies each of the following aspects of a metric:</p> <ul> <li>Metric name (<code>auc</code>, <code>mean_label</code>, etc)</li> <li>Model name (only used if multi-model evaluation)</li> <li>Output name (only used if multi-output models are evaluated)</li> <li>Sub key (e.g. class ID if multi-class model is binarized)</li> </ul>"},{"location":"metrics/#metric-value","title":"Metric Value","text":"<p>MetricValues are defined using a proto that encapulates the different value types supported by the different metrics (e.g. <code>double</code>, <code>ConfusionMatrixAtThresholds</code>, etc).</p> <p>Below are the supported metric value types:</p> <ul> <li><code>double_value</code> -     A wrapper for a double type.</li> <li><code>bytes_value</code> -     A bytes value.</li> <li><code>bounded_value</code> - Represents a real value which could be a pointwise     estimate, optionally with approximate bounds of some sort. Has properties     <code>value</code>, <code>lower_bound</code>, and <code>upper_bound</code>.</li> <li><code>value_at_cutoffs</code> - Value at cutoffs (e.g. precision@K, recall@K). Has     property <code>values</code>, each of which has properties <code>cutoff</code> and <code>value</code>.</li> <li><code>confusion_matrix_at_thresholds</code> - Confusion matrix at thresholds. Has     property <code>matrices</code>, each of which has properties for <code>threshold</code>,     <code>precision</code>, <code>recall</code>, and confusion matrix values such as     <code>false_negatives</code>.</li> <li><code>array_value</code> - For metrics which return an array of values.</li> </ul>"},{"location":"metrics/#plot-keys","title":"Plot Keys","text":"<p>PlotKeys are similar to metric keys except that for historical reasons all the plots values are stored in a single proto so the plot key does not have a name.</p>"},{"location":"metrics/#plot-values","title":"Plot Values","text":"<p>All the supported plots are stored in a single proto called PlotData.</p>"},{"location":"metrics/#evalresult","title":"EvalResult","text":"<p>The return from an evaluation run is an <code>tfma.EvalResult</code>. This record contains <code>slicing_metrics</code> that encode the metric key as a multi-level dict where the levels correspond to output name, class ID, metric name, and metric value respectively. This is intended to be used for UI display in a Jupiter notebook. If access to the underlying data is needed the <code>metrics</code> result file should be used instead (see metrics_for_slice.proto).</p>"},{"location":"metrics/#customization","title":"Customization","text":"<p>In addition to custom metrics that are added as part of a saved keras (or legacy EvalSavedModel). There are two ways to customize metrics in TFMA post saving: (1) by defining a custom keras metric class and (2) by defining a custom TFMA metrics class backed by a beam combiner.</p> <p>In both cases, the metrics are configured by specifying the name of the metric class and associated module. For example:</p> <pre><code>from google.protobuf import text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"MyMetric\" module: \"my.module\"}\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n</code></pre> <p>NOTE: When customizing metrics you must ensure that the module is available to beam.</p>"},{"location":"metrics/#custom-keras-metrics","title":"Custom Keras Metrics","text":"<p>To create a custom keras metric, users need to extend <code>tf.keras.metrics.Metric</code> with their implementation and then make sure the metric's module is available at evaluation time.</p> <p>Note that for metrics added post model save, TFMA only supports metrics that take label (i.e. y_true), prediction (y_pred), and example weight (sample_weight) as parameters to the <code>update_state</code> method.</p>"},{"location":"metrics/#keras-metric-example","title":"Keras Metric Example","text":"<p>The following is an example of a custom keras metric:</p> <pre><code>class MyMetric(tf.keras.metrics.Mean):\n\n  def __init__(self, name='my_metric', dtype=None):\n    super(MyMetric, self).__init__(name=name, dtype=dtype)\n\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    return super(MyMetric, self).update_state(\n        y_pred, sample_weight=sample_weight)\n</code></pre>"},{"location":"metrics/#custom-tfma-metrics","title":"Custom TFMA Metrics","text":"<p>To create a custom TFMA metric, users need to extend <code>tfma.metrics.Metric</code> with their implementation and then make sure the metric's module is available at evaluation time.</p>"},{"location":"metrics/#metric","title":"Metric","text":"<p>A <code>tfma.metrics.Metric</code> implementation is made up of a set of kwargs that define the metrics configuration along with a function for creating the computations (possibly multiple) needed to calcuate the metrics value. There are two main computation types that can be used: <code>tfma.metrics.MetricComputation</code> and <code>tfma.metrics.DerivedMetricComputation</code> that are described in the sections below. The function that creates these computations will be passed the following parameters as input:</p> <ul> <li><code>eval_config: tfam.EvalConfig</code><ul> <li>The eval config passed to the evaluator (useful for looking up model     spec settings such as prediction key to use, etc).</li> </ul> </li> <li><code>model_names: List[Text]</code><ul> <li>List of model names to compute metrics for (None if single-model)</li> </ul> </li> <li><code>output_names: List[Text]</code>.<ul> <li>List of output names to compute metrics for (None if single-model)</li> </ul> </li> <li><code>sub_keys: List[tfma.SubKey]</code>.<ul> <li>List of sub keys (class ID, top K, etc) to compute metrics for (or None)</li> </ul> </li> <li><code>aggregation_type: tfma.AggregationType</code><ul> <li>Type of aggregation if computing an aggregation metric.</li> </ul> </li> <li><code>class_weights: Dict[int, float]</code>.<ul> <li>Class weights to use if computing an aggregation metric.</li> </ul> </li> <li><code>query_key: Text</code><ul> <li>Query key used if computing a query/ranking based metric.</li> </ul> </li> </ul> <p>If a metric is not associated with one or more of these settings then it may leave those parameters out of its signature definition.</p> <p>If a metric is computed the same way for each model, output, and sub key, then the utility <code>tfma.metrics.merge_per_key_computations</code> can be used to perform the same computations for each of these inputs separately.</p>"},{"location":"metrics/#metriccomputation","title":"MetricComputation","text":"<p>A <code>MetricComputation</code> is made up of a combination of <code>preprocessors</code> and a <code>combiner</code>. The <code>preprocessors</code> is a list of <code>preprocessor</code>, which is a <code>beam.DoFn</code> that takes extracts as its input and outputs the initial state that will be used by the combiner (see architecture for more info on what are extracts). All preprocessors will be executed sequentially in the order of the list. If the <code>preprocessors</code> is empty, then the combiner will be passed StandardMetricInputs (standard metric inputs contains labels, predictions, and example_weights). The <code>combiner</code> is a <code>beam.CombineFn</code> that takes a tuple of (slice key, preprocessor output) as its input and outputs a tuple of (slice_key, metric results dict) as its result.</p> <p>Note that slicing happens between the <code>preprocessors</code> and <code>combiner</code>.</p> <p>Note that if a metric computation wants to make use of both the standard metric inputs, but augment it with a few of the features from the <code>features</code> extracts, then the special FeaturePreprocessor can be used which will merge the requested features from multiple combiners into a single shared StandardMetricsInputs value that is passed to all the combiners (the combiners are responsible for reading the features they are interested in and ignoring the rest).</p>"},{"location":"metrics/#example","title":"Example","text":"<p>The following is a very simple example of TFMA metric definition for computing the ExampleCount:</p> <pre><code>class ExampleCount(tfma.metrics.Metric):\n\n  def __init__(self, name: Text = 'example_count'):\n    super(ExampleCount, self).__init__(_example_count, name=name)\n\n\ndef _example_count(\n    name: Text = 'example_count') -&gt; tfma.metrics.MetricComputations:\n  key = tfma.metrics.MetricKey(name=name)\n  return [\n      tfma.metrics.MetricComputation(\n          keys=[key],\n          preprocessors=[_ExampleCountPreprocessor()],\n          combiner=_ExampleCountCombiner(key))\n  ]\n\n\nclass ExampleCountTest(tfma.test.testutil.TensorflowModelAnalysisTest):\n\n  def testExampleCount(self):\n    metric = ExampleCount()\n    computations = metric.computations(example_weighted=False)\n    computation = computations[0]\n\n    with beam.Pipeline() as pipeline:\n      result = (\n          pipeline\n          | 'Create' &gt;&gt; beam.Create([...])  # Add inputs\n          | 'PreProcess' &gt;&gt; beam.ParDo(computation.preprocessors[0])\n          | 'Process' &gt;&gt; beam.Map(tfma.metrics.to_standard_metric_inputs)\n          | 'AddSlice' &gt;&gt; beam.Map(lambda x: ((), x))\n          | 'ComputeMetric' &gt;&gt; beam.CombinePerKey(computation.combiner)\n      )\n\n      def check_result(got):\n        try:\n          self.assertLen(got, 1)\n          got_slice_key, got_metrics = got[0]\n          self.assertEqual(got_slice_key, ())\n          key = computation.keys[0]\n          self.assertIn(key, got_metrics)\n          self.assertAlmostEqual(got_metrics[key], expected_value, places=5)\n        except AssertionError as err:\n          raise util.BeamAssertException(err)\n\n      util.assert_that(result, check_result, label='result')\n\nclass _ExampleCountPreprocessor(beam.DoFn):\n\n  def process(self, extracts: tfma.Extracts) -&gt; Iterable[int]:\n    yield 1\n\n\nclass _ExampleCountPreprocessorTest(unittest.TestCase):\n\n  def testExampleCountPreprocessor(self):\n    ...  # Init the test case here\n    with beam.Pipeline() as pipeline:\n      updated_pcoll = (\n          pipeline\n          | 'Create' &gt;&gt; beam.Create([...])  # Add inputs\n          | 'Preprocess'\n          &gt;&gt; beam.ParDo(\n              _ExampleCountPreprocessor()\n          )\n      )\n\n      beam_testing_util.assert_that(\n          updated_pcoll,\n          lambda result: ...,  # Assert the test case\n      )\n\n\nclass _ExampleCountCombiner(beam.CombineFn):\n\n  def __init__(self, metric_key: tfma.metrics.MetricKey):\n    self._metric_key = metric_key\n\n  def create_accumulator(self) -&gt; int:\n    return 0\n\n  def add_input(self, accumulator: int, state: int) -&gt; int:\n    return accumulator + state\n\n  def merge_accumulators(self, accumulators: Iterable[int]) -&gt; int:\n    accumulators = iter(accumulators)\n    result = next(accumulator)\n    for accumulator in accumulators:\n      result += accumulator\n    return result\n\n  def extract_output(self,\n                     accumulator: int) -&gt; Dict[tfma.metrics.MetricKey, int]:\n    return {self._metric_key: accumulator}\n</code></pre>"},{"location":"metrics/#derivedmetriccomputation","title":"DerivedMetricComputation","text":"<p>A <code>DerivedMetricComputation</code> is made up of a result function that is used to calculate metric values based on the output of other metric computations. The result function takes a dict of computed values as its input and outputs a dict of additional metric results.</p> <p>Note that it is acceptable (recommended) to include the computations that a derived computation depends on in the list of computations created by a metric. This avoid having to pre-create and pass computations that are shared between multiple metrics. The evaluator will automatically de-dup computations that have the same definition so ony one computation is actually run.</p>"},{"location":"metrics/#example_1","title":"Example","text":"<p>The TJUR metrics provides a good example of derived metrics.</p>"},{"location":"model_validations/","title":"Tensorflow Model Analysis Model Validations","text":""},{"location":"model_validations/#overview","title":"Overview","text":"<p>TFMA supports validating a model by setting up value thresholds and change thresholds based on the supported metrics.</p>"},{"location":"model_validations/#configuration","title":"Configuration","text":""},{"location":"model_validations/#genericvaluethreshold","title":"GenericValueThreshold","text":"<p>Value threshold is useful to gate the candidate model by checking whether the corresponding metrics is larger than a lower bound and/or smaller than a upper bound. User can set either one or both of the lower_bound and upper_bound values. The lower_bound is default to negative infinity if unset, and the upper_bound defaults to infinity if unset.</p> <pre><code>import tensorflow_model_analysis as tfma\n\nlower_bound = tfma.GenericValueThreshold(lower_bound={'value':0})\nupper_bound = tfma.GenericValueThreshold(upper_bound={'value':1})\nlower_upper_bound = tfma.GenericValueThreshold(lower_bound={'value':0},\n                                               upper_bound={'value':1))\n</code></pre>"},{"location":"model_validations/#genericchangethreshold","title":"GenericChangeThreshold","text":"<p>Change threhold is useful to gate the candidate model by checking whether the corresponding metric is larger/smaller than that of a baseline model. There are two ways that the change can be measured: absolute change and relative change. Aboslute change is calculated as the value diference between the metrics of the candidate and baseline model, namely, v_c - v_b where v_c denotes the candidate metric value and v_b denotes the baseline value. Relative value is the relative difference between the metric of the candidate and the baseline, namely, v_c/v_b. The absolute and the relative threshold can co-exist to gate model by both criteria. Besides setting up threshold values, user also need to configure the MetricDirection. for metrics with favorably higher values (e.g., AUC), set the direction to HIGHER_IS_BETTER, for metrics with favorably lower values (e.g., loss), set the direction to LOWER_IS_BETTER. Change thresholds require a baseline model to be evaluated along with the candidate model. See Getting Started guide for an example.</p> <pre><code>import tensorflow_model_analysis as tfma\n\nabsolute_higher_is_better = tfma.GenericChangeThreshold(absolute={'value':1},\n                                                        direction=tfma.MetricDirection.HIGHER_IS_BETTER)\nabsolute_lower_is_better = tfma.GenericChangeThreshold(absolute={'value':1},\n                                                       direction=tfma.MetricDirection.LOWER_IS_BETTER)\nrelative_higher_is_better = tfma.GenericChangeThreshold(relative={'value':1},\n                                                        direction=tfma.MetricDirection.HIGHER_IS_BETTER)\nrelative_lower_is_better = tfma.GenericChangeThreshold(relative={'value':1},\n                                                       direction=tfma.MetricDirection.LOWER_IS_BETTER)\nabsolute_and_relative = tfma.GenericChangeThreshold(relative={'value':1},\n                                                    absolute={'value':0.2},\n                                                    direction=tfma.MetricDirection.LOWER_IS_BETTER)\n</code></pre>"},{"location":"model_validations/#putting-things-together","title":"Putting things together","text":"<p>The following example combines value and change thresholds:</p> <pre><code>import tensorflow_model_analysis as tfma\n\nlower_bound = tfma.GenericValueThreshold(lower_bound={'value':0.7})\nrelative_higher_is_better =\n    tfma.GenericChangeThreshold(relative={'value':1.01},\n                                direction=tfma.MetricDirection.HIGHER_IS_BETTER)\nauc_threshold = tfma.MetricThreshold(value_threshold=lower_bound,\n                                     change_threshold=relative_higher_is_better)\n</code></pre> <p>It might be more readable to write down the config in proto format:</p> <pre><code>from google.protobuf import text_format\n\nauc_threshold = text_format.Parse(\"\"\"\n  value_threshold { lower_bound { value: 0.6 } }\n  change_threshold { relative { value: 1.01 } }\n\"\"\", tfma.MetricThreshold())\n</code></pre> <p>The MetricThreshold can be set to gate on both model Training Time metrics (either EvalSavedModel or Keras saved model) and Post Training metrics (defined in TFMA config). For Training Time metrics, the thresholds are specified in the tfma.MetricsSpec:</p> <pre><code>metrics_spec = tfma.MetricSpec(thresholds={'auc': auc_threshold})\n</code></pre> <p>For post training metrics, thresholds are defined directly in the tfma.MetricConfig:</p> <pre><code>metric_config = tfma.MetricConfig(class_name='TotalWeightedExample',\n                                  threshold=lower_bound)\n</code></pre> <p>Here is an example along with the other settings in the EvalConfig:</p> <pre><code># Run in a Jupyter Notebook.\nfrom google.protobuf import text_format\n\neval_config = text_format.Parse(\"\"\"\n  model_specs {\n    # This assumes a serving model with a \"serving_default\" signature.\n    label_key: \"label\"\n    example_weight_key: \"weight\"\n  }\n  metrics_spec {\n    # Training Time metric thresholds\n    thresholds {\n      key: \"auc\"\n      value: {\n        value_threshold {\n          lower_bound { value: 0.7 }\n        }\n        change_threshold {\n          direction: HIGHER_IS_BETTER\n          absolute { value: -1e-10 }\n        }\n      }\n    }\n    # Post Training metrics and their thesholds.\n    metrics {\n      # This assumes a binary classification model.\n      class_name: \"AUC\"\n      threshold {\n        value_threshold {\n          lower_bound { value: 0 }\n        }\n      }\n    }\n  }\n  slicing_specs {}\n  slicing_specs {\n    feature_keys: [\"age\"]\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_models = [\n  tfma.default_eval_shared_model(\n      model_name=tfma.CANDIDATE_KEY,\n      eval_saved_model_path='/path/to/saved/candiate/model',\n      eval_config=eval_config),\n  tfma.default_eval_shared_model(\n      model_name=tfma.BASELINE_KEY,\n      eval_saved_model_path='/path/to/saved/baseline/model',\n      eval_config=eval_config),\n]\n\neval_result = tfma.run_model_analysis(\n    eval_shared_models,\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location=\"/path/to/file/containing/tfrecords\",\n    output_path=\"/path/for/output\")\n\ntfma.view.render_slicing_metrics(eval_result)\ntfma.load_validation_result(output_path)\n</code></pre>"},{"location":"model_validations/#output","title":"Output","text":"<p>In addition to the metrics file output by the evaluator, when validation is used, an additional \"validations\" file is also output. The payload format is ValidationResult. The output will have \"validation_ok\" set to True when there are no failures. When there are failures, information is provided about the associated metrics, the thresholds, and the metric values that were observed. The following is an example where the \"weighted_examle_count\" is failing a value threshold (1.5 is not smaller than 1.0, thus the failure):</p> <pre><code>  validation_ok: False\n  metric_validations_per_slice {\n    failures {\n      metric_key {\n        name: \"weighted_example_count\"\n        model_name: \"candidate\"\n      }\n      metric_threshold {\n        value_threshold {\n          upper_bound { value: 1.0 }\n        }\n      }\n      metric_value {\n        double_value { value: 1.5 }\n      }\n    }\n  }\n</code></pre>"},{"location":"post_export_metrics/","title":"Post export metrics","text":""},{"location":"post_export_metrics/#post-export-metrics","title":"Post Export Metrics","text":"<p>As the name suggests, this is a metric that is added post-export, before evaluation.</p> <p>TFMA is packaged with several pre-defined evaluation metrics, like example_count, auc, confusion_matrix_at_thresholds, precision_recall_at_k, mse, mae, to name a few. (Complete list here.)</p> <p>If you don\u2019t find an existing metrics relevant to your use-case, or want to customize a metric, you can define your own custom metric. Read on for the details!</p>"},{"location":"post_export_metrics/#adding-custom-metrics-in-tfma","title":"Adding Custom Metrics in TFMA","text":""},{"location":"post_export_metrics/#defining-custom-metrics-in-tfma-1x","title":"Defining Custom Metrics in TFMA 1.x","text":"<p>Tip: For code references, please check metrics like FairnessIndicators, MeanAbsoluteError etc.</p>"},{"location":"post_export_metrics/#extend-abstract-base-class","title":"Extend Abstract Base Class","text":"<p>To add a custom metric, create a new class extending _PostExportMetric abstract class and define its constructor and implement abstract / unimplemented methods.</p>"},{"location":"post_export_metrics/#define-constructor","title":"Define Constructor","text":"<p>In the constructor, take as parameters all the relevant information like label_key, prediction_key, example_weight_key, metric_tag, etc. required for custom metric.</p>"},{"location":"post_export_metrics/#implement-abstract-unimplemented-methods","title":"Implement Abstract / Unimplemented Methods","text":"<ul> <li> <p>check_compatibility</p> <p>Implement this method to check for compatibility of the metric with the model being evaluated, i.e. checking if all required features, expected label and prediction key are present in the model in appropriate data type. It takes three arguments:</p> <ul> <li>features_dict</li> <li>predictions_dict</li> <li>labels_dict</li> </ul> <p>These dictionaries contains references to Tensors for the model.</p> </li> <li> <p>get_metric_ops</p> <p>Implement this method to provide metric ops (value and update ops) to compute the metric. Similar to check_compatibility method, it also takes three arguments:</p> <ul> <li>features_dict</li> <li>predictions_dict</li> <li>labels_dict</li> </ul> <p>Define your metric computation logic using these references to Tensors for the model.</p> <p>Note: Result of update_ops must be additive for TFMA to sum up per worker update_op output. Check out examples here.</p> </li> <li> <p>populate_stats_and_pop     and     populate_plots_and_pop</p> <p>Implement this metric to convert raw metric results to MetricValue and PlotData proto format. This takes three arguments:</p> <ul> <li>slice_key: Name of slice metric belongs to.</li> <li>combined_metrics: Dictionary containing raw results.</li> <li>output_metrics: Output dictionary containing metric in desired proto     format.</li> </ul> </li> </ul> <pre><code>@_export('my_metric')\nclass _MyMetric(_PostExportMetric):\n   def __init__(self,\n                target_prediction_keys: Optional[List[Text]] = None,\n                labels_key: Optional[Text] = None,\n                metric_tag: Optional[Text] = None):\n      self._target_prediction_keys = target_prediction_keys\n      self._label_keys = label_keys\n      self._metric_tag = metric_tag\n      self._metric_key = 'my_metric_key'\n\n   def check_compatibility(self, features_dict:types.TensorTypeMaybeDict,\n                           predictions_dict: types.TensorTypeMaybeDict,\n                           labels_dict: types.TensorTypeMaybeDict) -&gt; None:\n       # Add compatibility check needed for the metric here.\n\n   def get_metric_ops(self, features_dict: types.TensorTypeMaybeDict,\n                      predictions_dict: types.TensorTypeMaybeDict,\n                      labels_dict: types.TensorTypeMaybeDict\n                     ) -&gt; Dict[bytes, Tuple[types.TensorType,\n                     types.TensorType]]:\n        # Metric computation logic here.\n        # Define value and update ops.\n        value_op = compute_metric_value(...)\n        update_op = create_update_op(... )\n        return {self._metric_key: (value_op, update_op)}\n\n   def populate_stats_and_pop(\n       self, slice_key: slicer.SliceKeyType, combined_metrics: Dict[Text, Any],\n       output_metrics: Dict[Text, metrics_pb2.MetricValue]) -&gt; None:\n       # Parses the metric and converts it into required metric format.\n       metric_result = combined_metrics[self._metric_key]\n       output_metrics[self._metric_key].double_value.value = metric_result\n</code></pre>"},{"location":"post_export_metrics/#usage","title":"Usage","text":"<pre><code># Custom metric callback\ncustom_metric_callback = my_metric(\n    labels_key='label',\n    target_prediction_keys=['prediction'])\n\nfairness_indicators_callback =\n   post_export_metrics.fairness_indicators(\n        thresholds=[0.1, 0.3, 0.5, 0.7, 0.9], labels_key=label)\n\nadd_metrics_callbacks = [custom_metric_callback,\n   fairness_indicators_callback]\n\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=eval_saved_model_path,\n    add_metrics_callbacks=add_metrics_callbacks)\n\neval_config = tfma.EvalConfig(...)\n\n# Run evaluation\ntfma.run_model_analysis(\n    eval_config=eval_config, eval_shared_model=eval_shared_model)\n</code></pre>"},{"location":"setup/","title":"Tensorflow Model Analysis Setup","text":""},{"location":"setup/#configuration","title":"Configuration","text":"<p>TFMA stores its configuration in a proto that is serialized to JSON. This proto consolidates the configuration required for input data, output data, model specifications, metric specifications, and slicing specifications.</p> <p>All TFMA pipelines are associated with a baseline (primary) model and zero or more candidate (secondary) models. The baseline and candidate model are defined by the user at the start of the pipeline and each require a unique name. The following are examples of typical configuration setups a user may use:</p> <ul> <li>Single model evaluation:<ul> <li>N/A (i.e. no name)</li> </ul> </li> <li>Validation-based evaluation:<ul> <li><code>baseline</code></li> <li><code>candidate</code></li> </ul> </li> <li>Model comparison evaluation:<ul> <li><code>my_model_a</code></li> <li><code>my_model_b</code></li> </ul> </li> </ul>"},{"location":"setup/#model-specs","title":"Model Specs","text":"<p>Model specs are of type <code>tfma.ModelSpec</code> and are used to define the location of a model as well as other model specific parameters. For example the following are typical settings that would need to be configured prior to running an evaluation:</p> <ul> <li><code>name</code> - name of model (if multiple models used)</li> <li><code>signature_name</code> - name of signature used for predictions (default is     <code>serving_default</code>). Use <code>eval</code> if using an EvalSavedModel.</li> <li><code>label_key</code> - name of the feature associated with the label.</li> <li><code>example_weight_key</code> - name of the feature assocated with the example     weight.</li> </ul>"},{"location":"setup/#metrics-specs","title":"Metrics Specs","text":"<p>Metrics specs are of type <code>tfma.MetricsSpec</code> and are used to configure the metrics that will be calculated as part of the evaluation. Different machine learning problems use different types of metrics and TFMA offers a lot of options for configuring and customizing the metrics that are computed. Since metrics are a very large part of TFMA, they are discussed in detail separately in metrics.</p>"},{"location":"setup/#slicing-specs","title":"Slicing Specs","text":"<p>Slicing specs are of type <code>tfma.SlicingSpec</code> and are used to configure the slices criteria that will be used during the evaluation. Slicing can be done either by <code>feature_keys</code>, <code>feature_values</code>, or both. Some examples of slicing specs are as follows:</p> <ul> <li><code>{}</code><ul> <li>Slice consisting of overall data.</li> </ul> </li> <li><code>{ feature_keys: [\"country\"] }</code><ul> <li>Slices for all values in feature \"country\". For example, we might get     slices \"country:us\", \"country:jp\", etc.</li> </ul> </li> <li><code>{ feature_values: [{key: \"country\", value: \"us\"}] }</code><ul> <li>Slice consisting of \"country:us\".</li> </ul> </li> <li><code>{ feature_keys: [\"country\", \"city\"] }</code><ul> <li>Slices for all values in feature \"country\" crossed with all values in     feature \"city\" (note this may be expensive).</li> </ul> </li> <li><code>{ feature_keys: [\"country\"] feature_values: [{key: \"age\", value: \"20\"}] }</code><ul> <li>Slices for all values in feature \"country\" crossed with value \"age:20\"</li> </ul> </li> </ul> <p>Note that feature keys may be either transformed features or raw input features. See <code>tfma.SlicingSpec</code> for more information.</p>"},{"location":"setup/#evalsharedmodel","title":"EvalSharedModel","text":"<p>In addition to the configuration settings, TFMA also requires that an instance of a <code>tfma.EvalSharedModel</code> be created for sharing a model between multiple threads in the same process. The shared model instance includes information about the type of model (keras, etc) and how to load and configure the model from its saved location on disk (e.g. tags, etc). The <code>tfma.default_eval_shared_model</code> API can be used to create a default instance given a path and set of tags.</p>"},{"location":"visualizations/","title":"TensorFlow Model Analysis Visualizations","text":"<p>The output of running an evaluation is a <code>tfma.EvalResult</code> which can be visualized in a Jupyter notebook by calling <code>tfma.view.render_slicing_metrics</code> (or <code>tfma.view.render_plot</code> for plots).</p>"},{"location":"visualizations/#metrics-view","title":"Metrics View","text":"<p>To view metrics, use the <code>tfma.view.render_slicing_metrics</code> API passing the <code>tfma.EvalResult</code> that was output from the evaluation run. The metrics view is composed of three parts:</p> <ul> <li> <p>Metrics Selector</p> <p>By default, all computed metrics are displayed and the columns are sorted alphabetically. Metrics selector allows the user to add / remove / reorder metrics. Simply check / uncheck metrics from the dropdown (hold Ctrl for multi-select) or type / re-arrange them directly in the input box.</p> <p></p> </li> <li> <p>Metric Visualization</p> <p>Metric visualization aims to provide intuition about slices in the feature chosen. A quick filtering is available to filter out slices with small weighted sample count.</p> <p></p> <p>Two type of visualizations are supported:</p> <ol> <li> <p>Slice overview</p> <p>In this view, value for the chosen metric is rendered for each slice and the slices can be sorted by the slice name or the value of another metric.</p> <p></p> <p>When the number of slices are small, this is the default view.</p> </li> <li> <p>Metrics Histogram</p> <p>In this view, slices are broken down into buckets based on their metric values. The value(s) displayed in each bucket can be the number of slices in the bucket or the total weighted sample count for all slices in the bucket or both.</p> <p></p> <p>The number of buckets can be changed and logarithmic scale can be applied in the settings menu by clicking on the gear icon.</p> <p></p> <p>It is also possible to filter out outliers in the histogram view. Simply drag the desired range in the histogram as shown in the screenshot below.</p> <p></p> <p>When the number of slices are large, this is the default view.</p> </li> </ol> </li> <li> <p>Metrics Table</p> <p>The metric table summarizes results for all metrics chosen in metrics selector. It can be sorted by clicking on the metric name. Only slices not filtered out will be rendered.</p> </li> </ul>"},{"location":"visualizations/#plot-views","title":"Plot Views","text":"<p>Each plot has it own visualization that is unique to the plot. For more information, see the relevant API documentation for the plot class. Note that in TFMA, plots and metrics are both defined under <code>tfma.metrics.*</code> By convention the classes related to plots end in <code>Plot</code>. To view plots, use the <code>tfma.view.render_plot</code> API passing the <code>tfma.EvalResult</code> that was output from the evaluation run.</p>"},{"location":"visualizations/#time-series-graphs","title":"Time Series Graphs","text":"<p>Time series graphs make it easy to spot trends of a specific metric over data spans or model runs. To create a time series graph, perform multiple evaluations (saving the output to different directories), and then load them into a <code>tfma.EvalResults</code> object by calling <code>tfma.load_eval_results</code>. The results can then be displayed using <code>tfma.view.render_time_series</code></p> <p>To display the graph for a specific metric, simply click on it from the dropdown list. To dismiss a graph, click on the X on the upper right corner.</p> <p></p> <p>Hover over any data point in the graph shows a tooltip indicating model run, data span, and metric value.</p>"},{"location":"api_docs/python/tfma-constants/","title":"TFMA Constants","text":""},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants","title":"tensorflow_model_analysis.constants","text":"<p>Constants used in TensorFlow Model Analysis.</p>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.ANALYSIS_KEY","title":"ANALYSIS_KEY  <code>module-attribute</code>","text":"<pre><code>ANALYSIS_KEY = 'analysis'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.ARROW_INPUT_COLUMN","title":"ARROW_INPUT_COLUMN  <code>module-attribute</code>","text":"<pre><code>ARROW_INPUT_COLUMN = '__raw_record__'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.ARROW_RECORD_BATCH_KEY","title":"ARROW_RECORD_BATCH_KEY  <code>module-attribute</code>","text":"<pre><code>ARROW_RECORD_BATCH_KEY = 'arrow_record_batch'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.ATTRIBUTIONS_KEY","title":"ATTRIBUTIONS_KEY  <code>module-attribute</code>","text":"<pre><code>ATTRIBUTIONS_KEY = 'attributions'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.BASELINE_KEY","title":"BASELINE_KEY  <code>module-attribute</code>","text":"<pre><code>BASELINE_KEY = 'baseline'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.BASELINE_SCORE_KEY","title":"BASELINE_SCORE_KEY  <code>module-attribute</code>","text":"<pre><code>BASELINE_SCORE_KEY = 'baseline_score'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.CANDIDATE_KEY","title":"CANDIDATE_KEY  <code>module-attribute</code>","text":"<pre><code>CANDIDATE_KEY = 'candidate'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.DATA_CENTRIC_MODE","title":"DATA_CENTRIC_MODE  <code>module-attribute</code>","text":"<pre><code>DATA_CENTRIC_MODE = 'data_centric_mode'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.ERROR_METRIC_NAME","title":"ERROR_METRIC_NAME  <code>module-attribute</code>","text":"<pre><code>ERROR_METRIC_NAME = '__ERROR__'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.EXAMPLE_SCORE_KEY","title":"EXAMPLE_SCORE_KEY  <code>module-attribute</code>","text":"<pre><code>EXAMPLE_SCORE_KEY = 'example_score'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.EXAMPLE_WEIGHTS_KEY","title":"EXAMPLE_WEIGHTS_KEY  <code>module-attribute</code>","text":"<pre><code>EXAMPLE_WEIGHTS_KEY = 'example_weights'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.FEATURES_KEY","title":"FEATURES_KEY  <code>module-attribute</code>","text":"<pre><code>FEATURES_KEY = 'features'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.FEATURES_PREDICTIONS_LABELS_KEY","title":"FEATURES_PREDICTIONS_LABELS_KEY  <code>module-attribute</code>","text":"<pre><code>FEATURES_PREDICTIONS_LABELS_KEY = '_fpl'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.INPUT_KEY","title":"INPUT_KEY  <code>module-attribute</code>","text":"<pre><code>INPUT_KEY = 'input'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.LABELS_KEY","title":"LABELS_KEY  <code>module-attribute</code>","text":"<pre><code>LABELS_KEY = 'labels'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.MATERIALIZED_PREDICTION","title":"MATERIALIZED_PREDICTION  <code>module-attribute</code>","text":"<pre><code>MATERIALIZED_PREDICTION = 'materialized_prediction'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.MATERIALIZE_COLUMNS","title":"MATERIALIZE_COLUMNS  <code>module-attribute</code>","text":"<pre><code>MATERIALIZE_COLUMNS = 'materialize'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.METRICS_KEY","title":"METRICS_KEY  <code>module-attribute</code>","text":"<pre><code>METRICS_KEY = 'metrics'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.METRICS_NAMESPACE","title":"METRICS_NAMESPACE  <code>module-attribute</code>","text":"<pre><code>METRICS_NAMESPACE = MakeTfxNamespace(['ModelAnalysis'])\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.MODEL_AGNOSTIC","title":"MODEL_AGNOSTIC  <code>module-attribute</code>","text":"<pre><code>MODEL_AGNOSTIC = 'model_agnostic'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.MODEL_CENTRIC_MODE","title":"MODEL_CENTRIC_MODE  <code>module-attribute</code>","text":"<pre><code>MODEL_CENTRIC_MODE = 'model_centric_mode'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.PLACEHOLDER","title":"PLACEHOLDER  <code>module-attribute</code>","text":"<pre><code>PLACEHOLDER = 'placeholder'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.PLOTS_KEY","title":"PLOTS_KEY  <code>module-attribute</code>","text":"<pre><code>PLOTS_KEY = 'plots'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.PREDICTIONS_KEY","title":"PREDICTIONS_KEY  <code>module-attribute</code>","text":"<pre><code>PREDICTIONS_KEY = 'predictions'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.PREDICTION_LOG_KEY","title":"PREDICTION_LOG_KEY  <code>module-attribute</code>","text":"<pre><code>PREDICTION_LOG_KEY = 'prediction_log'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.SLICE_KEYS_KEY","title":"SLICE_KEYS_KEY  <code>module-attribute</code>","text":"<pre><code>SLICE_KEYS_KEY = 'slice_keys'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.SLICE_KEY_TYPES_KEY","title":"SLICE_KEY_TYPES_KEY  <code>module-attribute</code>","text":"<pre><code>SLICE_KEY_TYPES_KEY = '_slice_key_types'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.SPARSE_PLACEHOLDER","title":"SPARSE_PLACEHOLDER  <code>module-attribute</code>","text":"<pre><code>SPARSE_PLACEHOLDER = 'sparse_placeholder'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.SPLIT_KEY","title":"SPLIT_KEY  <code>module-attribute</code>","text":"<pre><code>SPLIT_KEY = 'split'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.TFMA_EVAL","title":"TFMA_EVAL  <code>module-attribute</code>","text":"<pre><code>TFMA_EVAL = 'tfma_eval'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.TF_ESTIMATOR","title":"TF_ESTIMATOR  <code>module-attribute</code>","text":"<pre><code>TF_ESTIMATOR = 'tf_estimator'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.TF_GENERIC","title":"TF_GENERIC  <code>module-attribute</code>","text":"<pre><code>TF_GENERIC = 'tf_generic'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.TF_JS","title":"TF_JS  <code>module-attribute</code>","text":"<pre><code>TF_JS = 'tf_js'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.TF_KERAS","title":"TF_KERAS  <code>module-attribute</code>","text":"<pre><code>TF_KERAS = 'tf_keras'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.TF_LITE","title":"TF_LITE  <code>module-attribute</code>","text":"<pre><code>TF_LITE = 'tf_lite'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.TRANSFORMED_FEATURES_KEY","title":"TRANSFORMED_FEATURES_KEY  <code>module-attribute</code>","text":"<pre><code>TRANSFORMED_FEATURES_KEY = 'transformed_features'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.UNKNOWN_EVAL_MODE","title":"UNKNOWN_EVAL_MODE  <code>module-attribute</code>","text":"<pre><code>UNKNOWN_EVAL_MODE = 'unknown_eval_mode'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.VALIDATIONS_KEY","title":"VALIDATIONS_KEY  <code>module-attribute</code>","text":"<pre><code>VALIDATIONS_KEY = 'validations'\n</code></pre>"},{"location":"api_docs/python/tfma-constants/#tensorflow_model_analysis.constants.VALID_TF_MODEL_TYPES","title":"VALID_TF_MODEL_TYPES  <code>module-attribute</code>","text":"<pre><code>VALID_TF_MODEL_TYPES = (\n    TFMA_EVAL,\n    TF_GENERIC,\n    TF_ESTIMATOR,\n    TF_KERAS,\n    TF_LITE,\n    TF_JS,\n    MATERIALIZED_PREDICTION,\n)\n</code></pre>"},{"location":"api_docs/python/tfma-contrib/","title":"TFMA Constants","text":""},{"location":"api_docs/python/tfma-contrib/#tensorflow_model_analysis.contrib","title":"tensorflow_model_analysis.contrib","text":""},{"location":"api_docs/python/tfma-evaluators/","title":"TFMA Evaluators","text":""},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators","title":"tensorflow_model_analysis.evaluators","text":"<p>Init module for TensorFlow Model Analysis evaluators.</p>"},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators.Evaluation","title":"Evaluation  <code>module-attribute</code>","text":"<pre><code>Evaluation = Dict[str, PCollection]\n</code></pre>"},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators.AnalysisTableEvaluator","title":"AnalysisTableEvaluator","text":"<pre><code>AnalysisTableEvaluator(\n    key: str = ANALYSIS_KEY,\n    run_after: str = LAST_EXTRACTOR_STAGE_NAME,\n    include: Optional[\n        Union[Iterable[str], Dict[str, Any]]\n    ] = None,\n    exclude: Optional[\n        Union[Iterable[str], Dict[str, Any]]\n    ] = None,\n) -&gt; Evaluator\n</code></pre> <p>Creates an Evaluator for returning Extracts data for analysis.</p> <p>If both include and exclude are None then tfma.INPUT_KEY extracts will be excluded by default.</p> <p>key: Name to use for key in Evaluation output.   run_after: Extractor to run after (None means before any extractors).   include: List or map of keys to include in output. Keys starting with '_'     are automatically filtered out at write time. If a map of keys is passed     then the keys and sub-keys that exist in the map will be included in the     output. An empty dict behaves as a wildcard matching all keys or the value     itself. Since matching on feature values is not currently supported, an     empty dict must be used to represent the leaf nodes. For example: {'key1':     {'key1-subkey': {}}, 'key2': {}}.   exclude: List or map of keys to exclude from output. If a map of keys is     passed then the keys and sub-keys that exist in the map will be excluded     from the output. An empty dict behaves as a wildcard matching all keys or     the value itself. Since matching on feature values is not currently     supported, an empty dict must be used to represent the leaf nodes. For     example, {'key1': {'key1-subkey': {}}, 'key2': {}}.</p> <p>Evaluator for collecting analysis data. The output is stored under the key   'analysis'.</p> <p>ValueError: If both include and exclude are used.</p> Source code in <code>tensorflow_model_analysis/evaluators/analysis_table_evaluator.py</code> <pre><code>def AnalysisTableEvaluator(  # pylint: disable=invalid-name\n    key: str = constants.ANALYSIS_KEY,\n    run_after: str = extractor.LAST_EXTRACTOR_STAGE_NAME,\n    include: Optional[Union[Iterable[str], Dict[str, Any]]] = None,\n    exclude: Optional[Union[Iterable[str], Dict[str, Any]]] = None,\n) -&gt; evaluator.Evaluator:\n    \"\"\"Creates an Evaluator for returning Extracts data for analysis.\n\n    If both include and exclude are None then tfma.INPUT_KEY extracts will be\n    excluded by default.\n\n    Args:\n    ----\n      key: Name to use for key in Evaluation output.\n      run_after: Extractor to run after (None means before any extractors).\n      include: List or map of keys to include in output. Keys starting with '_'\n        are automatically filtered out at write time. If a map of keys is passed\n        then the keys and sub-keys that exist in the map will be included in the\n        output. An empty dict behaves as a wildcard matching all keys or the value\n        itself. Since matching on feature values is not currently supported, an\n        empty dict must be used to represent the leaf nodes. For example: {'key1':\n        {'key1-subkey': {}}, 'key2': {}}.\n      exclude: List or map of keys to exclude from output. If a map of keys is\n        passed then the keys and sub-keys that exist in the map will be excluded\n        from the output. An empty dict behaves as a wildcard matching all keys or\n        the value itself. Since matching on feature values is not currently\n        supported, an empty dict must be used to represent the leaf nodes. For\n        example, {'key1': {'key1-subkey': {}}, 'key2': {}}.\n\n    Returns:\n    -------\n      Evaluator for collecting analysis data. The output is stored under the key\n      'analysis'.\n\n    Raises:\n    ------\n      ValueError: If both include and exclude are used.\n    \"\"\"\n    # pylint: disable=no-value-for-parameter\n    return evaluator.Evaluator(\n        stage_name=\"EvaluateExtracts\",\n        run_after=run_after,\n        ptransform=EvaluateExtracts(key=key, include=include, exclude=exclude),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators.MetricsPlotsAndValidationsEvaluator","title":"MetricsPlotsAndValidationsEvaluator","text":"<pre><code>MetricsPlotsAndValidationsEvaluator(\n    eval_config: EvalConfig,\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    metrics_key: str = METRICS_KEY,\n    plots_key: str = PLOTS_KEY,\n    attributions_key: str = ATTRIBUTIONS_KEY,\n    run_after: str = SLICE_KEY_EXTRACTOR_STAGE_NAME,\n    schema: Optional[Schema] = None,\n    random_seed_for_testing: Optional[int] = None,\n) -&gt; Evaluator\n</code></pre> <p>Creates an Evaluator for evaluating metrics and plots.</p> <p>eval_config: Eval config.   eval_shared_model: Optional shared model (single-model evaluation) or list     of shared models (multi-model evaluation). Only required if there are     metrics to be computed in-graph using the model.   metrics_key: Name to use for metrics key in Evaluation output.   plots_key: Name to use for plots key in Evaluation output.   attributions_key: Name to use for attributions key in Evaluation output.   run_after: Extractor to run after (None means before any extractors).   schema: A schema to use for customizing metrics and plots.   random_seed_for_testing: Seed to use for unit testing.</p> <p>Evaluator for evaluating metrics and plots. The output will be stored under   'metrics' and 'plots' keys.</p> Source code in <code>tensorflow_model_analysis/evaluators/metrics_plots_and_validations_evaluator.py</code> <pre><code>def MetricsPlotsAndValidationsEvaluator(  # pylint: disable=invalid-name\n    eval_config: config_pb2.EvalConfig,\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    metrics_key: str = constants.METRICS_KEY,\n    plots_key: str = constants.PLOTS_KEY,\n    attributions_key: str = constants.ATTRIBUTIONS_KEY,\n    run_after: str = slice_key_extractor.SLICE_KEY_EXTRACTOR_STAGE_NAME,\n    schema: Optional[schema_pb2.Schema] = None,\n    random_seed_for_testing: Optional[int] = None,\n) -&gt; evaluator.Evaluator:\n    \"\"\"Creates an Evaluator for evaluating metrics and plots.\n\n    Args:\n    ----\n      eval_config: Eval config.\n      eval_shared_model: Optional shared model (single-model evaluation) or list\n        of shared models (multi-model evaluation). Only required if there are\n        metrics to be computed in-graph using the model.\n      metrics_key: Name to use for metrics key in Evaluation output.\n      plots_key: Name to use for plots key in Evaluation output.\n      attributions_key: Name to use for attributions key in Evaluation output.\n      run_after: Extractor to run after (None means before any extractors).\n      schema: A schema to use for customizing metrics and plots.\n      random_seed_for_testing: Seed to use for unit testing.\n\n    Returns:\n    -------\n      Evaluator for evaluating metrics and plots. The output will be stored under\n      'metrics' and 'plots' keys.\n    \"\"\"\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n    if eval_shared_models:\n        eval_shared_models = {m.model_name: m for m in eval_shared_models}\n\n    # pylint: disable=no-value-for-parameter\n    return evaluator.Evaluator(\n        stage_name=\"EvaluateMetricsAndPlots\",\n        run_after=run_after,\n        ptransform=_EvaluateMetricsPlotsAndValidations(\n            eval_config=eval_config,\n            eval_shared_models=eval_shared_models,\n            metrics_key=metrics_key,\n            plots_key=plots_key,\n            attributions_key=attributions_key,\n            schema=schema,\n            random_seed_for_testing=random_seed_for_testing,\n        ),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-evaluators/#tensorflow_model_analysis.evaluators.verify_evaluator","title":"verify_evaluator","text":"<pre><code>verify_evaluator(\n    evaluator: Evaluator, extractors: List[Extractor]\n)\n</code></pre> <p>Verifies evaluator is matched with an extractor.</p> <p>evaluator: Evaluator to verify.   extractors: Extractors to use in verification.</p> <p>ValueError: If an Extractor cannot be found for the Evaluator.</p> Source code in <code>tensorflow_model_analysis/evaluators/evaluator.py</code> <pre><code>def verify_evaluator(evaluator: Evaluator, extractors: List[extractor.Extractor]):\n    \"\"\"Verifies evaluator is matched with an extractor.\n\n    Args:\n    ----\n      evaluator: Evaluator to verify.\n      extractors: Extractors to use in verification.\n\n    Raises:\n    ------\n      ValueError: If an Extractor cannot be found for the Evaluator.\n    \"\"\"\n    if (\n        evaluator.run_after\n        and evaluator.run_after != extractor.LAST_EXTRACTOR_STAGE_NAME\n        and not any(evaluator.run_after == x.stage_name for x in extractors)\n    ):\n        raise ValueError(\n            \"Extractor matching run_after=%s for Evaluator %s not found\"\n            % (evaluator.run_after, evaluator.stage_name)\n        )\n</code></pre>"},{"location":"api_docs/python/tfma-experimental/","title":"TFMA Experimental","text":""},{"location":"api_docs/python/tfma-experimental/#tensorflow_model_analysis.experimental","title":"tensorflow_model_analysis.experimental","text":""},{"location":"api_docs/python/tfma-extractors/","title":"TFMA Extractors","text":""},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors","title":"tensorflow_model_analysis.extractors","text":"<p>Init module for TensorFlow Model Analysis extractors.</p>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.SLICE_KEY_EXTRACTOR_STAGE_NAME","title":"SLICE_KEY_EXTRACTOR_STAGE_NAME  <code>module-attribute</code>","text":"<pre><code>SLICE_KEY_EXTRACTOR_STAGE_NAME = 'ExtractSliceKeys'\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.Extractor","title":"Extractor","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.Extractor-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.Extractor.ptransform","title":"ptransform  <code>instance-attribute</code>","text":"<pre><code>ptransform: PTransform\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.Extractor.stage_name","title":"stage_name  <code>instance-attribute</code>","text":"<pre><code>stage_name: str\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.ExampleWeightsExtractor","title":"ExampleWeightsExtractor","text":"<pre><code>ExampleWeightsExtractor(\n    eval_config: EvalConfig,\n) -&gt; Extractor\n</code></pre> <p>Creates an extractor for extracting example weights.</p> <p>The extractor's PTransform uses the config's ModelSpec.example_weight_key(s) to lookup the associated example weight values stored as features under the tfma.FEATURES_KEY (and optionally tfma.TRANSFORMED_FEATURES_KEY) in extracts. The resulting values are then added to the extracts under the key tfma.EXAMPLE_WEIGHTS_KEY.</p> <p>eval_config: Eval config.</p> <p>Extractor for extracting example weights.</p> Source code in <code>tensorflow_model_analysis/extractors/example_weights_extractor.py</code> <pre><code>def ExampleWeightsExtractor(\n    eval_config: config_pb2.EvalConfig,\n) -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for extracting example weights.\n\n    The extractor's PTransform uses the config's ModelSpec.example_weight_key(s)\n    to lookup the associated example weight values stored as features under the\n    tfma.FEATURES_KEY (and optionally tfma.TRANSFORMED_FEATURES_KEY) in extracts.\n    The resulting values are then added to the extracts under the key\n    tfma.EXAMPLE_WEIGHTS_KEY.\n\n    Args:\n    ----\n      eval_config: Eval config.\n\n    Returns:\n    -------\n      Extractor for extracting example weights.\n    \"\"\"\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=_EXAMPLE_WEIGHTS_EXTRACTOR_STAGE_NAME,\n        ptransform=_ExtractExampleWeights(eval_config=eval_config),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.FeatureExtractor","title":"FeatureExtractor","text":"<pre><code>FeatureExtractor(\n    additional_extracts: Optional[List[str]] = None,\n    excludes: Optional[List[bytes]] = None,\n    extract_source: str = FEATURES_PREDICTIONS_LABELS_KEY,\n    extract_dest: str = MATERIALIZE_COLUMNS,\n)\n</code></pre> Source code in <code>tensorflow_model_analysis/extractors/legacy_feature_extractor.py</code> <pre><code>def FeatureExtractor(\n    additional_extracts: Optional[List[str]] = None,\n    excludes: Optional[List[bytes]] = None,\n    extract_source: str = constants.FEATURES_PREDICTIONS_LABELS_KEY,\n    extract_dest: str = constants.MATERIALIZE_COLUMNS,\n):\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=_FEATURE_EXTRACTOR_STAGE_NAME,\n        ptransform=_ExtractFeatures(\n            additional_extracts=additional_extracts,\n            excludes=excludes,\n            source=extract_source,\n            dest=extract_dest,\n        ),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.FeaturesExtractor","title":"FeaturesExtractor","text":"<pre><code>FeaturesExtractor(\n    eval_config: EvalConfig,\n    tensor_representations: Optional[\n        Mapping[str, TensorRepresentation]\n    ] = None,\n) -&gt; Extractor\n</code></pre> <p>Creates an extractor for extracting features.</p> <p>The extractor acts as follows depending on the existence of certain keys within the incoming extracts:</p> <p>1) Extracts contains tfma.ARROW_RECORD_BATCH_KEY</p> <p>The features stored in the RecordBatch will be extracted and added to the   output extract under the key tfma.FEATURES_KEY and the raw serialized inputs   will be added under the tfma.INPUT_KEY. Any extracts that already exist will   be merged with the values from the RecordBatch with the RecordBatch values   taking precedence when duplicate keys are detected. The   tfma.ARROW_RECORD_BATCH_KEY key will be removed from the output extracts.</p> <p>2) Extracts contains tfma.FEATURES_KEY (but not tfma.ARROW_RECORD_BATCH_KEY)</p> <p>The operation will be a no-op and the incoming extracts will be passed as is   to the output.</p> <p>3) Extracts contains neither tfma.FEATURES_KEY | tfma.ARROW_RECORD_BATCH_KEY</p> <p>An exception will be raised.</p> <p>eval_config: Eval config.   tensor_representations: Optional tensor representations to use when parsing     the data. If tensor_representations are not passed or a representation is     not found for a given feature name a default representation will be used     where possible, otherwise an exception will be raised.</p> <p>Extractor for extracting features.</p> Source code in <code>tensorflow_model_analysis/extractors/features_extractor.py</code> <pre><code>def FeaturesExtractor(  # pylint: disable=invalid-name\n    eval_config: config_pb2.EvalConfig,\n    tensor_representations: Optional[\n        Mapping[str, schema_pb2.TensorRepresentation]\n    ] = None,\n) -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for extracting features.\n\n    The extractor acts as follows depending on the existence of certain keys\n    within the incoming extracts:\n\n      1) Extracts contains tfma.ARROW_RECORD_BATCH_KEY\n\n      The features stored in the RecordBatch will be extracted and added to the\n      output extract under the key tfma.FEATURES_KEY and the raw serialized inputs\n      will be added under the tfma.INPUT_KEY. Any extracts that already exist will\n      be merged with the values from the RecordBatch with the RecordBatch values\n      taking precedence when duplicate keys are detected. The\n      tfma.ARROW_RECORD_BATCH_KEY key will be removed from the output extracts.\n\n      2) Extracts contains tfma.FEATURES_KEY (but not tfma.ARROW_RECORD_BATCH_KEY)\n\n      The operation will be a no-op and the incoming extracts will be passed as is\n      to the output.\n\n      3) Extracts contains neither tfma.FEATURES_KEY | tfma.ARROW_RECORD_BATCH_KEY\n\n      An exception will be raised.\n\n    Args:\n    ----\n      eval_config: Eval config.\n      tensor_representations: Optional tensor representations to use when parsing\n        the data. If tensor_representations are not passed or a representation is\n        not found for a given feature name a default representation will be used\n        where possible, otherwise an exception will be raised.\n\n    Returns:\n    -------\n      Extractor for extracting features.\n    \"\"\"\n    del eval_config\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=_FEATURES_EXTRACTOR_STAGE_NAME,\n        ptransform=_ExtractFeatures(tensor_representations or {}),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.Filter","title":"Filter","text":"<pre><code>Filter(\n    extracts: PCollection,\n    include: Optional[\n        Union[Iterable[str], Dict[str, Any]]\n    ] = None,\n    exclude: Optional[\n        Union[Iterable[str], Dict[str, Any]]\n    ] = None,\n) -&gt; PCollection\n</code></pre> <p>Filters extracts to include/exclude specified keys.</p> <p>extracts: PCollection of extracts.   include: List or map of keys to include in output. If a map of keys is     passed then the keys and sub-keys that exist in the map will be included     in the output. An empty dict behaves as a wildcard matching all keys or     the value itself. Since matching on feature values is not currently     supported, an empty dict must be used to represent the leaf nodes. For     example, {'key1': {'key1-subkey': {}}, 'key2': {}}.   exclude: List or map of keys to exclude from output. If a map of keys is     passed then the keys and sub-keys that exist in the map will be excluded     from the output. An empty dict behaves as a wildcard matching all keys or     the value itself. Since matching on feature values is not currently     supported, an empty dict must be used to represent the leaf nodes. For     example, {'key1': {'key1-subkey': {}}, 'key2': {}}.</p> <p>Filtered PCollection of Extracts.</p> <p>ValueError: If both include and exclude are used.</p> Source code in <code>tensorflow_model_analysis/extractors/extractor.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(types.Extracts)\n@beam.typehints.with_output_types(types.Extracts)\ndef Filter(  # pylint: disable=invalid-name\n    extracts: beam.pvalue.PCollection,\n    include: Optional[Union[Iterable[str], Dict[str, Any]]] = None,\n    exclude: Optional[Union[Iterable[str], Dict[str, Any]]] = None,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Filters extracts to include/exclude specified keys.\n\n    Args:\n    ----\n      extracts: PCollection of extracts.\n      include: List or map of keys to include in output. If a map of keys is\n        passed then the keys and sub-keys that exist in the map will be included\n        in the output. An empty dict behaves as a wildcard matching all keys or\n        the value itself. Since matching on feature values is not currently\n        supported, an empty dict must be used to represent the leaf nodes. For\n        example, {'key1': {'key1-subkey': {}}, 'key2': {}}.\n      exclude: List or map of keys to exclude from output. If a map of keys is\n        passed then the keys and sub-keys that exist in the map will be excluded\n        from the output. An empty dict behaves as a wildcard matching all keys or\n        the value itself. Since matching on feature values is not currently\n        supported, an empty dict must be used to represent the leaf nodes. For\n        example, {'key1': {'key1-subkey': {}}, 'key2': {}}.\n\n    Returns:\n    -------\n      Filtered PCollection of Extracts.\n\n    Raises:\n    ------\n      ValueError: If both include and exclude are used.\n    \"\"\"\n    if include and exclude:\n        raise ValueError(\"only one of include or exclude should be used.\")\n\n    if not isinstance(include, dict):\n        include = {k: {} for k in include or []}\n    if not isinstance(exclude, dict):\n        exclude = {k: {} for k in exclude or []}\n\n    def filter_extracts(extracts: types.Extracts) -&gt; types.Extracts:  # pylint: disable=invalid-name\n        \"\"\"Filters extracts.\"\"\"\n        if not include and not exclude:\n            return extracts\n        elif include:\n            return util.include_filter(include, extracts)\n        else:\n            return util.exclude_filter(exclude, extracts)\n\n    return extracts | beam.Map(filter_extracts)\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.InputExtractor","title":"InputExtractor","text":"<pre><code>InputExtractor(eval_config: EvalConfig) -&gt; Extractor\n</code></pre> <p>Creates an extractor for extracting features, labels, and example weights.</p> <p>The extractor's PTransform parses tf.train.Example protos stored under the tfma.INPUT_KEY in the incoming extracts and adds the resulting features, labels, and example weights to the extracts under the keys tfma.FEATURES_KEY, tfma.LABELS_KEY, and tfma.EXAMPLE_WEIGHTS_KEY. If the eval_config contains a prediction_key and a corresponding key is found in the parse example, then predictions will also be extracted and stored under the tfma.PREDICTIONS_KEY. Any extracts that already exist will be merged with the values parsed by this extractor with this extractor's values taking precedence when duplicate keys are detected.</p> Note that the use of a prediction_key in an eval_config serves two use cases <p>(1) as a key into the dict of predictions output by predict extractor (2) as the key for a pre-computed prediction stored as a feature.</p> <p>The InputExtractor can be used to handle case (2). These cases are meant to be exclusive (i.e. if approach (2) is used then a predict extractor would not be configured and if (1) is used then a key matching the predictons would not be stored in the features). However, if a feature key happens to match the same name as the prediction output key then both paths may be executed. In this case, the value stored here will be replaced by the predict extractor (though it will still be popped from the features).</p> <p>eval_config: Eval config.</p> <p>Extractor for extracting features, labels, and example weights inputs.</p> Source code in <code>tensorflow_model_analysis/extractors/legacy_input_extractor.py</code> <pre><code>def InputExtractor(eval_config: config_pb2.EvalConfig) -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for extracting features, labels, and example weights.\n\n    The extractor's PTransform parses tf.train.Example protos stored under the\n    tfma.INPUT_KEY in the incoming extracts and adds the resulting features,\n    labels, and example weights to the extracts under the keys tfma.FEATURES_KEY,\n    tfma.LABELS_KEY, and tfma.EXAMPLE_WEIGHTS_KEY. If the eval_config contains a\n    prediction_key and a corresponding key is found in the parse example, then\n    predictions will also be extracted and stored under the tfma.PREDICTIONS_KEY.\n    Any extracts that already exist will be merged with the values parsed by this\n    extractor with this extractor's values taking precedence when duplicate keys\n    are detected.\n\n    Note that the use of a prediction_key in an eval_config serves two use cases:\n      (1) as a key into the dict of predictions output by predict extractor\n      (2) as the key for a pre-computed prediction stored as a feature.\n    The InputExtractor can be used to handle case (2). These cases are meant to be\n    exclusive (i.e. if approach (2) is used then a predict extractor would not be\n    configured and if (1) is used then a key matching the predictons would not be\n    stored in the features). However, if a feature key happens to match the same\n    name as the prediction output key then both paths may be executed. In this\n    case, the value stored here will be replaced by the predict extractor (though\n    it will still be popped from the features).\n\n    Args:\n    ----\n      eval_config: Eval config.\n\n    Returns:\n    -------\n      Extractor for extracting features, labels, and example weights inputs.\n    \"\"\"\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=_INPUT_EXTRACTOR_STAGE_NAME,\n        ptransform=_ExtractInputs(eval_config=eval_config),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.LabelsExtractor","title":"LabelsExtractor","text":"<pre><code>LabelsExtractor(eval_config: EvalConfig) -&gt; Extractor\n</code></pre> <p>Creates an extractor for extracting labels.</p> <p>The extractor's PTransform uses the config's ModelSpec.label_key(s) to lookup the associated label values stored as features under the tfma.FEATURES_KEY (and optionally tfma.TRANSFORMED_FEATURES_KEY) in extracts. The resulting values are then added to the extracts under the key tfma.LABELS_KEY.</p> <p>eval_config: Eval config.</p> <p>Extractor for extracting labels.</p> Source code in <code>tensorflow_model_analysis/extractors/labels_extractor.py</code> <pre><code>def LabelsExtractor(eval_config: config_pb2.EvalConfig) -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for extracting labels.\n\n    The extractor's PTransform uses the config's ModelSpec.label_key(s) to lookup\n    the associated label values stored as features under the tfma.FEATURES_KEY\n    (and optionally tfma.TRANSFORMED_FEATURES_KEY) in extracts. The resulting\n    values are then added to the extracts under the key tfma.LABELS_KEY.\n\n    Args:\n    ----\n      eval_config: Eval config.\n\n    Returns:\n    -------\n      Extractor for extracting labels.\n    \"\"\"\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=LABELS_EXTRACTOR_STAGE_NAME,\n        ptransform=_ExtractLabels(eval_config=eval_config),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.PredictExtractor","title":"PredictExtractor","text":"<pre><code>PredictExtractor(\n    eval_shared_model: MaybeMultipleEvalSharedModels,\n    desired_batch_size: Optional[int] = None,\n    materialize: Optional[bool] = True,\n    eval_config: Optional[EvalConfig] = None,\n) -&gt; Extractor\n</code></pre> <p>Creates an Extractor for TFMAPredict.</p> <p>The extractor's PTransform loads and runs the eval_saved_model against every example yielding a copy of the Extracts input with an additional extract of type FeaturesPredictionsLabels keyed by tfma.FEATURES_PREDICTIONS_LABELS_KEY unless eval_config is not None in which case the features, predictions, and labels will be stored separately under tfma.FEATURES_KEY, tfma.PREDICTIONS_KEY, and tfma.LABELS_KEY respectively.</p> <p>eval_shared_model: Shared model (single-model evaluation) or list of shared     models (multi-model evaluation).   desired_batch_size: Optional batch size for batching in Aggregate.   materialize: True to call the FeatureExtractor to add MaterializedColumn     entries for the features, predictions, and labels.   eval_config: Eval config.</p> <p>Extractor for extracting features, predictions, labels, and other tensors   during predict.</p> Source code in <code>tensorflow_model_analysis/extractors/legacy_predict_extractor.py</code> <pre><code>def PredictExtractor(  # pylint: disable=invalid-name\n    eval_shared_model: types.MaybeMultipleEvalSharedModels,\n    desired_batch_size: Optional[int] = None,\n    materialize: Optional[bool] = True,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n) -&gt; extractor.Extractor:\n    \"\"\"Creates an Extractor for TFMAPredict.\n\n    The extractor's PTransform loads and runs the eval_saved_model against every\n    example yielding a copy of the Extracts input with an additional extract\n    of type FeaturesPredictionsLabels keyed by\n    tfma.FEATURES_PREDICTIONS_LABELS_KEY unless eval_config is not None in which\n    case the features, predictions, and labels will be stored separately under\n    tfma.FEATURES_KEY, tfma.PREDICTIONS_KEY, and tfma.LABELS_KEY respectively.\n\n    Args:\n    ----\n      eval_shared_model: Shared model (single-model evaluation) or list of shared\n        models (multi-model evaluation).\n      desired_batch_size: Optional batch size for batching in Aggregate.\n      materialize: True to call the FeatureExtractor to add MaterializedColumn\n        entries for the features, predictions, and labels.\n      eval_config: Eval config.\n\n    Returns:\n    -------\n      Extractor for extracting features, predictions, labels, and other tensors\n      during predict.\n    \"\"\"\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=_PREDICT_EXTRACTOR_STAGE_NAME,\n        ptransform=_TFMAPredict(\n            eval_shared_models={m.model_name: m for m in eval_shared_models},\n            desired_batch_size=desired_batch_size,\n            materialize=materialize,\n            eval_config=eval_config,\n        ),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.PredictionsExtractor","title":"PredictionsExtractor","text":"<pre><code>PredictionsExtractor(\n    eval_config: EvalConfig,\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    output_keypath: Sequence[str] = (PREDICTIONS_KEY,),\n) -&gt; Extractor\n</code></pre> <p>Creates an extractor for performing predictions over a batch.</p> <p>The extractor's PTransform loads and runs the serving saved_model(s) against every Extracts yielding a copy of the incoming Extracts with an additional Extracts added for the predictions keyed by tfma.PREDICTIONS_KEY. The model inputs are searched for under tfma.FEATURES_KEY (keras only) or tfma.INPUT_KEY (if tfma.FEATURES_KEY is not set or the model is non-keras). If multiple models are used the predictions will be stored in a dict keyed by model name.</p> <p>Note that the prediction_key in the ModelSpecs also serves as a key into the dict of the prediction's output.</p> <p>eval_config: Eval config.   eval_shared_model: Shared model (single-model evaluation) or list of shared     models (multi-model evaluation) or None (predictions obtained from     features).   output_keypath: A sequence of keys to be used as the path to traverse and     insert the outputs in the extract.</p> <p>Extractor for extracting predictions.</p> Source code in <code>tensorflow_model_analysis/extractors/predictions_extractor.py</code> <pre><code>def PredictionsExtractor(\n    eval_config: config_pb2.EvalConfig,\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    output_keypath: Sequence[str] = (constants.PREDICTIONS_KEY,),\n) -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for performing predictions over a batch.\n\n    The extractor's PTransform loads and runs the serving saved_model(s) against\n    every Extracts yielding a copy of the incoming Extracts with an additional\n    Extracts added for the predictions keyed by tfma.PREDICTIONS_KEY. The model\n    inputs are searched for under tfma.FEATURES_KEY (keras only) or tfma.INPUT_KEY\n    (if tfma.FEATURES_KEY is not set or the model is non-keras). If multiple\n    models are used the predictions will be stored in a dict keyed by model name.\n\n    Note that the prediction_key in the ModelSpecs also serves as a key into the\n    dict of the prediction's output.\n\n    Args:\n    ----\n      eval_config: Eval config.\n      eval_shared_model: Shared model (single-model evaluation) or list of shared\n        models (multi-model evaluation) or None (predictions obtained from\n        features).\n      output_keypath: A sequence of keys to be used as the path to traverse and\n        insert the outputs in the extract.\n\n    Returns:\n    -------\n      Extractor for extracting predictions.\n    \"\"\"\n    # TODO(b/239975835): Remove this Optional support for version 1.0.\n    if eval_shared_model is None:\n        logging.warning(\n            \"Calling the PredictionsExtractor with eval_shared_model=None is \"\n            \"deprecated and no longer supported. This will break in version 1.0. \"\n            \"Please update your implementation to call \"\n            \"MaterializedPredictionsExtractor directly.\"\n        )\n        _, ptransform = (\n            materialized_predictions_extractor.MaterializedPredictionsExtractor(\n                eval_config, output_keypath=output_keypath\n            )\n        )\n        # Note we are changing the stage name here for backwards compatibility. Old\n        # clients expect these code paths to have the same stage name. New clients\n        # should never reference the private stage name.\n        return extractor.Extractor(\n            stage_name=PREDICTIONS_EXTRACTOR_STAGE_NAME, ptransform=ptransform\n        )\n\n    return extractor.Extractor(\n        stage_name=PREDICTIONS_EXTRACTOR_STAGE_NAME,\n        ptransform=_ModelSignaturesInferenceWrapper(  # pylint: disable=no-value-for-parameter\n            model_specs=list(eval_config.model_specs),\n            eval_shared_model=eval_shared_model,\n            output_keypath=output_keypath,\n        ),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.SliceKeyExtractor","title":"SliceKeyExtractor","text":"<pre><code>SliceKeyExtractor(\n    slice_spec: Optional[List[SingleSliceSpec]] = None,\n    eval_config: Optional[EvalConfig] = None,\n    materialize: Optional[bool] = True,\n) -&gt; Extractor\n</code></pre> <p>Creates an extractor for extracting slice keys.</p> <p>The incoming Extracts must contain features stored under tfma.FEATURES_KEY and optionally under tfma.TRANSFORMED_FEATURES.</p> <p>The extractor's PTransform yields a copy of the Extracts input with an additional extract pointing at the list of SliceKeyType values keyed by tfma.SLICE_KEY_TYPES_KEY. If materialize is True then a materialized version of the slice keys will be added under the key tfma.SLICE_KEYS_KEY.</p> <p>slice_spec: Deprecated (use EvalConfig).   eval_config: Optional EvalConfig containing slicing_specs specifying the     slices to slice the data into. If slicing_specs are empty, defaults to     overall slice.   materialize: True to add MaterializedColumn entries for the slice keys.</p> <p>Extractor for slice keys.</p> Source code in <code>tensorflow_model_analysis/extractors/slice_key_extractor.py</code> <pre><code>def SliceKeyExtractor(\n    slice_spec: Optional[List[slicer.SingleSliceSpec]] = None,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    materialize: Optional[bool] = True,\n) -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for extracting slice keys.\n\n    The incoming Extracts must contain features stored under tfma.FEATURES_KEY\n    and optionally under tfma.TRANSFORMED_FEATURES.\n\n    The extractor's PTransform yields a copy of the Extracts input with an\n    additional extract pointing at the list of SliceKeyType values keyed by\n    tfma.SLICE_KEY_TYPES_KEY. If materialize is True then a materialized version\n    of the slice keys will be added under the key tfma.SLICE_KEYS_KEY.\n\n    Args:\n    ----\n      slice_spec: Deprecated (use EvalConfig).\n      eval_config: Optional EvalConfig containing slicing_specs specifying the\n        slices to slice the data into. If slicing_specs are empty, defaults to\n        overall slice.\n      materialize: True to add MaterializedColumn entries for the slice keys.\n\n    Returns:\n    -------\n      Extractor for slice keys.\n    \"\"\"\n    if slice_spec and eval_config:\n        raise ValueError(\"slice_spec is deprecated, only use eval_config\")\n    if eval_config:\n        slice_spec = [\n            slicer.SingleSliceSpec(spec=spec) for spec in eval_config.slicing_specs\n        ]\n        for cross_slice_spec in eval_config.cross_slicing_specs:\n            baseline_slice_spec = slicer.SingleSliceSpec(\n                spec=cross_slice_spec.baseline_spec\n            )\n            if baseline_slice_spec not in slice_spec:\n                slice_spec.append(baseline_slice_spec)\n            for spec in cross_slice_spec.slicing_specs:\n                comparison_slice_spec = slicer.SingleSliceSpec(spec=spec)\n                if comparison_slice_spec not in slice_spec:\n                    slice_spec.append(comparison_slice_spec)\n    if not slice_spec:\n        slice_spec = [slicer.SingleSliceSpec()]\n    return extractor.Extractor(\n        stage_name=SLICE_KEY_EXTRACTOR_STAGE_NAME,\n        ptransform=ExtractSliceKeys(slice_spec, eval_config, materialize),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.TransformedFeaturesExtractor","title":"TransformedFeaturesExtractor","text":"<pre><code>TransformedFeaturesExtractor(\n    eval_config: EvalConfig,\n    eval_shared_model: MaybeMultipleEvalSharedModels,\n) -&gt; Extractor\n</code></pre> <p>Creates an extractor for extracting transformed features.</p> <p>The extractor's PTransform loads the saved_model(s) invoking the preprocessing functions against every extract yielding a copy of the incoming extracts with a tfma.TRANSFORMED_FEATURES_KEY containing the output from the preprocessing functions.</p> <p>eval_config: Eval config.   eval_shared_model: Shared model (single-model evaluation) or list of shared     models (multi-model evaluation).</p> <p>Extractor for extracting preprocessed features.</p> Source code in <code>tensorflow_model_analysis/extractors/transformed_features_extractor.py</code> <pre><code>def TransformedFeaturesExtractor(\n    eval_config: config_pb2.EvalConfig,\n    eval_shared_model: types.MaybeMultipleEvalSharedModels,\n) -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for extracting transformed features.\n\n    The extractor's PTransform loads the saved_model(s) invoking the preprocessing\n    functions against every extract yielding a copy of the incoming extracts with\n    a tfma.TRANSFORMED_FEATURES_KEY containing the output from the preprocessing\n    functions.\n\n    Args:\n    ----\n      eval_config: Eval config.\n      eval_shared_model: Shared model (single-model evaluation) or list of shared\n        models (multi-model evaluation).\n\n    Returns:\n    -------\n      Extractor for extracting preprocessed features.\n    \"\"\"\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=_TRANSFORMED_FEATURES_EXTRACTOR_STAGE_NAME,\n        ptransform=_ExtractTransformedFeatures(\n            eval_config=eval_config,\n            eval_shared_models={m.model_name: m for m in eval_shared_models},\n        ),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.UnbatchExtractor","title":"UnbatchExtractor","text":"<pre><code>UnbatchExtractor() -&gt; Extractor\n</code></pre> <p>Creates an extractor for unbatching batched extracts.</p> <p>This extractor removes Arrow RecordBatch from the batched extract and outputs per-example extracts with the remaining keys. We assume that the remaining keys in the input extract contain list of objects (one per example).</p>"},{"location":"api_docs/python/tfma-extractors/#tensorflow_model_analysis.extractors.UnbatchExtractor--returns","title":"Returns","text":"<p>Extractor for unbatching batched extracts.</p> Source code in <code>tensorflow_model_analysis/extractors/unbatch_extractor.py</code> <pre><code>def UnbatchExtractor() -&gt; extractor.Extractor:\n    \"\"\"Creates an extractor for unbatching batched extracts.\n\n    This extractor removes Arrow RecordBatch from the batched extract and outputs\n    per-example extracts with the remaining keys. We assume that the remaining\n    keys in the input extract contain list of objects (one per example).\n\n    Returns\n    -------\n      Extractor for unbatching batched extracts.\n    \"\"\"\n    # pylint: disable=no-value-for-parameter\n    return extractor.Extractor(\n        stage_name=UNBATCH_EXTRACTOR_STAGE_NAME, ptransform=_UnbatchInputs()\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/","title":"TFMA Metrics","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics","title":"tensorflow_model_analysis.metrics","text":"<p>Init module for TensorFlow Model Analysis metrics.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricComputations","title":"MetricComputations  <code>module-attribute</code>","text":"<pre><code>MetricComputations = List[\n    Union[\n        MetricComputation,\n        DerivedMetricComputation,\n        CrossSliceMetricComputation,\n        CIDerivedMetricComputation,\n    ]\n]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricsDict","title":"MetricsDict  <code>module-attribute</code>","text":"<pre><code>MetricsDict = Dict[MetricKey, MetricValueType]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NO_PREDICTED_CLASS_ID","title":"NO_PREDICTED_CLASS_ID  <code>module-attribute</code>","text":"<pre><code>NO_PREDICTED_CLASS_ID = -1\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC","title":"AUC","text":"<pre><code>AUC(\n    num_thresholds: Optional[int] = None,\n    curve: str = \"ROC\",\n    summation_method: str = \"interpolation\",\n    name: Optional[str] = None,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetricBase</code></p> <p>Approximates the AUC (Area under the curve) of the ROC or PR curves.</p> <p>The AUC (Area under the curve) of the ROC (Receiver operating characteristic; default) or PR (Precision Recall) curves are quality measures of binary classifiers. Unlike the accuracy, and like cross-entropy losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.</p> <p>This class approximates AUCs using a Riemann sum. During the metric accumulation phase, predictions are accumulated within predefined buckets by value. The AUC is then computed by interpolating per-bucket averages. These buckets define the evaluated operational points.</p> <p>This metric uses <code>true_positives</code>, <code>true_negatives</code>, <code>false_positives</code> and <code>false_negatives</code> to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall.</p> <p>This value is ultimately returned as <code>auc</code>, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The <code>num_thresholds</code> variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on <code>num_thresholds</code>. The <code>thresholds</code> parameter can be used to manually specify thresholds which split the predictions more evenly.</p> <p>For a best approximation of the real AUC, <code>predictions</code> should be distributed approximately uniformly in the range [0, 1]. The quality of the AUC approximation may be poor if this is not the case. Setting <code>summation_method</code> to 'minoring' or 'majoring' can help quantify the error in the approximation by providing lower or upper bound estimate of the AUC.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes AUC metric.</p> <p>num_thresholds: (Optional) Defaults to 10000. The number of thresholds to     use when discretizing the roc curve. Values must be &gt; 1.   curve: (Optional) Specifies the name of the curve to be computed, 'ROC'     [default] or 'PR' for the Precision-Recall-curve.   summation_method: (Optional) Specifies the Riemann summation method used. 'interpolation'       (default) applies mid-point summation scheme for <code>ROC</code>. For PR-AUC,       interpolates (true/false) positives but not the ratio that is       precision (see Davis &amp; Goadrich 2006 for details); 'minoring' applies       left summation for increasing intervals and right summation for       decreasing intervals; 'majoring' does the opposite.   name: (Optional) string name of the metric instance.   thresholds: (Optional) A list of floating point values to use as the     thresholds for discretizing the curve. If set, the <code>num_thresholds</code>     parameter is ignored. Values should be in [0, 1]. Endpoint thresholds     equal to {-epsilon, 1+epsilon} for a small positive epsilon value will     be automatically included with these to correctly handle predictions     equal to exactly 0 or 1.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    num_thresholds: Optional[int] = None,\n    curve: str = \"ROC\",\n    summation_method: str = \"interpolation\",\n    name: Optional[str] = None,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes AUC metric.\n\n    Args:\n    ----\n      num_thresholds: (Optional) Defaults to 10000. The number of thresholds to\n        use when discretizing the roc curve. Values must be &gt; 1.\n      curve: (Optional) Specifies the name of the curve to be computed, 'ROC'\n        [default] or 'PR' for the Precision-Recall-curve.\n      summation_method: (Optional) Specifies the [Riemann summation method](\n        https://en.wikipedia.org/wiki/Riemann_sum) used. 'interpolation'\n          (default) applies mid-point summation scheme for `ROC`. For PR-AUC,\n          interpolates (true/false) positives but not the ratio that is\n          precision (see Davis &amp; Goadrich 2006 for details); 'minoring' applies\n          left summation for increasing intervals and right summation for\n          decreasing intervals; 'majoring' does the opposite.\n      name: (Optional) string name of the metric instance.\n      thresholds: (Optional) A list of floating point values to use as the\n        thresholds for discretizing the curve. If set, the `num_thresholds`\n        parameter is ignored. Values should be in [0, 1]. Endpoint thresholds\n        equal to {-epsilon, 1+epsilon} for a small positive epsilon value will\n        be automatically included with these to correctly handle predictions\n        equal to exactly 0 or 1.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        num_thresholds=num_thresholds,\n        thresholds=thresholds,\n        curve=curve,\n        summation_method=summation_method,\n        name=name,\n        top_k=top_k,\n        class_id=class_id,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUC.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCCurve","title":"AUCCurve","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCCurve-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCCurve.PR","title":"PR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PR = 'PR'\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCCurve.ROC","title":"ROC  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ROC = 'ROC'\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall","title":"AUCPrecisionRecall","text":"<pre><code>AUCPrecisionRecall(\n    num_thresholds: Optional[int] = None,\n    summation_method: str = \"interpolation\",\n    name: Optional[str] = None,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>AUC</code></p> <p>Alias for AUC(curve='PR').</p> <p>Initializes AUCPrecisionRecall metric.</p> <p>num_thresholds: (Optional) Defaults to 10000. The number of thresholds to     use when discretizing the roc curve. Values must be &gt; 1.   summation_method: (Optional) Specifies the Riemann summation method used. 'interpolation'       interpolates (true/false) positives but not the ratio that is       precision (see Davis &amp; Goadrich 2006 for details); 'minoring' applies       left summation for increasing intervals and right summation for       decreasing intervals; 'majoring' does the opposite.   name: (Optional) string name of the metric instance.   thresholds: (Optional) A list of floating point values to use as the     thresholds for discretizing the curve. If set, the <code>num_thresholds</code>     parameter is ignored. Values should be in [0, 1]. Endpoint thresholds     equal to {-epsilon, 1+epsilon} for a small positive epsilon value will     be automatically included with these to correctly handle predictions     equal to exactly 0 or 1.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    num_thresholds: Optional[int] = None,\n    summation_method: str = \"interpolation\",\n    name: Optional[str] = None,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes AUCPrecisionRecall metric.\n\n    Args:\n    ----\n      num_thresholds: (Optional) Defaults to 10000. The number of thresholds to\n        use when discretizing the roc curve. Values must be &gt; 1.\n      summation_method: (Optional) Specifies the [Riemann summation method](\n        https://en.wikipedia.org/wiki/Riemann_sum) used. 'interpolation'\n          interpolates (true/false) positives but not the ratio that is\n          precision (see Davis &amp; Goadrich 2006 for details); 'minoring' applies\n          left summation for increasing intervals and right summation for\n          decreasing intervals; 'majoring' does the opposite.\n      name: (Optional) string name of the metric instance.\n      thresholds: (Optional) A list of floating point values to use as the\n        thresholds for discretizing the curve. If set, the `num_thresholds`\n        parameter is ignored. Values should be in [0, 1]. Endpoint thresholds\n        equal to {-epsilon, 1+epsilon} for a small positive epsilon value will\n        be automatically included with these to correctly handle predictions\n        equal to exactly 0 or 1.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        num_thresholds=num_thresholds,\n        thresholds=thresholds,\n        curve=\"PR\",\n        summation_method=summation_method,\n        name=name,\n        top_k=top_k,\n        class_id=class_id,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCPrecisionRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Remove the irrelevant 'curve' keyword inherited from parent class AUC().\n    # This is needed when the __init__ of the child class has a different set of\n    # kwargs than that of its parent class.\n    result = super().get_config()\n    del result[\"curve\"]\n    return result\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCSummationMethod","title":"AUCSummationMethod","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCSummationMethod-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCSummationMethod.INTERPOLATION","title":"INTERPOLATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INTERPOLATION = 'interpolation'\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCSummationMethod.MAJORING","title":"MAJORING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAJORING = 'majoring'\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AUCSummationMethod.MINORING","title":"MINORING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MINORING = 'minoring'\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric","title":"AttributionsMetric","text":"<pre><code>AttributionsMetric(\n    create_computations_fn: Callable[\n        ..., MetricComputations\n    ],\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Base type for attribution metrics.</p> <p>Initializes metric.</p> <p>create_computations_fn: Function to create the metrics computations (e.g.     mean_label, etc). This function should take the args passed to init     as as input along with any of eval_config, schema, model_names,     output_names, sub_keys, aggregation_type, or query_key (where needed).   **kwargs: Any additional kwargs to pass to create_computations_fn. These     should only contain primitive types or lists/dicts of primitive types.     The kwargs passed to computations have precendence over these kwargs.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def __init__(\n    self, create_computations_fn: Callable[..., MetricComputations], **kwargs\n):\n    \"\"\"Initializes metric.\n\n    Args:\n    ----\n      create_computations_fn: Function to create the metrics computations (e.g.\n        mean_label, etc). This function should take the args passed to __init__\n        as as input along with any of eval_config, schema, model_names,\n        output_names, sub_keys, aggregation_type, or query_key (where needed).\n      **kwargs: Any additional kwargs to pass to create_computations_fn. These\n        should only contain primitive types or lists/dicts of primitive types.\n        The kwargs passed to computations have precendence over these kwargs.\n    \"\"\"\n    self.create_computations_fn = create_computations_fn\n    if \"name\" in kwargs:\n        if not kwargs[\"name\"] and self._default_name():\n            kwargs[\"name\"] = self._default_name()  # pylint: disable=assignment-from-none\n        name = kwargs[\"name\"]\n    else:\n        name = None\n    self.name = name\n    self.kwargs = kwargs\n    if hasattr(inspect, \"getfullargspec\"):\n        self._args = inspect.getfullargspec(self.create_computations_fn).args\n    else:\n        self._args = inspect.getargspec(self.create_computations_fn).args  # pylint: disable=deprecated-method\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.AttributionsMetric.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy","title":"BalancedAccuracy","text":"<pre><code>BalancedAccuracy(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Balanced accuracy (BA).</p> <p>Initializes balanced accuracy.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes balanced accuracy.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BalancedAccuracy.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    tpr_denominator = tp + fn\n    tnr_denominator = tn + fp\n    if tpr_denominator &gt; 0.0 and tnr_denominator &gt; 0.0:\n        tpr = tp / tpr_denominator\n        tnr = tn / tnr_denominator\n        return (tpr + tnr) / 2\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy","title":"BinaryAccuracy","text":"<pre><code>BinaryAccuracy(\n    threshold: Optional[float] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Calculates how often predictions match binary labels.</p> <p>This metric computes the accuracy based on (TP + TN) / (TP + FP + TN + FN).</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes BinaryAccuracy metric.</p> <p>threshold: (Optional) A float value in [0, 1]. The threshold is compared     with prediction values to determine the truth value of predictions     (i.e., above the threshold is <code>true</code>, below is <code>false</code>). If neither     threshold nor top_k are set, the default is to calculate with     <code>threshold=0.5</code>.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    threshold: Optional[float] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initializes BinaryAccuracy metric.\n\n    Args:\n    ----\n      threshold: (Optional) A float value in [0, 1]. The threshold is compared\n        with prediction values to determine the truth value of predictions\n        (i.e., above the threshold is `true`, below is `false`). If neither\n        threshold nor top_k are set, the default is to calculate with\n        `threshold=0.5`.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n    \"\"\"\n    super().__init__(\n        thresholds=threshold, top_k=top_k, class_id=class_id, name=name\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryAccuracy.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return _divide_only_positive_denominator(tp + tn, tp + fp + tn + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy","title":"BinaryCrossEntropy","text":"<pre><code>BinaryCrossEntropy(\n    name: str = BINARY_CROSSENTROPY_NAME,\n    from_logits: bool = False,\n    label_smoothing: float = 0.0,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calculates the binary cross entropy.</p> <p>The metric computes the cross entropy when there are only two label classes (0 and 1). See definition at: https://en.wikipedia.org/wiki/Cross_entropy</p> <p>Initializes binary cross entropy metric.</p> <p>name: The name of the metric.   from_logits: (Optional) Whether output is expected to be a logits tensor.     By default, we consider that output encodes a probability distribution.   label_smoothing: Float in [0, 1]. If &gt; <code>0</code> then smooth the labels by     squeezing them towards 0.5 That is, using <code>1. - 0.5 * label_smoothing</code>     for the target class and <code>0.5 * label_smoothing</code> for the non-target     class.</p> Source code in <code>tensorflow_model_analysis/metrics/cross_entropy_metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = BINARY_CROSSENTROPY_NAME,\n    from_logits: bool = False,\n    label_smoothing: float = 0.0,\n):\n    \"\"\"Initializes binary cross entropy metric.\n\n    Args:\n    ----\n      name: The name of the metric.\n      from_logits: (Optional) Whether output is expected to be a logits tensor.\n        By default, we consider that output encodes a probability distribution.\n      label_smoothing: Float in [0, 1]. If &gt; `0` then smooth the labels by\n        squeezing them towards 0.5 That is, using `1. - 0.5 * label_smoothing`\n        for the target class and `0.5 * label_smoothing` for the non-target\n        class.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_binary_cross_entropy_computations),\n        name=name,\n        from_logits=from_logits,\n        label_smoothing=label_smoothing,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BinaryCrossEntropy.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates","title":"BooleanFlipRates","text":"<pre><code>BooleanFlipRates(\n    threshold: float = _DEFAULT_FLIP_RATE_THRESHOLD,\n    flip_rate_name: str = FLIP_RATE_NAME,\n    neg_to_neg_flip_rate_name: str = NEG_TO_NEG_FLIP_RATE_NAME,\n    neg_to_pos_flip_rate_name: str = NEG_TO_POS_FLIP_RATE_NAME,\n    pos_to_neg_flip_rate_name: str = POS_TO_NEG_FLIP_RATE_NAME,\n    pos_to_pos_flip_rate_name: str = POS_TO_POS_FLIP_RATE_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>FlipRate is the rate at which predictions between models switch.</p> <p>Given a pair of models and a threshold for converting continuous model outputs into boolean predictions, this metric will produce three numbers (keyed by separate MetricKeys):</p> <ul> <li>(symmetric) flip rate: The number of times the boolean predictions don't     match, regardless of the direction of the flip.</li> <li>negative-to-positive flip rate: The rate at which the baseline model's     boolean prediction is negative but the candidate model's is positive.</li> <li>positive-to-negative flip rate: The rate at which the baseline model's     boolean prediction is positive but the candidate model's is negative.</li> </ul> <p>Initializes BooleanFlipRates metric.</p> <p>threshold: The threshold to use for converting the model prediction into a     boolean value that can be used for comparison between models.   flip_rate_name: Metric name for symmetric flip rate.   neg_to_neg_flip_rate_name: Metric name for the negative-to-negative flip     rate.   neg_to_pos_flip_rate_name: Metric name for the negative-to-positive flip     rate.   pos_to_neg_flip_rate_name: Metric name for the positive-to-negative flip     rate.   pos_to_pos_flip_rate_name: Metric name for the positive-to-positive flip     rate.</p> Source code in <code>tensorflow_model_analysis/metrics/flip_metrics.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = _DEFAULT_FLIP_RATE_THRESHOLD,\n    flip_rate_name: str = FLIP_RATE_NAME,\n    neg_to_neg_flip_rate_name: str = NEG_TO_NEG_FLIP_RATE_NAME,\n    neg_to_pos_flip_rate_name: str = NEG_TO_POS_FLIP_RATE_NAME,\n    pos_to_neg_flip_rate_name: str = POS_TO_NEG_FLIP_RATE_NAME,\n    pos_to_pos_flip_rate_name: str = POS_TO_POS_FLIP_RATE_NAME,\n):\n    \"\"\"Initializes BooleanFlipRates metric.\n\n    Args:\n    ----\n      threshold: The threshold to use for converting the model prediction into a\n        boolean value that can be used for comparison between models.\n      flip_rate_name: Metric name for symmetric flip rate.\n      neg_to_neg_flip_rate_name: Metric name for the negative-to-negative flip\n        rate.\n      neg_to_pos_flip_rate_name: Metric name for the negative-to-positive flip\n        rate.\n      pos_to_neg_flip_rate_name: Metric name for the positive-to-negative flip\n        rate.\n      pos_to_pos_flip_rate_name: Metric name for the positive-to-positive flip\n        rate.\n    \"\"\"\n    super().__init__(\n        _boolean_flip_rates_computations,\n        symmetric_flip_rate_name=flip_rate_name,\n        neg_to_neg_flip_rate_name=neg_to_neg_flip_rate_name,\n        neg_to_pos_flip_rate_name=neg_to_pos_flip_rate_name,\n        pos_to_neg_flip_rate_name=pos_to_neg_flip_rate_name,\n        pos_to_pos_flip_rate_name=pos_to_pos_flip_rate_name,\n        threshold=threshold,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.BooleanFlipRates.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision","title":"COCOAveragePrecision","text":"<pre><code>COCOAveragePrecision(\n    num_thresholds: Optional[int] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    recalls: Optional[List[float]] = None,\n    num_recalls: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Confusion matrix at thresholds.</p> <p>It computes the average precision of object detections for a single class and a single iou_threshold.</p> <p>Initialize average precision metric.</p> <p>This metric is only used in object-detection setting. It does not support sub_key parameters due to the matching algorithm of bounding boxes.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>num_thresholds: (Optional) Number of thresholds to use for calculating the     matrices and finding the precision at given recall.   iou_threshold: (Optional) Threholds for a detection and ground truth pair     with specific iou to be considered as a match.   class_id: (Optional) The class id for calculating metrics.   class_weight: (Optional) The weight associated with the object class id.   area_range: (Optional) The area-range for objects to be considered for     metrics.   max_num_detections: (Optional) The maximum number of detections for a     single image.   recalls: (Optional) recalls at which precisions will be calculated.   num_recalls: (Optional) Used for objecth detection, the number of recalls     for calculating average precision, it equally generates points bewteen 0     and 1. (Only one of recalls and num_recalls should be used).   name: (Optional) string name of the metric instance.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_metrics.py</code> <pre><code>def __init__(\n    self,\n    num_thresholds: Optional[int] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    recalls: Optional[List[float]] = None,\n    num_recalls: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initialize average precision metric.\n\n    This metric is only used in object-detection setting. It does not support\n    sub_key parameters due to the matching algorithm of bounding boxes.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      num_thresholds: (Optional) Number of thresholds to use for calculating the\n        matrices and finding the precision at given recall.\n      iou_threshold: (Optional) Threholds for a detection and ground truth pair\n        with specific iou to be considered as a match.\n      class_id: (Optional) The class id for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class id.\n      area_range: (Optional) The area-range for objects to be considered for\n        metrics.\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image.\n      recalls: (Optional) recalls at which precisions will be calculated.\n      num_recalls: (Optional) Used for objecth detection, the number of recalls\n        for calculating average precision, it equally generates points bewteen 0\n        and 1. (Only one of recalls and num_recalls should be used).\n      name: (Optional) string name of the metric instance.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    if recalls is not None:\n        recall_thresholds = recalls\n    elif num_recalls is not None:\n        recall_thresholds = np.linspace(0.0, 1.0, num_recalls)\n    else:\n        # by default set recall_thresholds to [0.0:0.01:1.0].\n        recall_thresholds = np.linspace(0.0, 1.0, 101)\n\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        num_thresholds=num_thresholds,\n        iou_threshold=iou_threshold,\n        class_id=class_id,\n        class_weight=class_weight,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        recall_thresholds=recall_thresholds,\n        name=name,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAveragePrecision.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall","title":"COCOAverageRecall","text":"<pre><code>COCOAverageRecall(\n    iou_thresholds: Optional[List[float]] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Average recall metric for object detection.</p> <p>It computes the average precision metric for object detections for a single class. It averages MaxRecall metric over mulitple IoU thresholds.</p> <p>Initializes average recall metric.</p> <p>This metric is only used in object-detection setting. It does not support sub_key parameters due to the matching algorithm of bounding boxes.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>iou_thresholds: (Optional) Threholds for a detection and ground truth pair     with specific iou to be considered as a match.   class_id: (Optional) The class ids for calculating metrics.   class_weight: (Optional) The weight associated with the object class ids.     If it is provided, it should have the same length as class_ids.   area_range: (Optional) The area-range for objects to be considered for     metrics.   max_num_detections: (Optional) The maximum number of detections for a     single image.   name: (Optional) Metric name.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_metrics.py</code> <pre><code>def __init__(\n    self,\n    iou_thresholds: Optional[List[float]] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes average recall metric.\n\n    This metric is only used in object-detection setting. It does not support\n    sub_key parameters due to the matching algorithm of bounding boxes.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      iou_thresholds: (Optional) Threholds for a detection and ground truth pair\n        with specific iou to be considered as a match.\n      class_id: (Optional) The class ids for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class ids.\n        If it is provided, it should have the same length as class_ids.\n      area_range: (Optional) The area-range for objects to be considered for\n        metrics.\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image.\n      name: (Optional) Metric name.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        iou_thresholds=iou_thresholds,\n        class_id=class_id,\n        class_weight=class_weight,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        name=name,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOAverageRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision","title":"COCOMeanAveragePrecision","text":"<pre><code>COCOMeanAveragePrecision(\n    num_thresholds: Optional[int] = None,\n    iou_thresholds: Optional[List[float]] = None,\n    class_ids: Optional[List[int]] = None,\n    class_weights: Optional[List[float]] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    recalls: Optional[List[float]] = None,\n    num_recalls: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Mean average precision for object detections.</p> <p>It calculates the mean average precision metric for object detections. It averages COCOAveragePrecision over multiple classes and IoU thresholds.</p> <p>Initializes mean average precision metric.</p> <p>This metric is only used in object-detection setting. It does not support sub_key parameters due to the matching algorithm of bounding boxes.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>num_thresholds: (Optional) Number of thresholds to use for calculating the     matrices and finding the precision at given recall.   iou_thresholds: (Optional) Threholds for a detection and ground truth pair     with specific iou to be considered as a match.   class_ids: (Optional) The class ids for calculating metrics.   class_weights: (Optional) The weight associated with the object class ids.     If it is provided, it should have the same length as class_ids.   area_range: (Optional) The area-range for objects to be considered for     metrics.   max_num_detections: (Optional) The maximum number of detections for a     single image.   recalls: (Optional) recalls at which precisions will be calculated.   num_recalls: (Optional) Used for objecth detection, the number of recalls     for calculating average precision, it equally generates points bewteen 0     and 1. (Only one of recalls and num_recalls should be used).   name: (Optional) Metric name.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_metrics.py</code> <pre><code>def __init__(\n    self,\n    num_thresholds: Optional[int] = None,\n    iou_thresholds: Optional[List[float]] = None,\n    class_ids: Optional[List[int]] = None,\n    class_weights: Optional[List[float]] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    recalls: Optional[List[float]] = None,\n    num_recalls: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes mean average precision metric.\n\n    This metric is only used in object-detection setting. It does not support\n    sub_key parameters due to the matching algorithm of bounding boxes.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      num_thresholds: (Optional) Number of thresholds to use for calculating the\n        matrices and finding the precision at given recall.\n      iou_thresholds: (Optional) Threholds for a detection and ground truth pair\n        with specific iou to be considered as a match.\n      class_ids: (Optional) The class ids for calculating metrics.\n      class_weights: (Optional) The weight associated with the object class ids.\n        If it is provided, it should have the same length as class_ids.\n      area_range: (Optional) The area-range for objects to be considered for\n        metrics.\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image.\n      recalls: (Optional) recalls at which precisions will be calculated.\n      num_recalls: (Optional) Used for objecth detection, the number of recalls\n        for calculating average precision, it equally generates points bewteen 0\n        and 1. (Only one of recalls and num_recalls should be used).\n      name: (Optional) Metric name.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        num_thresholds=num_thresholds,\n        iou_thresholds=iou_thresholds,\n        class_ids=class_ids,\n        class_weights=class_weights,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        recalls=recalls,\n        num_recalls=num_recalls,\n        name=name,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAveragePrecision.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall","title":"COCOMeanAverageRecall","text":"<pre><code>COCOMeanAverageRecall(\n    iou_thresholds: Optional[List[float]] = None,\n    class_ids: Optional[List[int]] = None,\n    class_weights: Optional[List[float]] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Mean Average recall metric for object detection.</p> <p>It computes the mean average precision metric for object detections for a single class. It averages COCOAverageRecall metric over mulitple classes.</p> <p>Initializes average recall metric.</p> <p>This metric is only used in object-detection setting. It does not support sub_key parameters due to the matching algorithm of bounding boxes.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>iou_thresholds: (Optional) Threholds for a detection and ground truth pair     with specific iou to be considered as a match.   class_ids: (Optional) The class ids for calculating metrics.   class_weights: (Optional) The weight associated with the object class ids.     If it is provided, it should have the same length as class_ids.   area_range: (Optional) The area-range for objects to be considered for     metrics.   max_num_detections: (Optional) The maximum number of detections for a     single image.   name: (Optional) Metric name.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_metrics.py</code> <pre><code>def __init__(\n    self,\n    iou_thresholds: Optional[List[float]] = None,\n    class_ids: Optional[List[int]] = None,\n    class_weights: Optional[List[float]] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    name: Optional[str] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes average recall metric.\n\n    This metric is only used in object-detection setting. It does not support\n    sub_key parameters due to the matching algorithm of bounding boxes.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      iou_thresholds: (Optional) Threholds for a detection and ground truth pair\n        with specific iou to be considered as a match.\n      class_ids: (Optional) The class ids for calculating metrics.\n      class_weights: (Optional) The weight associated with the object class ids.\n        If it is provided, it should have the same length as class_ids.\n      area_range: (Optional) The area-range for objects to be considered for\n        metrics.\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image.\n      name: (Optional) Metric name.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        iou_thresholds=iou_thresholds,\n        class_ids=class_ids,\n        class_weights=class_weights,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        name=name,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.COCOMeanAverageRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration","title":"Calibration","text":"<pre><code>Calibration(name: str = CALIBRATION_NAME)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calibration.</p> <p>Calibration in this context is defined as the total weighted predictions / total weighted labels.</p> <p>Initializes calibration.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/calibration.py</code> <pre><code>def __init__(self, name: str = CALIBRATION_NAME):\n    \"\"\"Initializes calibration.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_calibration), name=name\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Calibration.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot","title":"CalibrationPlot","text":"<pre><code>CalibrationPlot(\n    num_buckets: int = DEFAULT_NUM_BUCKETS,\n    left: Optional[float] = None,\n    right: Optional[float] = None,\n    name: str = CALIBRATION_PLOT_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calibration plot.</p> <p>Initializes calibration plot.</p> <p>num_buckets: Number of buckets to use when creating the plot. Defaults to     1000.   left: Left boundary of plot. Defaults to 0.0 when a schema is not     provided.   right: Right boundary of plot. Defaults to 1.0 when a schema is not     provided.   name: Plot name.</p> Source code in <code>tensorflow_model_analysis/metrics/calibration_plot.py</code> <pre><code>def __init__(\n    self,\n    num_buckets: int = DEFAULT_NUM_BUCKETS,\n    left: Optional[float] = None,\n    right: Optional[float] = None,\n    name: str = CALIBRATION_PLOT_NAME,\n):\n    \"\"\"Initializes calibration plot.\n\n    Args:\n    ----\n      num_buckets: Number of buckets to use when creating the plot. Defaults to\n        1000.\n      left: Left boundary of plot. Defaults to 0.0 when a schema is not\n        provided.\n      right: Right boundary of plot. Defaults to 1.0 when a schema is not\n        provided.\n      name: Plot name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_calibration_plot),\n        num_buckets=num_buckets,\n        left=left,\n        right=right,\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CalibrationPlot.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy","title":"CategoricalCrossEntropy","text":"<pre><code>CategoricalCrossEntropy(\n    name: str = CATEGORICAL_CROSSENTROPY_NAME,\n    from_logits: bool = False,\n    label_smoothing: float = 0.0,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calculates the categorical cross entropy.</p> <p>The metric computes the cross entropy when there are multiple classes. It outputs a numpy array.</p> <p>Initializes categorical cross entropy metric.</p> <p>name: The name of the metric.   from_logits: (Optional) Whether output is expected to be a logits tensor.     By default, we consider that output encodes a probability distribution.   label_smoothing: Float in [0, 1]. If &gt; <code>0</code> then smooth the labels. For     example, if <code>0.1</code>, use <code>0.1 / num_classes</code> for non-target labels and     <code>0.9 + 0.1 / num_classes</code> for target labels.</p> Source code in <code>tensorflow_model_analysis/metrics/cross_entropy_metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = CATEGORICAL_CROSSENTROPY_NAME,\n    from_logits: bool = False,\n    label_smoothing: float = 0.0,\n):\n    \"\"\"Initializes categorical cross entropy metric.\n\n    Args:\n    ----\n      name: The name of the metric.\n      from_logits: (Optional) Whether output is expected to be a logits tensor.\n        By default, we consider that output encodes a probability distribution.\n      label_smoothing: Float in [0, 1]. If &gt; `0` then smooth the labels. For\n        example, if `0.1`, use `0.1 / num_classes` for non-target labels and\n        `0.9 + 0.1 / num_classes` for target labels.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            _categorical_cross_entropy_computations\n        ),\n        name=name,\n        from_logits=from_logits,\n        label_smoothing=label_smoothing,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CategoricalCrossEntropy.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination","title":"CoefficientOfDiscrimination","text":"<pre><code>CoefficientOfDiscrimination(\n    name: str = COEFFICIENT_OF_DISCRIMINATION_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Coefficient of discrimination metric.</p> <p>The coefficient of discrimination measures the differences between the average prediction for the positive examples and the average prediction for the negative examples.</p> <p>The formula is: AVG(pred | label = 1) - AVG(pred | label = 0) More details can be found in the following paper: https://www.tandfonline.com/doi/abs/10.1198/tast.2009.08210</p> <p>Initializes coefficient of discrimination metric.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/tjur_discrimination.py</code> <pre><code>def __init__(self, name: str = COEFFICIENT_OF_DISCRIMINATION_NAME):\n    \"\"\"Initializes coefficient of discrimination metric.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_coefficient_of_discrimination),\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CoefficientOfDiscrimination.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds","title":"ConfusionMatrixAtThresholds","text":"<pre><code>ConfusionMatrixAtThresholds(\n    thresholds: List[float],\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Confusion matrix at thresholds.</p> <p>Initializes confusion matrix at thresholds.</p> <p>thresholds: Thresholds to use for confusion matrix.   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: List[float],\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes confusion matrix at thresholds.\n\n    Args:\n    ----\n      thresholds: Thresholds to use for confusion matrix.\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        thresholds=thresholds,\n        name=name,\n        top_k=top_k,\n        class_id=class_id,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixAtThresholds.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot","title":"ConfusionMatrixPlot","text":"<pre><code>ConfusionMatrixPlot(\n    num_thresholds: int = DEFAULT_NUM_THRESHOLDS,\n    name: str = CONFUSION_MATRIX_PLOT_NAME,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Confusion matrix plot.</p> <p>Initializes confusion matrix plot.</p> <p>num_thresholds: Number of thresholds to use when discretizing the curve.     Values must be &gt; 1. Defaults to 1000.   name: Metric name.   **kwargs: (Optional) Additional args to pass along to init (and eventually     on to _confusion_matrix_plot). These kwargs are useful for subclasses to     pass information from their init to the create_computation_fn.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_plot.py</code> <pre><code>def __init__(\n    self,\n    num_thresholds: int = DEFAULT_NUM_THRESHOLDS,\n    name: str = CONFUSION_MATRIX_PLOT_NAME,\n    **kwargs,\n):\n    \"\"\"Initializes confusion matrix plot.\n\n    Args:\n    ----\n      num_thresholds: Number of thresholds to use when discretizing the curve.\n        Values must be &gt; 1. Defaults to 1000.\n      name: Metric name.\n      **kwargs: (Optional) Additional args to pass along to init (and eventually\n        on to _confusion_matrix_plot). These kwargs are useful for subclasses to\n        pass information from their init to the create_computation_fn.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._confusion_matrix_plot),\n        num_thresholds=num_thresholds,\n        name=name,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ConfusionMatrixPlot.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DerivedMetricComputation","title":"DerivedMetricComputation","text":"<p>               Bases: <code>NamedTuple('DerivedMetricComputation', [('keys', List[MetricKey]), ('result', Callable)])</code></p> <p>DerivedMetricComputation derives its result from other computations.</p> <p>When creating derived metric computations it is recommended (but not required) that the underlying MetricComputations that they depend on are defined at the same time. This is to avoid having to pre-construct and pass around all the required dependencies in order to construct a derived metric. The evaluation pipeline is responsible for de-duplicating overlapping MetricComputations so that only one computation is actually run.</p> <p>A DerivedMetricComputation is uniquely identified by the combination of the result function's name and the keys. Duplicate computations will be removed automatically.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DerivedMetricComputation--attributes","title":"Attributes","text":"<p>keys: List of metric keys associated with derived computation. If the keys     are defined as part of the computation then this may be empty in which     case only the result function name will be used for identifying     computation uniqueness.   result: Function (called per slice) to compute the result using the results     of other metric computations.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio","title":"DiagnosticOddsRatio","text":"<pre><code>DiagnosticOddsRatio(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Diagnostic odds ratio (DOR).</p> <p>Initializes diagnostic odds ratio.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes diagnostic odds ratio.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.DiagnosticOddsRatio.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    if fn &gt; 0.0 and fp &gt; 0.0 and tn &gt; 0.0:\n        return (tp / fn) / (fp / tn)\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch","title":"ExactMatch","text":"<pre><code>ExactMatch(\n    name: str = EXACT_MATCH_NAME,\n    convert_to: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Exact Match Metric.</p> <p>Initializes exact match metric.</p> <p>name: The name of the metric to use.   convert_to: The conversion to perform before checking equality.</p> Source code in <code>tensorflow_model_analysis/metrics/exact_match.py</code> <pre><code>def __init__(self, name: str = EXACT_MATCH_NAME, convert_to: Optional[str] = None):\n    \"\"\"Initializes exact match metric.\n\n    Args:\n    ----\n      name: The name of the metric to use.\n      convert_to: The conversion to perform before checking equality.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_exact_match),\n        name=name,\n        convert_to=convert_to,\n    )\n    if convert_to and convert_to not in _CONVERT_TO_VALUES:\n        raise ValueError(\n            \"convert_to can only be one of the following: %s\" % str(convert_to)\n        )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExactMatch.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount","title":"ExampleCount","text":"<pre><code>ExampleCount(name: str = EXAMPLE_COUNT_NAME)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Example count.</p> <p>Note that although the example_count is independent of the model, this metric will be associated with a model for consistency with other metrics.</p> <p>Initializes example count.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/example_count.py</code> <pre><code>def __init__(self, name: str = EXAMPLE_COUNT_NAME):\n    \"\"\"Initializes example count.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(example_count, name=name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Always disable confidence intervals for ExampleCount.</p> <p>Confidence intervals capture uncertainty in a metric if it were computed on more examples. For ExampleCount, this sort of uncertainty is not meaningful, so confidence intervals are disabled.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ExampleCount.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score","title":"F1Score","text":"<pre><code>F1Score(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>F1 score.</p> <p>Initializes F1 score.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes F1 score.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.F1Score.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn\n    # This is the harmonic mean of precision and recall or the same as\n    # 2 * (precision * recall) / (precision + recall).\n    # See https://en.wikipedia.org/wiki/Confusion_matrix for more information.\n    return _divide_only_positive_denominator(\n        numerator=2 * tp, denominator=2 * tp + fp + fn\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN","title":"FN","text":"<pre><code>FN(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>FalseNegatives</code></p> <p>Alias for FalseNegatives.</p> <p>Initializes FN metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes FN metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FN.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR","title":"FNR","text":"<pre><code>FNR(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>MissRate</code></p> <p>Alias for MissRate.</p> <p>Initializes FNR metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes FNR metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FNR.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(fn, fn + tp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP","title":"FP","text":"<pre><code>FP(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>FalsePositives</code></p> <p>Alias for FalsePositives.</p> <p>Initializes FP metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes FP metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FP.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return fp\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR","title":"FPR","text":"<pre><code>FPR(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>FallOut</code></p> <p>Alias for FallOut.</p> <p>Initializes FPR metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes FPR metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FPR.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tp, fn\n    return _divide_only_positive_denominator(fp, fp + tn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut","title":"FallOut","text":"<pre><code>FallOut(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Fall-out (FPR).</p> <p>Initializes fall-out metric.</p> <p>thresholds: (Optional) Thresholds to use for fall-out. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes fall-out metric.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use for fall-out. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FallOut.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tp, fn\n    return _divide_only_positive_denominator(fp, fp + tn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate","title":"FalseDiscoveryRate","text":"<pre><code>FalseDiscoveryRate(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>False discovery rate (FDR).</p> <p>Initializes false discovery rate.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes false discovery rate.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseDiscoveryRate.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fn\n    return _divide_only_positive_denominator(fp, fp + tp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives","title":"FalseNegatives","text":"<pre><code>FalseNegatives(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Calculates the number of false negatives.</p> <p>If <code>sample_weight</code> is given, calculates the sum of the weights of false negatives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes FalseNegatives metric.</p> <p>thresholds: (Optional) Defaults to [0.5]. A float value or a python     list/tuple of float threshold values in [0, 1]. A threshold is compared     with prediction values to determine the truth value of predictions     (i.e., above the threshold is <code>true</code>, below is <code>false</code>). One metric     value is generated for each threshold value.   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes FalseNegatives metric.\n\n    Args:\n    ----\n      thresholds: (Optional) Defaults to [0.5]. A float value or a python\n        list/tuple of float threshold values in [0, 1]. A threshold is compared\n        with prediction values to determine the truth value of predictions\n        (i.e., above the threshold is `true`, below is `false`). One metric\n        value is generated for each threshold value.\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseNegatives.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate","title":"FalseOmissionRate","text":"<pre><code>FalseOmissionRate(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>False omission rate (FOR).</p> <p>Initializes false omission rate.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes false omission rate.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalseOmissionRate.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tp, fp\n    return _divide_only_positive_denominator(fn, fn + tn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives","title":"FalsePositives","text":"<pre><code>FalsePositives(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Calculates the number of false positives.</p> <p>If <code>sample_weight</code> is given, calculates the sum of the weights of false positives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes FalsePositives metric.</p> <p>thresholds: (Optional) Defaults to [0.5]. A float value or a python     list/tuple of float threshold values in [0, 1]. A threshold is compared     with prediction values to determine the truth value of predictions     (i.e., above the threshold is <code>true</code>, below is <code>false</code>). One metric     value is generated for each threshold value.   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes FalsePositives metric.\n\n    Args:\n    ----\n      thresholds: (Optional) Defaults to [0.5]. A float value or a python\n        list/tuple of float threshold values in [0, 1]. A threshold is compared\n        with prediction values to determine the truth value of predictions\n        (i.e., above the threshold is `true`, below is `false`). One metric\n        value is generated for each threshold value.\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FalsePositives.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return fp\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex","title":"FowlkesMallowsIndex","text":"<pre><code>FowlkesMallowsIndex(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Fowlkes-Mallows index (FM).</p> <p>Initializes fowlkes-mallows index.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes fowlkes-mallows index.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FowlkesMallowsIndex.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn\n    ppv_denominator = tp + fp\n    tpr_denominator = tp + fn\n    if ppv_denominator &gt; 0.0 and tpr_denominator &gt; 0.0:\n        ppv = tp / ppv_denominator\n        tnr = tp / tpr_denominator\n        return _pos_sqrt(ppv * tnr)\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness","title":"Informedness","text":"<pre><code>Informedness(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Informedness or bookmaker informedness (BM).</p> <p>Initializes informedness.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes informedness.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Informedness.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    positives = tp + fn\n    negatives = tn + fp\n    if positives &gt; 0.0 and negatives &gt; 0.0:\n        tpr = tp / positives\n        tnr = tn / negatives\n        return tpr + tnr - 1\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness","title":"Markedness","text":"<pre><code>Markedness(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Markedness (MK) or deltaP.</p> <p>Initializes markedness.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes markedness.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Markedness.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    ppv_denominator = tp + fp\n    npv_denominator = tn + fn\n    if ppv_denominator &gt; 0.0 and npv_denominator &gt; 0.0:\n        ppv = tp / ppv_denominator\n        npv = tn / npv_denominator\n        return ppv + npv - 1\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient","title":"MatthewsCorrelationCoefficient","text":"<pre><code>MatthewsCorrelationCoefficient(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Matthews corrrelation coefficient (MCC).</p> <p>Initializes matthews corrrelation coefficient.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes matthews corrrelation coefficient.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MatthewsCorrelationCoefficient.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return _divide_only_positive_denominator(\n        numerator=tp * tn - fp * fn,\n        denominator=_pos_sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall","title":"MaxRecall","text":"<pre><code>MaxRecall(\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Recall</code></p> <p>Computes the max recall of the predictions with respect to the labels.</p> <p>The metric uses true positives and false negatives to compute recall by dividing the true positives by the sum of true positives and false negatives.</p> <p>Effectively the recall at threshold = epsilon(1.0e-12). It is equilvalent to the recall defined in COCO metrics.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes MaxRecall metrics, it calculates the maximum recall.</p> <p>top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.   **kwargs: (Optional) Additional args to pass along to init (and eventually     on to _metric_computation and _metric_value)</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Initializes MaxRecall metrics, it calculates the maximum recall.\n\n    Args:\n    ----\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n      **kwargs: (Optional) Additional args to pass along to init (and eventually\n        on to _metric_computation and _metric_value)\n    \"\"\"\n    super().__init__(\n        thresholds=_DEFAULT_THRESHOLD_FOR_MAX_RECALL,\n        top_k=top_k,\n        class_id=class_id,\n        name=name,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MaxRecall.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(tp, tp + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean","title":"Mean","text":"<pre><code>Mean(\n    feature_key_path: _KeyPath,\n    example_weights_key_path: Optional[_KeyPath] = None,\n    name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Mean metric.</p> <p>Initializes mean metric.</p> <p>feature_key_path: key path to feature to calculate the mean of.   example_weights_key_path: key path to example weights.   name: Metric base name.</p> Source code in <code>tensorflow_model_analysis/metrics/stats.py</code> <pre><code>def __init__(\n    self,\n    feature_key_path: _KeyPath,\n    example_weights_key_path: Optional[_KeyPath] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initializes mean metric.\n\n    Args:\n    ----\n      feature_key_path: key path to feature to calculate the mean of.\n      example_weights_key_path: key path to example weights.\n      name: Metric base name.\n    \"\"\"\n    super().__init__(\n        _mean_metric,\n        feature_key_path=feature_key_path,\n        example_weights_key_path=example_weights_key_path,\n        name=name or f\"{_MEAN_METRIC_BASE_NAME}_{'.'.join(feature_key_path)}\",\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Mean.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions","title":"MeanAbsoluteAttributions","text":"<pre><code>MeanAbsoluteAttributions(\n    name: str = MEAN_ABSOLUTE_ATTRIBUTIONS_NAME,\n)\n</code></pre> <p>               Bases: <code>AttributionsMetric</code></p> <p>Mean aboslute attributions metric.</p> <p>Initializes mean absolute attributions metric.</p> <p>name: Attribution metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/attributions.py</code> <pre><code>def __init__(self, name: str = MEAN_ABSOLUTE_ATTRIBUTIONS_NAME):\n    \"\"\"Initializes mean absolute attributions metric.\n\n    Args:\n    ----\n      name: Attribution metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            functools.partial(_mean_attributions, True)\n        ),\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteAttributions.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError","title":"MeanAbsoluteError","text":"<pre><code>MeanAbsoluteError(\n    name: str = MEAN_ABSOLUTE_ERROR_NAME, **kwargs\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calculates the mean of absolute error between labels and predictions.</p> <p>Formula: error = abs(label - prediction)</p> <p>The metric computes the mean of absolute error between labels and predictions. The labels and predictions should be floats.</p> <p>Initializes mean regression error metric.</p> <p>name: The name of the metric.   **kwargs: Additional named keyword arguments.</p> Source code in <code>tensorflow_model_analysis/metrics/mean_regression_error.py</code> <pre><code>def __init__(self, name: str = MEAN_ABSOLUTE_ERROR_NAME, **kwargs):\n    \"\"\"Initializes mean regression error metric.\n\n    Args:\n    ----\n      name: The name of the metric.\n      **kwargs: Additional named keyword arguments.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_mean_absolute_error_computations),\n        name=name,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsoluteError.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError","title":"MeanAbsolutePercentageError","text":"<pre><code>MeanAbsolutePercentageError(\n    name: str = MEAN_ABSOLUTE_PERCENTAGE_ERROR_NAME,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calculates the mean of absolute percentage error.</p> <p>Formula: error = 100 * abs( (label - prediction) / label )</p> <p>The metric computes the mean of absolute percentage error between labels and predictions. The labels and predictions should be floats.</p> <p>Initializes mean regression error metric.</p> <p>name: The name of the metric.   **kwargs: Additional named keyword arguments.</p> Source code in <code>tensorflow_model_analysis/metrics/mean_regression_error.py</code> <pre><code>def __init__(self, name: str = MEAN_ABSOLUTE_PERCENTAGE_ERROR_NAME, **kwargs):\n    \"\"\"Initializes mean regression error metric.\n\n    Args:\n    ----\n      name: The name of the metric.\n      **kwargs: Additional named keyword arguments.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            _mean_absolute_percentage_error_computations\n        ),\n        name=name,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAbsolutePercentageError.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions","title":"MeanAttributions","text":"<pre><code>MeanAttributions(name: str = MEAN_ATTRIBUTIONS_NAME)\n</code></pre> <p>               Bases: <code>AttributionsMetric</code></p> <p>Mean attributions metric.</p> <p>Initializes mean attributions metric.</p> <p>name: Attribution metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/attributions.py</code> <pre><code>def __init__(self, name: str = MEAN_ATTRIBUTIONS_NAME):\n    \"\"\"Initializes mean attributions metric.\n\n    Args:\n    ----\n      name: Attribution metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            functools.partial(_mean_attributions, False)\n        ),\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanAttributions.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel","title":"MeanLabel","text":"<pre><code>MeanLabel(name: str = MEAN_LABEL_NAME)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Mean label.</p> <p>Initializes mean label.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/calibration.py</code> <pre><code>def __init__(self, name: str = MEAN_LABEL_NAME):\n    \"\"\"Initializes mean label.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(metric_util.merge_per_key_computations(_mean_label), name=name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanLabel.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction","title":"MeanPrediction","text":"<pre><code>MeanPrediction(name: str = MEAN_PREDICTION_NAME)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Mean prediction.</p> <p>Initializes mean prediction.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/calibration.py</code> <pre><code>def __init__(self, name: str = MEAN_PREDICTION_NAME):\n    \"\"\"Initializes mean prediction.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_mean_prediction), name=name\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanPrediction.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError","title":"MeanSquaredError","text":"<pre><code>MeanSquaredError(\n    name: str = MEAN_SQUARED_ERROR_NAME, **kwargs\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calculates the mean of squared error between labels and predictions.</p> <p>Formula: error = L2_norm(label - prediction)**2</p> <p>The metric computes the mean of squared error (square of L2 norm) between labels and predictions. The labels and predictions could be arrays of arbitrary dimensions. Their dimension should match.</p> <p>Initializes mean regression error metric.</p> <p>name: The name of the metric.   **kwargs: Additional named keyword arguments.</p> Source code in <code>tensorflow_model_analysis/metrics/mean_regression_error.py</code> <pre><code>def __init__(self, name: str = MEAN_SQUARED_ERROR_NAME, **kwargs):\n    \"\"\"Initializes mean regression error metric.\n\n    Args:\n    ----\n      name: The name of the metric.\n      **kwargs: Additional named keyword arguments.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_mean_squared_error_computations),\n        name=name,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredError.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError","title":"MeanSquaredLogarithmicError","text":"<pre><code>MeanSquaredLogarithmicError(\n    name: str = MEAN_SQUARED_LOGARITHMIC_ERROR_NAME,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Calculates the mean of squared logarithmic error.</p> <p>Formula: error = L2_norm(log(label + 1) - log(prediction + 1))**2 Note: log of an array will be elementwise,   i.e. log([x1, x2]) = [log(x1), log(x2)]</p> <p>The metric computes the mean of squared logarithmic error (square of L2 norm) between labels and predictions. The labels and predictions could be arrays of arbitrary dimensions. Their dimension should match.</p> <p>Initializes mean regression error metric.</p> <p>name: The name of the metric.   **kwargs: Additional named keyword arguments.</p> Source code in <code>tensorflow_model_analysis/metrics/mean_regression_error.py</code> <pre><code>def __init__(self, name: str = MEAN_SQUARED_LOGARITHMIC_ERROR_NAME, **kwargs):\n    \"\"\"Initializes mean regression error metric.\n\n    Args:\n    ----\n      name: The name of the metric.\n      **kwargs: Additional named keyword arguments.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            _mean_squared_logarithmic_error_computations\n        ),\n        name=name,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MeanSquaredLogarithmicError.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric","title":"Metric","text":"<pre><code>Metric(\n    create_computations_fn: Callable[\n        ..., MetricComputations\n    ],\n    **kwargs,\n)\n</code></pre> <p>Metric wraps a set of metric computations.</p> <p>This class exists to provide similarity between tfma.metrics.Metric and tf.keras.metics.Metric.</p> <p>Calling computations creates the metric computations. The parameters passed to init will be combined with the parameters passed to the computations method. This allows some of the parameters (e.g. model_names, output_names, sub_keys) to be set at the time the computations are created instead of when the metric is defined.</p> <p>Initializes metric.</p> <p>create_computations_fn: Function to create the metrics computations (e.g.     mean_label, etc). This function should take the args passed to init     as as input along with any of eval_config, schema, model_names,     output_names, sub_keys, aggregation_type, or query_key (where needed).   **kwargs: Any additional kwargs to pass to create_computations_fn. These     should only contain primitive types or lists/dicts of primitive types.     The kwargs passed to computations have precendence over these kwargs.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def __init__(\n    self, create_computations_fn: Callable[..., MetricComputations], **kwargs\n):\n    \"\"\"Initializes metric.\n\n    Args:\n    ----\n      create_computations_fn: Function to create the metrics computations (e.g.\n        mean_label, etc). This function should take the args passed to __init__\n        as as input along with any of eval_config, schema, model_names,\n        output_names, sub_keys, aggregation_type, or query_key (where needed).\n      **kwargs: Any additional kwargs to pass to create_computations_fn. These\n        should only contain primitive types or lists/dicts of primitive types.\n        The kwargs passed to computations have precendence over these kwargs.\n    \"\"\"\n    self.create_computations_fn = create_computations_fn\n    if \"name\" in kwargs:\n        if not kwargs[\"name\"] and self._default_name():\n            kwargs[\"name\"] = self._default_name()  # pylint: disable=assignment-from-none\n        name = kwargs[\"name\"]\n    else:\n        name = None\n    self.name = name\n    self.kwargs = kwargs\n    if hasattr(inspect, \"getfullargspec\"):\n        self._args = inspect.getfullargspec(self.create_computations_fn).args\n    else:\n        self._args = inspect.getargspec(self.create_computations_fn).args  # pylint: disable=deprecated-method\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Metric.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricComputation","title":"MetricComputation","text":"<p>               Bases: <code>NamedTuple('MetricComputation', [('keys', List[MetricKey]), ('preprocessors', List[Preprocessor]), ('combiner', CombineFn)])</code></p> <p>MetricComputation represents one or more metric computations.</p> <p>The preprocessors are called with a PCollection of extracts (or list of extracts if query_key is used) to compute the initial combiner input state which is then passed to the combiner. This needs to be done in two steps because slicing happens between the call to the preprocessors and the combiner and this state may end up in multiple slices so we want the representation to be as efficient as possible. If the preprocessors are None, then StandardMetricInputs will be passed.</p> <p>A MetricComputation is uniquely identified by the combination of the combiner's name and the keys. Duplicate computations will be removed automatically.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricComputation--attributes","title":"Attributes","text":"<p>keys: List of metric keys associated with computation. If the keys are     defined as part of the computation then this may be empty in which case     only the combiner name will be used for identifying computation     uniqueness.   preprocessors: Takes a extracts (or a list of extracts) as input (which     typically will contain labels, predictions, example weights, and     optionally features) and should return the initial state that the combiner     will use as input. The output of a processor should only contain     information needed by the combiner.   combiner: Takes preprocessor output as input and outputs a tuple: (slice,     metric results). The metric results should be a dict from MetricKey to     value (float, int, distribution, ...).</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricKey","title":"MetricKey","text":"<p>               Bases: <code>NamedTuple('MetricKey', [('name', str), ('model_name', str), ('output_name', str), ('sub_key', Optional[SubKey]), ('aggregation_type', Optional[AggregationType]), ('example_weighted', Optional[bool]), ('is_diff', bool)])</code></p> <p>A MetricKey uniquely identifies a metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricKey--attributes","title":"Attributes","text":"<p>name: Metric name. Names starting with '_' are private and will be filtered     from the final results. Names starting with two underscores, '__' are     reserved for internal use.   model_name: Optional model name (if multi-model evaluation).   output_name: Optional output name (if multi-output model type).   sub_key: Optional sub key.   aggregation_type: Optional Aggregation type.   example_weighted: Indicates whether this metric was weighted by examples.   is_diff: Optional flag to indicate whether this metrics is a diff metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricKey-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricKey.from_proto","title":"from_proto  <code>staticmethod</code>","text":"<pre><code>from_proto(pb: MetricKey) -&gt; MetricKey\n</code></pre> <p>Configures class from proto.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@staticmethod\ndef from_proto(pb: metrics_for_slice_pb2.MetricKey) -&gt; \"MetricKey\":\n    \"\"\"Configures class from proto.\"\"\"\n    example_weighted = None\n    if pb.HasField(\"example_weighted\"):\n        example_weighted = pb.example_weighted.value\n    return MetricKey(\n        name=pb.name,\n        model_name=pb.model_name,\n        output_name=pb.output_name,\n        sub_key=SubKey.from_proto(pb.sub_key),\n        aggregation_type=AggregationType.from_proto(pb.aggregation_type),\n        example_weighted=example_weighted,\n        is_diff=pb.is_diff,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricKey.make_baseline_key","title":"make_baseline_key","text":"<pre><code>make_baseline_key(model_name: str) -&gt; MetricKey\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def make_baseline_key(self, model_name: str) -&gt; \"MetricKey\":\n    return self._replace(model_name=model_name, is_diff=False)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricKey.make_diff_key","title":"make_diff_key","text":"<pre><code>make_diff_key() -&gt; MetricKey\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def make_diff_key(self) -&gt; \"MetricKey\":\n    return self._replace(is_diff=True)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MetricKey.to_proto","title":"to_proto","text":"<pre><code>to_proto() -&gt; MetricKey\n</code></pre> <p>Converts key to proto.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def to_proto(self) -&gt; metrics_for_slice_pb2.MetricKey:\n    \"\"\"Converts key to proto.\"\"\"\n    metric_key = metrics_for_slice_pb2.MetricKey()\n    if self.name:\n        metric_key.name = self.name\n    if self.model_name:\n        metric_key.model_name = self.model_name\n    if self.output_name:\n        metric_key.output_name = self.output_name\n    if self.sub_key:\n        metric_key.sub_key.CopyFrom(self.sub_key.to_proto())\n    if self.aggregation_type:\n        metric_key.aggregation_type.CopyFrom(self.aggregation_type.to_proto())\n    if self.example_weighted is not None:\n        metric_key.example_weighted.value = self.example_weighted\n    if self.is_diff:\n        metric_key.is_diff = self.is_diff\n    return metric_key\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition","title":"MinLabelPosition","text":"<pre><code>MinLabelPosition(\n    name=MIN_LABEL_POSITION_NAME,\n    label_key: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Min label position metric.</p> <p>Calculates the least index in a query which has a positive label. The final returned value is the weighted average over all queries in the evaluation set which have at least one labeled entry. Note, ranking is indexed from one, so the optimal value for this metric is one. If there are no labeled rows in the evaluation set, the final output will be zero.</p> <p>This is a query/ranking based metric so a query_key must also be provided in the associated metrics spec.</p> <p>Initializes min label position metric.</p> <p>name: Metric name.   label_key: Optional label key to override default label.</p> Source code in <code>tensorflow_model_analysis/metrics/min_label_position.py</code> <pre><code>def __init__(self, name=MIN_LABEL_POSITION_NAME, label_key: Optional[str] = None):\n    \"\"\"Initializes min label position metric.\n\n    Args:\n    ----\n      name: Metric name.\n      label_key: Optional label key to override default label.\n    \"\"\"\n    super().__init__(_min_label_position, name=name, label_key=label_key)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MinLabelPosition.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate","title":"MissRate","text":"<pre><code>MissRate(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Miss rate (FNR).</p> <p>Initializes miss rate metric.</p> <p>thresholds: (Optional) Thresholds to use for miss rate. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes miss rate metric.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use for miss rate. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MissRate.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(fn, fn + tp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds","title":"MultiClassConfusionMatrixAtThresholds","text":"<pre><code>MultiClassConfusionMatrixAtThresholds(\n    thresholds: Optional[List[float]] = None,\n    name: str = MULTI_CLASS_CONFUSION_MATRIX_AT_THRESHOLDS_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Multi-class confusion matrix metrics at thresholds.</p> <p>Computes weighted example counts for all combinations of actual / (top) predicted classes.</p> <p>The inputs are assumed to contain a single positive label per example (i.e. only one class can be true at a time) while the predictions are assumed to sum to 1.0.</p> <p>Initializes multi-class confusion matrix.</p> <p>thresholds: Optional thresholds, defaults to 0.5 if not specified. If the     top prediction is less than a threshold then the associated example will     be assumed to have no prediction associated with it (the     predicted_class_id will be set to NO_PREDICTED_CLASS_ID).   name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/multi_class_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[List[float]] = None,\n    name: str = MULTI_CLASS_CONFUSION_MATRIX_AT_THRESHOLDS_NAME,\n):\n    \"\"\"Initializes multi-class confusion matrix.\n\n    Args:\n    ----\n      thresholds: Optional thresholds, defaults to 0.5 if not specified. If the\n        top prediction is less than a threshold then the associated example will\n        be assumed to have no prediction associated with it (the\n        predicted_class_id will be set to NO_PREDICTED_CLASS_ID).\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            _multi_class_confusion_matrix_at_thresholds\n        ),\n        thresholds=thresholds,\n        name=name,\n    )  # pytype: disable=wrong-arg-types\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixAtThresholds.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot","title":"MultiClassConfusionMatrixPlot","text":"<pre><code>MultiClassConfusionMatrixPlot(\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: str = MULTI_CLASS_CONFUSION_MATRIX_PLOT_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Multi-class confusion matrix plot.</p> <p>Computes weighted example counts for all combinations of actual / (top) predicted classes.</p> <p>The inputs are assumed to contain a single positive label per example (i.e. only one class can be true at a time) while the predictions are assumed to sum to 1.0.</p> <p>Initializes multi-class confusion matrix.</p> <p>thresholds: Optional thresholds. If the top prediction is less than a     threshold then the associated example will be assumed to have no     prediction associated with it (the predicted_class_id will be set to     tfma.metrics.NO_PREDICTED_CLASS_ID). Only one of     either thresholds or num_thresholds should be used. If both are unset,     then [0.0] will be assumed.   num_thresholds: Number of thresholds to use. The thresholds will be evenly     spaced between 0.0 and 1.0 and inclusive of the boundaries (i.e. to     configure the thresholds to [0.0, 0.25, 0.5, 0.75, 1.0], the parameter     should be set to 5). Only one of either thresholds or num_thresholds     should be used.   name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/multi_class_confusion_matrix_plot.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: str = MULTI_CLASS_CONFUSION_MATRIX_PLOT_NAME,\n):\n    \"\"\"Initializes multi-class confusion matrix.\n\n    Args:\n    ----\n      thresholds: Optional thresholds. If the top prediction is less than a\n        threshold then the associated example will be assumed to have no\n        prediction associated with it (the predicted_class_id will be set to\n        tfma.metrics.NO_PREDICTED_CLASS_ID). Only one of\n        either thresholds or num_thresholds should be used. If both are unset,\n        then [0.0] will be assumed.\n      num_thresholds: Number of thresholds to use. The thresholds will be evenly\n        spaced between 0.0 and 1.0 and inclusive of the boundaries (i.e. to\n        configure the thresholds to [0.0, 0.25, 0.5, 0.75, 1.0], the parameter\n        should be set to 5). Only one of either thresholds or num_thresholds\n        should be used.\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_multi_class_confusion_matrix_plot),\n        thresholds=thresholds,\n        num_thresholds=num_thresholds,\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiClassConfusionMatrixPlot.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot","title":"MultiLabelConfusionMatrixPlot","text":"<pre><code>MultiLabelConfusionMatrixPlot(\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: str = MULTI_LABEL_CONFUSION_MATRIX_PLOT_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Multi-label confusion matrix.</p> <p>For each actual class (positive label) a confusion matrix is computed for each class based on the associated predicted values such that:</p> <p>TP = positive_prediction_class_label &amp; positive_prediction   TN = negative_prediction_class_label &amp; negative_prediction   FP = negative_prediction_class_label &amp; positive_prediction   FN = positive_prediction_class_label &amp; negative_prediction</p> <p>For example, given classes 0, 1 and a given threshold, the following matrices will be computed:</p> <p>Actual: class_0   Predicted: class_0       TP = is_class_0 &amp; is_class_0 &amp; predict_class_0       TN = is_class_0 &amp; not_class_0 &amp; predict_not_class_0       FN = is_class_0 &amp; is_class_0 &amp; predict_not_class_0       FP = is_class_0 &amp; not_class_0 &amp; predict_class_0   Actual: class_0   Predicted: class_1       TP = is_class_0 &amp; is_class_1 &amp; predict_class_1       TN = is_class_0 &amp; not_class_1 &amp; predict_not_class_1       FN = is_class_0 &amp; is_class_1 &amp; predict_not_class_1       FP = is_class_0 &amp; not_class_1 &amp; predict_class_1   Actual: class_1   Predicted: class_0       TP = is_class_1 &amp; is_class_0 &amp; predict_class_0       TN = is_class_1 &amp; not_class_0 &amp; predict_not_class_0       FN = is_class_1 &amp; is_class_0 &amp; predict_not_class_0       FP = is_class_1 &amp; not_class_0 &amp; predict_class_0   Actual: class_1   Predicted: class_1       TP = is_class_1 &amp; is_class_1 &amp; predict_class_1       TN = is_class_1 &amp; not_class_1 &amp; predict_not_class_1       FN = is_class_1 &amp; is_class_1 &amp; predict_not_class_1       FP = is_class_1 &amp; not_class_1 &amp; predict_class_1</p> <p>Note that unlike the multi-class confusion matrix, the inputs are assumed to be multi-label whereby the predictions may not necessarily sum to 1.0 and multiple classes can be true as the same time.</p> <p>Initializes multi-label confusion matrix.</p> <p>thresholds: Optional thresholds. Only one of either thresholds or     num_thresholds should be used. If both are unset, then [0.5] will be     assumed.   num_thresholds: Number of thresholds to use. The thresholds will be evenly     spaced between 0.0 and 1.0 and inclusive of the boundaries (i.e. to     configure the thresholds to [0.0, 0.25, 0.5, 0.75, 1.0], the parameter     should be set to 5). Only one of either thresholds or num_thresholds     should be used.   name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/multi_label_confusion_matrix_plot.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: str = MULTI_LABEL_CONFUSION_MATRIX_PLOT_NAME,\n):\n    \"\"\"Initializes multi-label confusion matrix.\n\n    Args:\n    ----\n      thresholds: Optional thresholds. Only one of either thresholds or\n        num_thresholds should be used. If both are unset, then [0.5] will be\n        assumed.\n      num_thresholds: Number of thresholds to use. The thresholds will be evenly\n        spaced between 0.0 and 1.0 and inclusive of the boundaries (i.e. to\n        configure the thresholds to [0.0, 0.25, 0.5, 0.75, 1.0], the parameter\n        should be set to 5). Only one of either thresholds or num_thresholds\n        should be used.\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_multi_label_confusion_matrix_plot),\n        thresholds=thresholds,\n        num_thresholds=num_thresholds,\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.MultiLabelConfusionMatrixPlot.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG","title":"NDCG","text":"<pre><code>NDCG(\n    gain_key: str,\n    top_k_list: Optional[List[int]] = None,\n    name: str = NDCG_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>NDCG (normalized discounted cumulative gain) metric.</p> <p>Calculates NDCG@k for a given set of top_k values calculated from a list of gains (relevance scores) that are sorted based on the associated predictions. The top_k_list can be passed as part of the NDCG metric config or using tfma.MetricsSpec.binarize.top_k_list if configuring multiple top_k metrics. The gain (relevance score) is determined from the value stored in the 'gain_key' feature. The value of NDCG@k returned is a weighted average of NDCG@k over the set of queries using the example weights.</p> <p>NDCG@k = (DCG@k for the given rank)/(DCG@k DCG@k = sum_{i=1}^k gain_i/log_2(i+1), where gain_i is the gain (relevance         score) of the i^th ranked response, indexed from 1.</p> <p>This is a query/ranking based metric so a query_key must also be provided in the associated tfma.MetricsSpec.</p> <p>Initializes NDCG.</p> <p>gain_key: Key of feature in features dictionary that holds gain values.   top_k_list: Values for top k. This can also be set using the     tfma.MetricsSpec.binarize.top_k_list associated with the metric.   name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/ndcg.py</code> <pre><code>def __init__(\n    self,\n    gain_key: str,\n    top_k_list: Optional[List[int]] = None,\n    name: str = NDCG_NAME,\n):\n    \"\"\"Initializes NDCG.\n\n    Args:\n    ----\n      gain_key: Key of feature in features dictionary that holds gain values.\n      top_k_list: Values for top k. This can also be set using the\n        tfma.MetricsSpec.binarize.top_k_list associated with the metric.\n      name: Metric name.\n    \"\"\"\n    super().__init__(_ndcg, gain_key=gain_key, top_k_list=top_k_list, name=name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NDCG.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV","title":"NPV","text":"<pre><code>NPV(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>NegativePredictiveValue</code></p> <p>Alias for NegativePredictiveValue.</p> <p>Initializes PPV metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes PPV metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NPV.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tp, fp\n    return _divide_only_positive_denominator(tn, tn + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio","title":"NegativeLikelihoodRatio","text":"<pre><code>NegativeLikelihoodRatio(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Negative likelihood ratio (LR-).</p> <p>Initializes negative likelihood ratio.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes negative likelihood ratio.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativeLikelihoodRatio.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    fnr_denominator = fn + tp\n    tnr_denominator = tn + fp\n    if fnr_denominator &gt; 0.0 and tnr_denominator &gt; 0.0 and tn &gt; 0.0:\n        fnr = fn / fnr_denominator\n        tnr = tn / tnr_denominator\n        return fnr / tnr\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue","title":"NegativePredictiveValue","text":"<pre><code>NegativePredictiveValue(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Negative predictive value (NPV).</p> <p>Initializes negative predictive value.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes negative predictive value.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.NegativePredictiveValue.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tp, fp\n    return _divide_only_positive_denominator(tn, tn + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot","title":"ObjectDetectionConfusionMatrixPlot","text":"<pre><code>ObjectDetectionConfusionMatrixPlot(\n    num_thresholds: int = DEFAULT_NUM_THRESHOLDS,\n    name: str = CONFUSION_MATRIX_PLOT_NAME,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixPlot</code></p> <p>Object Detection Confusion matrix plot.</p> <p>Initializes confusion matrix plot for object detection.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>num_thresholds: Number of thresholds to use when discretizing the curve.     Values must be &gt; 1. Defaults to 1000.   name: Metric name.   iou_threshold: (Optional) Thresholds for a detection and ground truth pair     with specific iou to be considered as a match. Default to 0.5   class_id: (Optional) The class id for calculating metrics.   class_weight: (Optional) The weight associated with the object class id.   area_range: (Optional) A tuple (inclusive) representing the area-range for     objects to be considered for metrics. Default to (0, inf).   max_num_detections: (Optional) The maximum number of detections for a     single image. Default to None.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_confusion_matrix_plot.py</code> <pre><code>def __init__(\n    self,\n    num_thresholds: int = DEFAULT_NUM_THRESHOLDS,\n    name: str = CONFUSION_MATRIX_PLOT_NAME,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes confusion matrix plot for object detection.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      num_thresholds: Number of thresholds to use when discretizing the curve.\n        Values must be &gt; 1. Defaults to 1000.\n      name: Metric name.\n      iou_threshold: (Optional) Thresholds for a detection and ground truth pair\n        with specific iou to be considered as a match. Default to 0.5\n      class_id: (Optional) The class id for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class id.\n      area_range: (Optional) A tuple (inclusive) representing the area-range for\n        objects to be considered for metrics. Default to (0, inf).\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image. Default to None.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    super().__init__(\n        num_thresholds=num_thresholds,\n        name=name,\n        class_id=class_id,\n        iou_threshold=iou_threshold,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        class_weight=class_weight,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionConfusionMatrixPlot.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall","title":"ObjectDetectionMaxRecall","text":"<pre><code>ObjectDetectionMaxRecall(\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>MaxRecall</code></p> <p>Computes the max recall of the predictions with respect to the labels.</p> <p>The metric uses true positives and false negatives to compute recall by dividing the true positives by the sum of true positives and false negatives.</p> <p>Effectively the recall at threshold = epsilon(1.0e-12). It is equilvalent to the recall defined in COCO metrics.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes MaxRecall metrics, it calculates the maximum recall.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>name: (Optional) string name of the metric instance.   iou_threshold: (Optional) Thresholds for a detection and ground truth pair     with specific iou to be considered as a match. Default to 0.5   class_id: (Optional) The class id for calculating metrics.   class_weight: (Optional) The weight associated with the object class id.   area_range: (Optional) A tuple (inclusive) representing the area-range for     objects to be considered for metrics. Default to (0, inf).   max_num_detections: (Optional) The maximum number of detections for a     single image. Default to None.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes MaxRecall metrics, it calculates the maximum recall.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      name: (Optional) string name of the metric instance.\n      iou_threshold: (Optional) Thresholds for a detection and ground truth pair\n        with specific iou to be considered as a match. Default to 0.5\n      class_id: (Optional) The class id for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class id.\n      area_range: (Optional) A tuple (inclusive) representing the area-range for\n        objects to be considered for metrics. Default to (0, inf).\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image. Default to None.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    super().__init__(\n        name=name,\n        class_id=class_id,\n        iou_threshold=iou_threshold,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        class_weight=class_weight,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionMaxRecall.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(tp, tp + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision","title":"ObjectDetectionPrecision","text":"<pre><code>ObjectDetectionPrecision(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>Precision</code></p> <p>Computes the precision of the predictions with respect to the labels.</p> <p>The metric uses true positives and false positives to compute precision by dividing the true positives by the sum of true positives and false positives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes Recall metric.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>thresholds: (Optional) A float value or a python list/tuple of float     threshold values in [0, 1]. A threshold is compared with prediction     values to determine the truth value of predictions (i.e., above the     threshold is <code>true</code>, below is <code>false</code>). One metric value is generated     for each threshold value. The default is to calculate precision with     <code>thresholds=0.5</code>.   name: (Optional) string name of the metric instance.   iou_threshold: (Optional) Thresholds for a detection and ground truth pair     with specific iou to be considered as a match. Default to 0.5   class_id: (Optional) The class id for calculating metrics.   class_weight: (Optional) The weight associated with the object class id.   area_range: (Optional) A tuple (inclusive) representing the area-range for     objects to be considered for metrics. Default to (0, inf).   max_num_detections: (Optional) The maximum number of detections for a     single image. Default to None.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes Recall metric.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      thresholds: (Optional) A float value or a python list/tuple of float\n        threshold values in [0, 1]. A threshold is compared with prediction\n        values to determine the truth value of predictions (i.e., above the\n        threshold is `true`, below is `false`). One metric value is generated\n        for each threshold value. The default is to calculate precision with\n        `thresholds=0.5`.\n      name: (Optional) string name of the metric instance.\n      iou_threshold: (Optional) Thresholds for a detection and ground truth pair\n        with specific iou to be considered as a match. Default to 0.5\n      class_id: (Optional) The class id for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class id.\n      area_range: (Optional) A tuple (inclusive) representing the area-range for\n        objects to be considered for metrics. Default to (0, inf).\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image. Default to None.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds,\n        name=name,\n        class_id=class_id,\n        iou_threshold=iou_threshold,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        class_weight=class_weight,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecision.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fn\n    return _divide_only_positive_denominator(tp, tp + fp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall","title":"ObjectDetectionPrecisionAtRecall","text":"<pre><code>ObjectDetectionPrecisionAtRecall(\n    recall: Union[float, List[float]],\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>PrecisionAtRecall</code></p> <p>Computes best precision where recall is &gt;= specified value.</p> <p>The threshold for the given recall value is computed and used to evaluate the corresponding precision.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes PrecisionAtRecall metric.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>recall: A scalar or a list of scalar values in range <code>[0, 1]</code>.   thresholds: (Optional) Thresholds to use for calculating the matrices. Use     one of either thresholds or num_thresholds.   num_thresholds: (Optional) Defaults to 1000. The number of thresholds to     use for matching the given recall.   name: (Optional) string name of the metric instance.   iou_threshold: (Optional) Thresholds for a detection and ground truth pair     with specific iou to be considered as a match. Default to 0.5   class_id: (Optional) The class id for calculating metrics.   class_weight: (Optional) The weight associated with the object class id.   area_range: (Optional) A tuple (inclusive) representing the area-range for     objects to be considered for metrics. Default to (0, inf).   max_num_detections: (Optional) The maximum number of detections for a     single image. Default to None.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    recall: Union[float, List[float]],\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes PrecisionAtRecall metric.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      recall: A scalar or a list of scalar values in range `[0, 1]`.\n      thresholds: (Optional) Thresholds to use for calculating the matrices. Use\n        one of either thresholds or num_thresholds.\n      num_thresholds: (Optional) Defaults to 1000. The number of thresholds to\n        use for matching the given recall.\n      name: (Optional) string name of the metric instance.\n      iou_threshold: (Optional) Thresholds for a detection and ground truth pair\n        with specific iou to be considered as a match. Default to 0.5\n      class_id: (Optional) The class id for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class id.\n      area_range: (Optional) A tuple (inclusive) representing the area-range for\n        objects to be considered for metrics. Default to (0, inf).\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image. Default to None.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    for r in [recall] if isinstance(recall, float) else recall:\n        if r &lt; 0 or r &gt; 1:\n            raise ValueError(\n                \"Argument `recall` must be in the range [0, 1]. \"\n                f\"Received: recall={r}\"\n            )\n\n    super().__init__(\n        thresholds=thresholds,\n        num_thresholds=num_thresholds,\n        recall=recall,\n        name=name,\n        class_id=class_id,\n        iou_threshold=iou_threshold,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        class_weight=class_weight,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionPrecisionAtRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall","title":"ObjectDetectionRecall","text":"<pre><code>ObjectDetectionRecall(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>Recall</code></p> <p>Computes the recall of the predictions with respect to the labels.</p> <p>The metric uses true positives and false negatives to compute recall by dividing the true positives by the sum of true positives and false negatives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes Recall metric.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>thresholds: (Optional) A float value or a python list/tuple of float     threshold values in [0, 1]. A threshold is compared with prediction     values to determine the truth value of predictions (i.e., above the     threshold is <code>true</code>, below is <code>false</code>). One metric value is generated     for each threshold value. The default is to calculate recall with     <code>thresholds=0.5</code>.   name: (Optional) string name of the metric instance.   iou_threshold: (Optional) Thresholds for a detection and ground truth pair     with specific iou to be considered as a match. Default to 0.5   class_id: (Optional) The class id for calculating metrics.   class_weight: (Optional) The weight associated with the object class id.   area_range: (Optional) A tuple (inclusive) representing the area-range for     objects to be considered for metrics. Default to (0, inf).   max_num_detections: (Optional) The maximum number of detections for a     single image. Default to None.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes Recall metric.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      thresholds: (Optional) A float value or a python list/tuple of float\n        threshold values in [0, 1]. A threshold is compared with prediction\n        values to determine the truth value of predictions (i.e., above the\n        threshold is `true`, below is `false`). One metric value is generated\n        for each threshold value. The default is to calculate recall with\n        `thresholds=0.5`.\n      name: (Optional) string name of the metric instance.\n      iou_threshold: (Optional) Thresholds for a detection and ground truth pair\n        with specific iou to be considered as a match. Default to 0.5\n      class_id: (Optional) The class id for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class id.\n      area_range: (Optional) A tuple (inclusive) representing the area-range for\n        objects to be considered for metrics. Default to (0, inf).\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image. Default to None.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds,\n        name=name,\n        class_id=class_id,\n        iou_threshold=iou_threshold,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        class_weight=class_weight,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionRecall.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(tp, tp + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall","title":"ObjectDetectionThresholdAtRecall","text":"<pre><code>ObjectDetectionThresholdAtRecall(\n    recall: Union[float, List[float]],\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n)\n</code></pre> <p>               Bases: <code>ThresholdAtRecall</code></p> <p>Computes maximum threshold where recall is &gt;= specified value.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes ThresholdAtRecall metric.</p> <p>The metric supports using multiple outputs to form the labels/predictions if the user specifies the label/predcition keys to stack. In this case, the metric is not expected to work with multi-outputs. The metric only supports multi outputs if the output of model is already pre-stacked in the expected format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for predictions.</p> <p>recall: A scalar or a list of scalar values in range <code>[0, 1]</code>.   thresholds: (Optional) Thresholds to use for calculating the matrices. Use     one of either thresholds or num_thresholds.   num_thresholds: (Optional) Defaults to 1000. The number of thresholds to     use for matching the given recall.   name: (Optional) string name of the metric instance.   iou_threshold: (Optional) Thresholds for a detection and ground truth pair     with specific iou to be considered as a match. Default to 0.5   class_id: (Optional) The class id for calculating metrics.   class_weight: (Optional) The weight associated with the object class id.   area_range: (Optional) A tuple (inclusive) representing the area-range for     objects to be considered for metrics. Default to (0, inf).   max_num_detections: (Optional) The maximum number of detections for a     single image. Default to None.   labels_to_stack: (Optional) Keys for columns to be stacked as a single     numpy array as the labels. It is searched under the key labels, features     and transformed features. The desired format is [left bounadary, top     boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id']   predictions_to_stack: (Optional) Output names for columns to be stacked as     a single numpy array as the prediction. It should be the model's output     names. The desired format is [left bounadary, top boudnary, right     boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',     'ymin', 'xmax', 'ymax', 'class_id', 'scores']   num_detections_key: (Optional) An output name in which to find the number     of detections to use for evaluation for a given example. It does nothing     if predictions_to_stack is not set. The value for this output should be     a scalar value or a single-value tensor. The stacked predicitions will     be truncated with the specified number of detections.   allow_missing_key: (Optional) If true, the preprocessor will return empty     array instead of raising errors.</p> Source code in <code>tensorflow_model_analysis/metrics/object_detection_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    recall: Union[float, List[float]],\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    name: Optional[str] = None,\n    iou_threshold: Optional[float] = None,\n    class_id: Optional[int] = None,\n    class_weight: Optional[float] = None,\n    area_range: Optional[Tuple[float, float]] = None,\n    max_num_detections: Optional[int] = None,\n    labels_to_stack: Optional[List[str]] = None,\n    predictions_to_stack: Optional[List[str]] = None,\n    num_detections_key: Optional[str] = None,\n    allow_missing_key: bool = False,\n):\n    \"\"\"Initializes ThresholdAtRecall metric.\n\n    The metric supports using multiple outputs to form the labels/predictions if\n    the user specifies the label/predcition keys to stack. In this case, the\n    metric is not expected to work with multi-outputs. The metric only supports\n    multi outputs if the output of model is already pre-stacked in the expected\n    format, i.e. ['xmin', 'ymin', 'xmax', 'ymax', 'class_id'] for labels and\n    ['xmin', 'ymin', 'xmax', 'ymax', 'class_id', 'confidence scores'] for\n    predictions.\n\n    Args:\n    ----\n      recall: A scalar or a list of scalar values in range `[0, 1]`.\n      thresholds: (Optional) Thresholds to use for calculating the matrices. Use\n        one of either thresholds or num_thresholds.\n      num_thresholds: (Optional) Defaults to 1000. The number of thresholds to\n        use for matching the given recall.\n      name: (Optional) string name of the metric instance.\n      iou_threshold: (Optional) Thresholds for a detection and ground truth pair\n        with specific iou to be considered as a match. Default to 0.5\n      class_id: (Optional) The class id for calculating metrics.\n      class_weight: (Optional) The weight associated with the object class id.\n      area_range: (Optional) A tuple (inclusive) representing the area-range for\n        objects to be considered for metrics. Default to (0, inf).\n      max_num_detections: (Optional) The maximum number of detections for a\n        single image. Default to None.\n      labels_to_stack: (Optional) Keys for columns to be stacked as a single\n        numpy array as the labels. It is searched under the key labels, features\n        and transformed features. The desired format is [left bounadary, top\n        boudnary, right boundary, bottom boundary, class id]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id']\n      predictions_to_stack: (Optional) Output names for columns to be stacked as\n        a single numpy array as the prediction. It should be the model's output\n        names. The desired format is [left bounadary, top boudnary, right\n        boundary, bottom boundary, class id, confidence score]. e.g. ['xmin',\n        'ymin', 'xmax', 'ymax', 'class_id', 'scores']\n      num_detections_key: (Optional) An output name in which to find the number\n        of detections to use for evaluation for a given example. It does nothing\n        if predictions_to_stack is not set. The value for this output should be\n        a scalar value or a single-value tensor. The stacked predicitions will\n        be truncated with the specified number of detections.\n      allow_missing_key: (Optional) If true, the preprocessor will return empty\n        array instead of raising errors.\n    \"\"\"\n    for r in [recall] if isinstance(recall, float) else recall:\n        if r &lt; 0 or r &gt; 1:\n            raise ValueError(\n                \"Argument `recall` must be in the range [0, 1]. \"\n                f\"Received: recall={r}\"\n            )\n\n    super().__init__(\n        thresholds=thresholds,\n        num_thresholds=num_thresholds,\n        recall=recall,\n        name=name,\n        class_id=class_id,\n        iou_threshold=iou_threshold,\n        area_range=area_range,\n        max_num_detections=max_num_detections,\n        class_weight=class_weight,\n        labels_to_stack=labels_to_stack,\n        predictions_to_stack=predictions_to_stack,\n        num_detections_key=num_detections_key,\n        allow_missing_key=allow_missing_key,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ObjectDetectionThresholdAtRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV","title":"PPV","text":"<pre><code>PPV(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>Precision</code></p> <p>Alias for Precision.</p> <p>Initializes PPV metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes PPV metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PPV.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fn\n    return _divide_only_positive_denominator(tp, tp + fp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PlotKey","title":"PlotKey","text":"<p>               Bases: <code>MetricKey</code></p> <p>A PlotKey is a metric key that uniquely identifies a plot.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PlotKey-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PlotKey.from_proto","title":"from_proto  <code>staticmethod</code>","text":"<pre><code>from_proto(pb: PlotKey) -&gt; PlotKey\n</code></pre> <p>Configures class from proto.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@staticmethod\ndef from_proto(pb: metrics_for_slice_pb2.PlotKey) -&gt; \"PlotKey\":\n    \"\"\"Configures class from proto.\"\"\"\n    example_weighted = None\n    if pb.HasField(\"example_weighted\"):\n        example_weighted = pb.example_weighted.value\n    return PlotKey(\n        name=pb.name,\n        model_name=pb.model_name,\n        output_name=pb.output_name,\n        sub_key=SubKey.from_proto(pb.sub_key),\n        example_weighted=example_weighted,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PlotKey.make_baseline_key","title":"make_baseline_key","text":"<pre><code>make_baseline_key(model_name: str) -&gt; MetricKey\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def make_baseline_key(self, model_name: str) -&gt; \"MetricKey\":\n    return self._replace(model_name=model_name, is_diff=False)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PlotKey.make_diff_key","title":"make_diff_key","text":"<pre><code>make_diff_key() -&gt; MetricKey\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def make_diff_key(self) -&gt; \"MetricKey\":\n    return self._replace(is_diff=True)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PlotKey.to_proto","title":"to_proto","text":"<pre><code>to_proto() -&gt; PlotKey\n</code></pre> <p>Converts key to proto.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def to_proto(\n    self,\n) -&gt; (\n    metrics_for_slice_pb2.PlotKey\n):  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n    \"\"\"Converts key to proto.\"\"\"\n    plot_key = metrics_for_slice_pb2.PlotKey()\n    if self.name:\n        plot_key.name = self.name\n    if self.model_name:\n        plot_key.model_name = self.model_name\n    if self.output_name:\n        plot_key.output_name = self.output_name\n    if self.sub_key:\n        plot_key.sub_key.CopyFrom(self.sub_key.to_proto())\n    if self.example_weighted is not None:\n        plot_key.example_weighted.value = self.example_weighted\n    return plot_key\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio","title":"PositiveLikelihoodRatio","text":"<pre><code>PositiveLikelihoodRatio(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Positive likelihood ratio (LR+).</p> <p>Initializes positive likelihood ratio.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes positive likelihood ratio.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PositiveLikelihoodRatio.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    tpr_denominator = tp + fn\n    fpr_denominator = fp + tn\n    if tpr_denominator &gt; 0.0 and fpr_denominator &gt; 0.0 and fp &gt; 0.0:\n        tpr = tp / tpr_denominator\n        fpr = fp / fpr_denominator\n        return tpr / fpr\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision","title":"Precision","text":"<pre><code>Precision(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Computes the precision of the predictions with respect to the labels.</p> <p>The metric uses true positives and false positives to compute precision by dividing the true positives by the sum of true positives and false positives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes Precision metric.</p> <p>thresholds: (Optional) A float value or a python list/tuple of float     threshold values in [0, 1]. A threshold is compared with prediction     values to determine the truth value of predictions (i.e., above the     threshold is <code>true</code>, below is <code>false</code>). One metric value is generated     for each threshold value. If neither thresholds nor top_k are set, the     default is to calculate precision with <code>thresholds=0.5</code>.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.   **kwargs: (Optional) Additional args to pass along to init (and eventually     on to _metric_computation and _metric_value).</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Initializes Precision metric.\n\n    Args:\n    ----\n      thresholds: (Optional) A float value or a python list/tuple of float\n        threshold values in [0, 1]. A threshold is compared with prediction\n        values to determine the truth value of predictions (i.e., above the\n        threshold is `true`, below is `false`). One metric value is generated\n        for each threshold value. If neither thresholds nor top_k are set, the\n        default is to calculate precision with `thresholds=0.5`.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n      **kwargs: (Optional) Additional args to pass along to init (and eventually\n        on to _metric_computation and _metric_value).\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, top_k=top_k, class_id=class_id, name=name, **kwargs\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Precision.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fn\n    return _divide_only_positive_denominator(tp, tp + fp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall","title":"PrecisionAtRecall","text":"<pre><code>PrecisionAtRecall(\n    recall: Union[float, List[float]],\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetricBase</code></p> <p>Computes best precision where recall is &gt;= specified value.</p> <p>The threshold for the given recall value is computed and used to evaluate the corresponding precision.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes PrecisionAtRecall metric.</p> <p>recall: A scalar or a list of scalar values in range <code>[0, 1]</code>.   thresholds: (Optional) Thresholds to use for calculating the matrices. Use     one of either thresholds or num_thresholds.   num_thresholds: (Optional) Defaults to 1000. The number of thresholds to     use for matching the given recall.   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   **kwargs: (Optional) Additional args to pass along to init (and eventually     on to _metric_computation and _metric_value)</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    recall: Union[float, List[float]],\n    thresholds: Optional[List[float]] = None,\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    **kwargs,\n):\n    \"\"\"Initializes PrecisionAtRecall metric.\n\n    Args:\n    ----\n      recall: A scalar or a list of scalar values in range `[0, 1]`.\n      thresholds: (Optional) Thresholds to use for calculating the matrices. Use\n        one of either thresholds or num_thresholds.\n      num_thresholds: (Optional) Defaults to 1000. The number of thresholds to\n        use for matching the given recall.\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      **kwargs: (Optional) Additional args to pass along to init (and eventually\n        on to _metric_computation and _metric_value)\n    \"\"\"\n    for r in [recall] if isinstance(recall, float) else recall:\n        if r &lt; 0 or r &gt; 1:\n            raise ValueError(\n                \"Argument `recall` must be in the range [0, 1]. \"\n                f\"Received: recall={r}\"\n            )\n\n    super().__init__(\n        thresholds=thresholds,\n        num_thresholds=num_thresholds,\n        recall=recall,\n        class_id=class_id,\n        name=name,\n        top_k=top_k,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrecisionAtRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Preprocessor","title":"Preprocessor","text":"<pre><code>Preprocessor(name: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>DoFn</code></p> <p>Preprocessor wrapper for preprocessing data in the metric computation.</p> <p>The preprocessor is a beam.DoFn that takes a extracts (or a list of extracts) as input (which typically will contain labels, predictions, example weights, and optionally features) and should return the initial state that the combiner will use as input. The output of a processor should only contain information needed by the combiner. Note that if a query_key is used the preprocessor will be passed a list of extracts as input representing the extracts that matched the query_key. The special FeaturePreprocessor can be used to add additional features to the default standard metric inputs.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Preprocessor--attributes","title":"Attributes","text":"<p>name: The name of the preprocessor. It should only be accessed by a property     function. It is a read only attribute, and is used to distinguish     different preprocessors.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def __init__(self, name: Optional[str] = None, **kwargs):\n    super().__init__(**kwargs)\n    self._name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Preprocessor-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Preprocessor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Preprocessor.preprocessor_id","title":"preprocessor_id  <code>property</code>","text":"<pre><code>preprocessor_id\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Preprocessor-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence","title":"Prevalence","text":"<pre><code>Prevalence(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Prevalence.</p> <p>Initializes prevalence.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes prevalence.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Prevalence.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return _divide_only_positive_denominator(tp + fn, tp + tn + fp + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold","title":"PrevalenceThreshold","text":"<pre><code>PrevalenceThreshold(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Prevalence threshold (PT).</p> <p>Initializes prevalence threshold.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes prevalence threshold.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.PrevalenceThreshold.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    tpr_denominator = tp + fn\n    tnr_denominator = tn + fp\n    if tpr_denominator &gt; 0.0 and tnr_denominator &gt; 0.0:\n        tpr = tp / tpr_denominator\n        tnr = tn / tnr_denominator\n        return (_pos_sqrt(tpr * (1 - tnr)) + tnr - 1) / (tpr + tnr - 1)\n    else:\n        return float(\"nan\")\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics","title":"QueryStatistics","text":"<pre><code>QueryStatistics(\n    total_queries_name: str = TOTAL_QUERIES_NAME,\n    total_documents_name: str = TOTAL_DOCUMENTS_NAME,\n    min_documents_name: str = MIN_DOCUMENTS_NAME,\n    max_documents_name: str = MAX_DOCUMENTS_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Query statistic metrics.</p> <p>These metrics are query/ranking based so a query_key must also be provided in the associated metrics spec.</p> <p>Initializes query statistics metrics.</p> <p>total_queries_name: Total queries metric name.   total_documents_name: Total documents metric name.   min_documents_name: Min documents name.   max_documents_name: Max documents name.</p> Source code in <code>tensorflow_model_analysis/metrics/query_statistics.py</code> <pre><code>def __init__(\n    self,\n    total_queries_name: str = TOTAL_QUERIES_NAME,\n    total_documents_name: str = TOTAL_DOCUMENTS_NAME,\n    min_documents_name: str = MIN_DOCUMENTS_NAME,\n    max_documents_name: str = MAX_DOCUMENTS_NAME,\n):\n    \"\"\"Initializes query statistics metrics.\n\n    Args:\n    ----\n      total_queries_name: Total queries metric name.\n      total_documents_name: Total documents metric name.\n      min_documents_name: Min documents name.\n      max_documents_name: Max documents name.\n    \"\"\"\n    super().__init__(\n        _query_statistics,\n        total_queries_name=total_queries_name,\n        total_documents_name=total_documents_name,\n        min_documents_name=min_documents_name,\n        max_documents_name=max_documents_name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.QueryStatistics.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall","title":"Recall","text":"<pre><code>Recall(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Computes the recall of the predictions with respect to the labels.</p> <p>The metric uses true positives and false negatives to compute recall by dividing the true positives by the sum of true positives and false negatives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes Recall metric.</p> <p>thresholds: (Optional) A float value or a python list/tuple of float     threshold values in [0, 1]. A threshold is compared with prediction     values to determine the truth value of predictions (i.e., above the     threshold is <code>true</code>, below is <code>false</code>). One metric value is generated     for each threshold value. If neither thresholds nor top_k are set, the     default is to calculate precision with <code>thresholds=0.5</code>.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.   **kwargs: (Optional) Additional args to pass along to init (and eventually     on to _metric_computation and _metric_value)</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Initializes Recall metric.\n\n    Args:\n    ----\n      thresholds: (Optional) A float value or a python list/tuple of float\n        threshold values in [0, 1]. A threshold is compared with prediction\n        values to determine the truth value of predictions (i.e., above the\n        threshold is `true`, below is `false`). One metric value is generated\n        for each threshold value. If neither thresholds nor top_k are set, the\n        default is to calculate precision with `thresholds=0.5`.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n      **kwargs: (Optional) Additional args to pass along to init (and eventually\n        on to _metric_computation and _metric_value)\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, top_k=top_k, class_id=class_id, name=name, **kwargs\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Recall.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(tp, tp + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision","title":"RecallAtPrecision","text":"<pre><code>RecallAtPrecision(\n    precision: float,\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetricBase</code></p> <p>Computes best recall where precision is &gt;= specified value.</p> <p>For a given score-label-distribution the required precision might not be achievable, in this case 0.0 is returned as recall.</p> <p>This metric creates three local variables, <code>true_positives</code>, <code>false_positives</code> and <code>false_negatives</code> that are used to compute the recall at the given precision. The threshold for the given precision value is computed and used to evaluate the corresponding recall.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes RecallAtPrecision.</p> <p>precision: A scalar value in range <code>[0, 1]</code>.   num_thresholds: (Optional) Defaults to 1000. The number of thresholds to     use for matching the given precision.   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    precision: float,\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n):\n    \"\"\"Initializes RecallAtPrecision.\n\n    Args:\n    ----\n      precision: A scalar value in range `[0, 1]`.\n      num_thresholds: (Optional) Defaults to 1000. The number of thresholds to\n        use for matching the given precision.\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n    \"\"\"\n    if precision &lt; 0 or precision &gt; 1:\n        raise ValueError(\n            \"Argument `precision` must be in the range [0, 1]. \"\n            f\"Received: precision={precision}\"\n        )\n    super().__init__(\n        num_thresholds=num_thresholds,\n        precision=precision,\n        class_id=class_id,\n        name=name,\n        top_k=top_k,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RecallAtPrecision.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination","title":"RelativeCoefficientOfDiscrimination","text":"<pre><code>RelativeCoefficientOfDiscrimination(\n    name: str = RELATIVE_COEFFICIENT_OF_DISCRIMINATION_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Relative coefficient of discrimination metric.</p> <p>The relative coefficient of discrimination measures the ratio between the average prediction for the positive examples and the average prediction for the negative examples. This has a very simple intuitive explanation, measuring how much higher is the prediction going to be for a positive example than for a negative example.</p> <p>Initializes relative coefficient of discrimination metric.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/tjur_discrimination.py</code> <pre><code>def __init__(self, name: str = RELATIVE_COEFFICIENT_OF_DISCRIMINATION_NAME):\n    \"\"\"Initializes relative coefficient of discrimination metric.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            _relative_coefficient_of_discrimination\n        ),\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.RelativeCoefficientOfDiscrimination.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot","title":"ScoreDistributionPlot","text":"<pre><code>ScoreDistributionPlot(\n    num_thresholds: int = DEFAULT_NUM_THRESHOLDS,\n    name: str = SCORE_DISTRIBUTION_PLOT_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Score distribution plot.</p> <p>Initializes confusion matrix plot.</p> <p>num_thresholds: Number of thresholds to use when discretizing the curve.     Values must be &gt; 1. Defaults to 1000.   name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/score_distribution_plot.py</code> <pre><code>def __init__(\n    self,\n    num_thresholds: int = DEFAULT_NUM_THRESHOLDS,\n    name: str = SCORE_DISTRIBUTION_PLOT_NAME,\n):\n    \"\"\"Initializes confusion matrix plot.\n\n    Args:\n    ----\n      num_thresholds: Number of thresholds to use when discretizing the curve.\n        Values must be &gt; 1. Defaults to 1000.\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_confusion_matrix_plot),\n        num_thresholds=num_thresholds,\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ScoreDistributionPlot.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix","title":"SemanticSegmentationConfusionMatrix","text":"<pre><code>SemanticSegmentationConfusionMatrix(\n    class_ids: List[int],\n    ground_truth_key: str,\n    prediction_key: str,\n    decode_ground_truth: bool = True,\n    decode_prediction: bool = False,\n    ignore_ground_truth_id: Optional[int] = None,\n    name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Computes confusion matrices for semantic segmentation.</p> <p>Initializes PrecisionAtRecall metric.</p> <p>class_ids: the class ids for calculating metrics.   ground_truth_key: the key for storing the ground truth of encoded image     with class ids.   prediction_key: the key for storing the predictions of encoded image with     class ids.   decode_ground_truth: If true, the ground truth is assumed to be bytes of     images and will be decoded. By default it is true assuming the label is     the bytes of image.   decode_prediction: If true, the prediction is assumed to be bytes of     images and will be decoded. By default it is false assuming the model     outputs numpy arrays or tensors.   ignore_ground_truth_id: (Optional) The id of ground truth to be ignored.   name: (Optional) string name of the metric instance.</p> Source code in <code>tensorflow_model_analysis/metrics/semantic_segmentation_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    class_ids: List[int],\n    ground_truth_key: str,\n    prediction_key: str,\n    decode_ground_truth: bool = True,\n    decode_prediction: bool = False,\n    ignore_ground_truth_id: Optional[int] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initializes PrecisionAtRecall metric.\n\n    Args:\n    ----\n      class_ids: the class ids for calculating metrics.\n      ground_truth_key: the key for storing the ground truth of encoded image\n        with class ids.\n      prediction_key: the key for storing the predictions of encoded image with\n        class ids.\n      decode_ground_truth: If true, the ground truth is assumed to be bytes of\n        images and will be decoded. By default it is true assuming the label is\n        the bytes of image.\n      decode_prediction: If true, the prediction is assumed to be bytes of\n        images and will be decoded. By default it is false assuming the model\n        outputs numpy arrays or tensors.\n      ignore_ground_truth_id: (Optional) The id of ground truth to be ignored.\n      name: (Optional) string name of the metric instance.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        name=name,\n        class_ids=class_ids,\n        ground_truth_key=ground_truth_key,\n        prediction_key=prediction_key,\n        decode_ground_truth=decode_ground_truth,\n        decode_prediction=decode_prediction,\n        ignore_ground_truth_id=ignore_ground_truth_id,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationConfusionMatrix.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive","title":"SemanticSegmentationFalsePositive","text":"<pre><code>SemanticSegmentationFalsePositive(\n    class_ids: List[int],\n    ground_truth_key: str,\n    prediction_key: str,\n    decode_ground_truth: bool = True,\n    decode_prediction: bool = False,\n    ignore_ground_truth_id: Optional[int] = None,\n    name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>SemanticSegmentationConfusionMatrixMetricBase</code></p> <p>Calculates the true postive for semantic segmentation.</p> <p>Initializes PrecisionAtRecall metric.</p> <p>class_ids: the class ids for calculating metrics.   ground_truth_key: the key for storing the ground truth of encoded image     with class ids.   prediction_key: the key for storing the predictions of encoded image with     class ids.   decode_ground_truth: If true, the ground truth is assumed to be bytes of     images and will be decoded. By default it is true assuming the label is     the bytes of image.   decode_prediction: If true, the prediction is assumed to be bytes of     images and will be decoded. By default it is false assuming the model     outputs numpy arrays or tensors.   ignore_ground_truth_id: (Optional) The id of ground truth to be ignored.   name: (Optional) string name of the metric instance.</p> Source code in <code>tensorflow_model_analysis/metrics/semantic_segmentation_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    class_ids: List[int],\n    ground_truth_key: str,\n    prediction_key: str,\n    decode_ground_truth: bool = True,\n    decode_prediction: bool = False,\n    ignore_ground_truth_id: Optional[int] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initializes PrecisionAtRecall metric.\n\n    Args:\n    ----\n      class_ids: the class ids for calculating metrics.\n      ground_truth_key: the key for storing the ground truth of encoded image\n        with class ids.\n      prediction_key: the key for storing the predictions of encoded image with\n        class ids.\n      decode_ground_truth: If true, the ground truth is assumed to be bytes of\n        images and will be decoded. By default it is true assuming the label is\n        the bytes of image.\n      decode_prediction: If true, the prediction is assumed to be bytes of\n        images and will be decoded. By default it is false assuming the model\n        outputs numpy arrays or tensors.\n      ignore_ground_truth_id: (Optional) The id of ground truth to be ignored.\n      name: (Optional) string name of the metric instance.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        name=name,\n        class_ids=class_ids,\n        ground_truth_key=ground_truth_key,\n        prediction_key=prediction_key,\n        decode_ground_truth=decode_ground_truth,\n        decode_prediction=decode_prediction,\n        ignore_ground_truth_id=ignore_ground_truth_id,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationFalsePositive.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive","title":"SemanticSegmentationTruePositive","text":"<pre><code>SemanticSegmentationTruePositive(\n    class_ids: List[int],\n    ground_truth_key: str,\n    prediction_key: str,\n    decode_ground_truth: bool = True,\n    decode_prediction: bool = False,\n    ignore_ground_truth_id: Optional[int] = None,\n    name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>SemanticSegmentationConfusionMatrixMetricBase</code></p> <p>Calculates the true postive for semantic segmentation.</p> <p>Initializes PrecisionAtRecall metric.</p> <p>class_ids: the class ids for calculating metrics.   ground_truth_key: the key for storing the ground truth of encoded image     with class ids.   prediction_key: the key for storing the predictions of encoded image with     class ids.   decode_ground_truth: If true, the ground truth is assumed to be bytes of     images and will be decoded. By default it is true assuming the label is     the bytes of image.   decode_prediction: If true, the prediction is assumed to be bytes of     images and will be decoded. By default it is false assuming the model     outputs numpy arrays or tensors.   ignore_ground_truth_id: (Optional) The id of ground truth to be ignored.   name: (Optional) string name of the metric instance.</p> Source code in <code>tensorflow_model_analysis/metrics/semantic_segmentation_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    class_ids: List[int],\n    ground_truth_key: str,\n    prediction_key: str,\n    decode_ground_truth: bool = True,\n    decode_prediction: bool = False,\n    ignore_ground_truth_id: Optional[int] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initializes PrecisionAtRecall metric.\n\n    Args:\n    ----\n      class_ids: the class ids for calculating metrics.\n      ground_truth_key: the key for storing the ground truth of encoded image\n        with class ids.\n      prediction_key: the key for storing the predictions of encoded image with\n        class ids.\n      decode_ground_truth: If true, the ground truth is assumed to be bytes of\n        images and will be decoded. By default it is true assuming the label is\n        the bytes of image.\n      decode_prediction: If true, the prediction is assumed to be bytes of\n        images and will be decoded. By default it is false assuming the model\n        outputs numpy arrays or tensors.\n      ignore_ground_truth_id: (Optional) The id of ground truth to be ignored.\n      name: (Optional) string name of the metric instance.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(self._metric_computations),\n        name=name,\n        class_ids=class_ids,\n        ground_truth_key=ground_truth_key,\n        prediction_key=prediction_key,\n        decode_ground_truth=decode_ground_truth,\n        decode_prediction=decode_prediction,\n        ignore_ground_truth_id=ignore_ground_truth_id,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SemanticSegmentationTruePositive.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity","title":"SensitivityAtSpecificity","text":"<pre><code>SensitivityAtSpecificity(\n    specificity: Union[float, List[float]],\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetricBase</code></p> <p>Computes best sensitivity where specificity is &gt;= specified value.</p> <p><code>Sensitivity</code> measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). <code>Specificity</code> measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)).</p> <p>The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>For additional information about specificity and sensitivity, see the following.</p> <p>Initializes SensitivityAtSpecificity metric.</p> <p>specificity: A scalar value in range <code>[0, 1]</code>.   num_thresholds: (Optional) Defaults to 1000. The number of thresholds to     use for matching the given specificity.   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    specificity: Union[float, List[float]],\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n):\n    \"\"\"Initializes SensitivityAtSpecificity metric.\n\n    Args:\n    ----\n      specificity: A scalar value in range `[0, 1]`.\n      num_thresholds: (Optional) Defaults to 1000. The number of thresholds to\n        use for matching the given specificity.\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n    \"\"\"\n    super().__init__(\n        num_thresholds=num_thresholds,\n        specificity=specificity,\n        class_id=class_id,\n        name=name,\n        top_k=top_k,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SensitivityAtSpecificity.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision","title":"SetMatchPrecision","text":"<pre><code>SetMatchPrecision(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    name: Optional[str] = None,\n    prediction_class_key: str = \"classes\",\n    prediction_score_key: str = \"scores\",\n    class_key: Optional[str] = None,\n    weight_key: Optional[str] = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Precision</code></p> <p>Computes precision for sets of labels and predictions.</p> <p>The metric deals with labels and predictions which are provided in the format of sets (stored as variable length numpy arrays). The precision is the micro averaged classification precision. The metric is suitable for the case where the number of classes is large or the list of classes could not be provided in advance.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision--example","title":"Example:","text":"<p>Label: ['cats'], Predictions: {'classes': ['cats, dogs']}</p> <p>The precision is 0.5.</p> <p>Initializes Precision metric.</p> <p>thresholds: (Optional) A float value or a python list/tuple of float     threshold values in [0, 1]. A threshold is compared with prediction     values to determine the truth value of predictions (i.e., above the     threshold is <code>true</code>, below is <code>false</code>). One metric value is generated     for each threshold value. If neither thresholds nor top_k are set, the     default is to calculate precision with <code>thresholds=0.5</code>.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are truncated and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. When     top_k is used, the default threshold is float('-inf'). In this case,     unmatched labels are still considered false negative, since they have     prediction with confidence score float('-inf'),   name: (Optional) string name of the metric instance.   prediction_class_key: the key name of the classes in prediction.   prediction_score_key: the key name of the scores in prediction.   class_key: (Optional) The key name of the classes in class-weight pairs.     If it is not provided, the classes are assumed to be the label classes.   weight_key: (Optional) The key name of the weights of classes in     class-weight pairs. The value in this key should be a numpy array of the     same length as the classes in class_key. The key should be stored under     the features key.   **kwargs: (Optional) Additional args to pass along to init (and eventually     on to _metric_computations and _metric_values). The args are passed to     the precision metric, the confusion matrix metric and binary     classification metric.</p> Source code in <code>tensorflow_model_analysis/metrics/set_match_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    name: Optional[str] = None,\n    prediction_class_key: str = \"classes\",\n    prediction_score_key: str = \"scores\",\n    class_key: Optional[str] = None,\n    weight_key: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Initializes Precision metric.\n\n    Args:\n    ----\n      thresholds: (Optional) A float value or a python list/tuple of float\n        threshold values in [0, 1]. A threshold is compared with prediction\n        values to determine the truth value of predictions (i.e., above the\n        threshold is `true`, below is `false`). One metric value is generated\n        for each threshold value. If neither thresholds nor top_k are set, the\n        default is to calculate precision with `thresholds=0.5`.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are truncated and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. When\n        top_k is used, the default threshold is float('-inf'). In this case,\n        unmatched labels are still considered false negative, since they have\n        prediction with confidence score float('-inf'),\n      name: (Optional) string name of the metric instance.\n      prediction_class_key: the key name of the classes in prediction.\n      prediction_score_key: the key name of the scores in prediction.\n      class_key: (Optional) The key name of the classes in class-weight pairs.\n        If it is not provided, the classes are assumed to be the label classes.\n      weight_key: (Optional) The key name of the weights of classes in\n        class-weight pairs. The value in this key should be a numpy array of the\n        same length as the classes in class_key. The key should be stored under\n        the features key.\n      **kwargs: (Optional) Additional args to pass along to init (and eventually\n        on to _metric_computations and _metric_values). The args are passed to\n        the precision metric, the confusion matrix metric and binary\n        classification metric.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds,\n        top_k=top_k,\n        name=name,\n        prediction_class_key=prediction_class_key,\n        prediction_score_key=prediction_score_key,\n        class_key=class_key,\n        weight_key=weight_key,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchPrecision.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fn\n    return _divide_only_positive_denominator(tp, tp + fp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall","title":"SetMatchRecall","text":"<pre><code>SetMatchRecall(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    name: Optional[str] = None,\n    prediction_class_key: str = \"classes\",\n    prediction_score_key: str = \"scores\",\n    class_key: Optional[str] = None,\n    weight_key: Optional[str] = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Recall</code></p> <p>Computes recall for sets of labels and predictions.</p> <p>The metric deals with labels and predictions which are provided in the format of sets (stored as variable length numpy arrays). The recall is the micro averaged classification recall. The metric is suitable for the case where the number of classes is large or the list of classes could not be provided in advance.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall--example","title":"Example:","text":"<p>Label: ['cats'], Predictions: {'classes': ['cats, dogs']}</p> <p>The recall is 1.</p> <p>Initializes recall metric.</p> <p>thresholds: (Optional) A float value or a python list/tuple of float     threshold values in [0, 1]. A threshold is compared with prediction     values to determine the truth value of predictions (i.e., above the     threshold is <code>true</code>, below is <code>false</code>). One metric value is generated     for each threshold value. If neither thresholds nor top_k are set, the     default is to calculate precision with <code>thresholds=0.5</code>.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are truncated and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. When     top_k is used, the default threshold is float('-inf'). In this case,     unmatched labels are still considered false negative, since they have     prediction with confidence score float('-inf'),   name: (Optional) string name of the metric instance.   prediction_class_key: the key name of the classes in prediction.   prediction_score_key: the key name of the scores in prediction.   class_key: (Optional) The key name of the classes in class-weight pairs.     If it is not provided, the classes are assumed to be the label classes.   weight_key: (Optional) The key name of the weights of classes in     class-weight pairs. The value in this key should be a numpy array of the     same length as the classes in class_key. The key should be stored under     the features key.   **kwargs: (Optional) Additional args to pass along to init (and eventually     on to _metric_computations and _metric_values). The args are passed to     the recall metric, the confusion matrix metric and binary classification     metric.</p> Source code in <code>tensorflow_model_analysis/metrics/set_match_confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    top_k: Optional[int] = None,\n    name: Optional[str] = None,\n    prediction_class_key: str = \"classes\",\n    prediction_score_key: str = \"scores\",\n    class_key: Optional[str] = None,\n    weight_key: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Initializes recall metric.\n\n    Args:\n    ----\n      thresholds: (Optional) A float value or a python list/tuple of float\n        threshold values in [0, 1]. A threshold is compared with prediction\n        values to determine the truth value of predictions (i.e., above the\n        threshold is `true`, below is `false`). One metric value is generated\n        for each threshold value. If neither thresholds nor top_k are set, the\n        default is to calculate precision with `thresholds=0.5`.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are truncated and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. When\n        top_k is used, the default threshold is float('-inf'). In this case,\n        unmatched labels are still considered false negative, since they have\n        prediction with confidence score float('-inf'),\n      name: (Optional) string name of the metric instance.\n      prediction_class_key: the key name of the classes in prediction.\n      prediction_score_key: the key name of the scores in prediction.\n      class_key: (Optional) The key name of the classes in class-weight pairs.\n        If it is not provided, the classes are assumed to be the label classes.\n      weight_key: (Optional) The key name of the weights of classes in\n        class-weight pairs. The value in this key should be a numpy array of the\n        same length as the classes in class_key. The key should be stored under\n        the features key.\n      **kwargs: (Optional) Additional args to pass along to init (and eventually\n        on to _metric_computations and _metric_values). The args are passed to\n        the recall metric, the confusion matrix metric and binary classification\n        metric.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds,\n        top_k=top_k,\n        name=name,\n        prediction_class_key=prediction_class_key,\n        prediction_score_key=prediction_score_key,\n        class_key=class_key,\n        weight_key=weight_key,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SetMatchRecall.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(tp, tp + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity","title":"Specificity","text":"<pre><code>Specificity(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Specificity (TNR) or selectivity.</p> <p>Initializes specificity metric.</p> <p>thresholds: (Optional) Thresholds to use for specificity. Defaults to     [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes specificity metric.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use for specificity. Defaults to\n        [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.Specificity.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tp, fn\n    return _divide_only_positive_denominator(tn, tn + fp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity","title":"SpecificityAtSensitivity","text":"<pre><code>SpecificityAtSensitivity(\n    sensitivity: float,\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetricBase</code></p> <p>Computes best specificity where sensitivity is &gt;= specified value.</p> <p><code>Sensitivity</code> measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). <code>Specificity</code> measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)).</p> <p>The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>For additional information about specificity and sensitivity, see the following.</p> <p>Initializes SpecificityAtSensitivity metric.</p> <p>sensitivity: A scalar value or a list of scalar value in range <code>[0, 1]</code>.   num_thresholds: (Optional) Defaults to 1000. The number of thresholds to     use for matching the given sensitivity.   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.   name: (Optional) string name of the metric instance.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    sensitivity: float,\n    num_thresholds: Optional[int] = None,\n    class_id: Optional[int] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n):\n    \"\"\"Initializes SpecificityAtSensitivity metric.\n\n    Args:\n    ----\n      sensitivity: A scalar value or a list of scalar value in range `[0, 1]`.\n      num_thresholds: (Optional) Defaults to 1000. The number of thresholds to\n        use for matching the given sensitivity.\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n      name: (Optional) string name of the metric instance.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n    \"\"\"\n    super().__init__(\n        num_thresholds=num_thresholds,\n        sensitivity=sensitivity,\n        class_id=class_id,\n        name=name,\n        top_k=top_k,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SpecificityAtSensitivity.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation","title":"SquaredPearsonCorrelation","text":"<pre><code>SquaredPearsonCorrelation(\n    name: str = SQUARED_PEARSON_CORRELATION_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>Squared pearson correlation (r^2) metric.</p> <p>Initializes squared pearson correlation (r^2) metric.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/squared_pearson_correlation.py</code> <pre><code>def __init__(self, name: str = SQUARED_PEARSON_CORRELATION_NAME):\n    \"\"\"Initializes squared pearson correlation (r^2) metric.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(_squared_pearson_correlation),\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SquaredPearsonCorrelation.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs","title":"StandardMetricInputs","text":"<pre><code>StandardMetricInputs(\n    extracts: Optional[Extracts] = None, **kwargs\n)\n</code></pre> <p>               Bases: <code>StandardExtracts</code></p> <p>Standard inputs used by most metric computations.</p> <p>StandardMetricInputs is a wrapper around Extracts where only the extracts keys used by one or more ExtractsPreprocessors will be present.</p> <p>Initializes StandardExtracts.</p> <p>extracts: Reference to existing extracts to use.   **kwargs: Name/value pairs to create new extracts from. Only one of either     extracts or kwargs should be used.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def __init__(self, extracts: Optional[types.Extracts] = None, **kwargs):\n    \"\"\"Initializes StandardExtracts.\n\n    Args:\n    ----\n      extracts: Reference to existing extracts to use.\n      **kwargs: Name/value pairs to create new extracts from. Only one of either\n        extracts or kwargs should be used.\n    \"\"\"\n    if extracts is not None and kwargs:\n        raise ValueError(\"only one of extracts or kwargs should be used\")\n    if extracts is not None:\n        self.extracts = extracts\n    else:\n        self.extracts = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.attributions","title":"attributions  <code>property</code>","text":"<pre><code>attributions: Optional[DictOfTensorValueMaybeDict]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.combined_features","title":"combined_features  <code>property</code>","text":"<pre><code>combined_features: Mapping[str, Any]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.example_weight","title":"example_weight  <code>property</code>","text":"<pre><code>example_weight: Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Same as example_weights (DEPRECATED - use example_weights).</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.example_weights","title":"example_weights  <code>property</code>","text":"<pre><code>example_weights: Optional[TensorValueMaybeMultiLevelDict]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.extracts","title":"extracts  <code>instance-attribute</code>","text":"<pre><code>extracts = extracts\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.features","title":"features  <code>property</code>","text":"<pre><code>features: Optional[DictOfTensorValueMaybeDict]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs: Any\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.label","title":"label  <code>property</code>","text":"<pre><code>label: Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Same as labels (DEPRECATED - use labels).</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.labels","title":"labels  <code>property</code>","text":"<pre><code>labels: Optional[TensorValueMaybeMultiLevelDict]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.prediction","title":"prediction  <code>property</code>","text":"<pre><code>prediction: Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Same as predictions (DEPRECATED - use predictions).</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: Optional[TensorValueMaybeMultiLevelDict]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.transformed_features","title":"transformed_features  <code>property</code>","text":"<pre><code>transformed_features: Optional[DictOfTensorValueMaybeDict]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_attributions","title":"get_attributions","text":"<pre><code>get_attributions(\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[DictOfTensorValueMaybeDict]\n</code></pre> <p>Returns tfma.ATTRIBUTIONS_KEY extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_attributions(\n    self, model_name: Optional[str] = None, output_name: Optional[str] = None\n) -&gt; Optional[types.DictOfTensorValueMaybeDict]:\n    \"\"\"Returns tfma.ATTRIBUTIONS_KEY extract.\"\"\"\n    return self.get_by_key(constants.ATTRIBUTIONS_KEY, model_name, output_name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_by_key","title":"get_by_key","text":"<pre><code>get_by_key(\n    key: str,\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Any\n</code></pre> <p>Returns item for key possibly filtered by model and/or output names.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_by_key(\n    self,\n    key: str,\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Any:\n    if key not in self and key.endswith(\"s\"):\n        # The previous version of StandardMetricInputs was a NamedTuple that\n        # used label, prediction, and example_weight as the field names. Some\n        # tests may be creating StandardMetricInputs using these names, so also\n        # search under the non-pluralized form of the key.\n        key = key[:-1]\n    return super().get_by_key(key, model_name, output_name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_combined_features","title":"get_combined_features","text":"<pre><code>get_combined_features(\n    model_name: Optional[str] = None,\n) -&gt; Mapping[str, Any]\n</code></pre> <p>Returns a combined extract of transformed features and features.</p> <p>In case of name collision, transformed features is looked up first and the value is returned when found.</p> <p>model_name: Optionally, the model name assosicated to the transformed     feature. This has no effect on the raw features extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_combined_features(\n    self, model_name: Optional[str] = None\n) -&gt; Mapping[str, Any]:\n    \"\"\"Returns a combined extract of transformed features and features.\n\n    In case of name collision, transformed features is looked up first and\n    the value is returned when found.\n\n    Args:\n    ----\n      model_name: Optionally, the model name assosicated to the transformed\n        feature. This has no effect on the raw features extract.\n    \"\"\"\n    return collections.ChainMap(\n        self.get_transformed_features(model_name) or {}, self.get_features() or {}\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_example_weights","title":"get_example_weights","text":"<pre><code>get_example_weights(\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Returns tfma.EXAMPLE_WEIGHTS_KEY extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_example_weights(\n    self, model_name: Optional[str] = None, output_name: Optional[str] = None\n) -&gt; Optional[types.TensorValueMaybeMultiLevelDict]:\n    \"\"\"Returns tfma.EXAMPLE_WEIGHTS_KEY extract.\"\"\"\n    return self.get_by_key(constants.EXAMPLE_WEIGHTS_KEY, model_name, output_name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_features","title":"get_features","text":"<pre><code>get_features() -&gt; Optional[DictOfTensorValueMaybeDict]\n</code></pre> <p>Returns tfma.FEATURES_KEY extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_features(self) -&gt; Optional[types.DictOfTensorValueMaybeDict]:\n    \"\"\"Returns tfma.FEATURES_KEY extract.\"\"\"\n    return self.get_by_key(constants.FEATURES_KEY)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_inputs","title":"get_inputs","text":"<pre><code>get_inputs() -&gt; Any\n</code></pre> <p>Returns tfma.INPUT_KEY extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_inputs(self) -&gt; Any:\n    \"\"\"Returns tfma.INPUT_KEY extract.\"\"\"\n    return self[constants.INPUT_KEY]\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_labels","title":"get_labels","text":"<pre><code>get_labels(\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Returns tfma.LABELS_KEY extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_labels(\n    self, model_name: Optional[str] = None, output_name: Optional[str] = None\n) -&gt; Optional[types.TensorValueMaybeMultiLevelDict]:\n    \"\"\"Returns tfma.LABELS_KEY extract.\"\"\"\n    return self.get_by_key(constants.LABELS_KEY, model_name, output_name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_model_and_output_names","title":"get_model_and_output_names","text":"<pre><code>get_model_and_output_names(\n    eval_config: EvalConfig,\n) -&gt; List[Tuple[Optional[str], Optional[str]]]\n</code></pre> <p>Returns a list of model_name-output_name tuples present in extracts.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_model_and_output_names(\n    self, eval_config: config_pb2.EvalConfig\n) -&gt; List[Tuple[Optional[str], Optional[str]]]:\n    \"\"\"Returns a list of model_name-output_name tuples present in extracts.\"\"\"\n    if constants.PREDICTIONS_KEY not in self:\n        logging.warning(\n            \"Attempting to get model names and output names from \"\n            \"extracts that don't contain predictions\"\n        )\n        return []\n    keys = []\n    predictions = self[constants.PREDICTIONS_KEY]\n    config_model_names = {spec.name for spec in eval_config.model_specs}\n    if not config_model_names:\n        config_model_names = {\"\"}\n    if isinstance(predictions, Mapping):\n        for key in _get_keys(predictions, max_depth=2):\n            if len(key) == 1:\n                # Because of the dynamic structure of Extracts, we don't know whether\n                # a single key is a model name or output name. To distinguish between\n                # these cases, we inspect the config.\n                if key[0] in config_model_names:\n                    # multi-model, single output case\n                    key = key + (None,)\n                else:\n                    # single model, multi-output case\n                    key = (None,) + key\n            keys.append(key)\n    else:\n        keys = [(None, None)]\n    return keys\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_predictions","title":"get_predictions","text":"<pre><code>get_predictions(\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Returns tfma.PREDICTIONS_KEY extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_predictions(\n    self, model_name: Optional[str] = None, output_name: Optional[str] = None\n) -&gt; Optional[types.TensorValueMaybeMultiLevelDict]:\n    \"\"\"Returns tfma.PREDICTIONS_KEY extract.\"\"\"\n    return self.get_by_key(constants.PREDICTIONS_KEY, model_name, output_name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.get_transformed_features","title":"get_transformed_features","text":"<pre><code>get_transformed_features(\n    model_name: Optional[str] = None,\n) -&gt; Optional[DictOfTensorValueMaybeDict]\n</code></pre> <p>Returns tfma.TRANSFORMED_FEATURES_KEY extract.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_transformed_features(\n    self, model_name: Optional[str] = None\n) -&gt; Optional[types.DictOfTensorValueMaybeDict]:\n    \"\"\"Returns tfma.TRANSFORMED_FEATURES_KEY extract.\"\"\"\n    return self.get_by_key(constants.TRANSFORMED_FEATURES_KEY, model_name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.set_labels","title":"set_labels","text":"<pre><code>set_labels(\n    labels: TensorValueMaybeMultiLevelDict,\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Sets tfma.LABELS_KEY extract for a given model and output.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def set_labels(\n    self,\n    labels: types.TensorValueMaybeMultiLevelDict,\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[types.TensorValueMaybeMultiLevelDict]:\n    \"\"\"Sets tfma.LABELS_KEY extract for a given model and output.\"\"\"\n    self._set_by_key(\n        key=constants.LABELS_KEY,\n        value=labels,\n        model_name=model_name,\n        output_name=output_name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.StandardMetricInputs.set_predictions","title":"set_predictions","text":"<pre><code>set_predictions(\n    predictions: TensorValueMaybeMultiLevelDict,\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[TensorValueMaybeMultiLevelDict]\n</code></pre> <p>Sets tfma.PREDICTIONS_KEY extract for a given model and output.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def set_predictions(\n    self,\n    predictions: types.TensorValueMaybeMultiLevelDict,\n    model_name: Optional[str] = None,\n    output_name: Optional[str] = None,\n) -&gt; Optional[types.TensorValueMaybeMultiLevelDict]:\n    \"\"\"Sets tfma.PREDICTIONS_KEY extract for a given model and output.\"\"\"\n    self._set_by_key(\n        key=constants.PREDICTIONS_KEY,\n        value=predictions,\n        model_name=model_name,\n        output_name=output_name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SubKey","title":"SubKey","text":"<p>               Bases: <code>NamedTuple('SubKey', [('class_id', int), ('k', int), ('top_k', int)])</code></p> <p>A SubKey identifies a sub-types of metrics and plots.</p> <p>Only one of class_id, k, or top_k can be set at a time.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SubKey--attributes","title":"Attributes","text":"<p>class_id: Used with multi-class metrics to identify a specific class ID.   k: Used with multi-class metrics to identify the kth predicted value.   top_k: Used with multi-class and ranking metrics to identify top-k predicted     values.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SubKey-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SubKey.from_proto","title":"from_proto  <code>staticmethod</code>","text":"<pre><code>from_proto(pb: SubKey) -&gt; Optional[SubKey]\n</code></pre> <p>Creates class from proto.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@staticmethod\ndef from_proto(pb: metrics_for_slice_pb2.SubKey) -&gt; Optional[\"SubKey\"]:\n    \"\"\"Creates class from proto.\"\"\"\n    class_id = None\n    if pb.HasField(\"class_id\"):\n        class_id = pb.class_id.value\n    k = None\n    if pb.HasField(\"k\"):\n        k = pb.k.value\n    top_k = None\n    if pb.HasField(\"top_k\"):\n        top_k = pb.top_k.value\n    if class_id is None and k is None and top_k is None:\n        return None\n    else:\n        return SubKey(class_id=class_id, k=k, top_k=top_k)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SubKey.to_proto","title":"to_proto","text":"<pre><code>to_proto() -&gt; SubKey\n</code></pre> <p>Converts key to proto.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def to_proto(self) -&gt; metrics_for_slice_pb2.SubKey:\n    \"\"\"Converts key to proto.\"\"\"\n    sub_key = metrics_for_slice_pb2.SubKey()\n    if self.class_id is not None:\n        sub_key.class_id.value = self.class_id\n    if self.k is not None:\n        sub_key.k.value = self.k\n    if self.top_k is not None:\n        sub_key.top_k.value = self.top_k\n    return sub_key\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference","title":"SymmetricPredictionDifference","text":"<pre><code>SymmetricPredictionDifference(\n    name: str = SYMMETRIC_PREDICITON_DIFFERENCE_NAME,\n)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>PredictionDifference computes the avg pointwise diff between models.</p> <p>Initializes PredictionDifference metric.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/prediction_difference_metrics.py</code> <pre><code>def __init__(self, name: str = SYMMETRIC_PREDICITON_DIFFERENCE_NAME):\n    \"\"\"Initializes PredictionDifference metric.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(_symmetric_prediction_difference_computations, name=name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.SymmetricPredictionDifference.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN","title":"TN","text":"<pre><code>TN(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>TrueNegatives</code></p> <p>Alias for TrueNegatives.</p> <p>Initializes TN metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes TN metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TN.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return tn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR","title":"TNR","text":"<pre><code>TNR(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>Specificity</code></p> <p>Alias for Specificity.</p> <p>Initializes TNR metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes TNR metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TNR.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tp, fn\n    return _divide_only_positive_denominator(tn, tn + fp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP","title":"TP","text":"<pre><code>TP(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>TruePositives</code></p> <p>Alias for TruePositives.</p> <p>Initializes TP metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes TP metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TP.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return tp\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR","title":"TPR","text":"<pre><code>TPR(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>Recall</code></p> <p>Alias for Recall.</p> <p>Initializes TPR metric.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes TPR metric.\"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TPR.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn, fp\n    return _divide_only_positive_denominator(tp, tp + fn)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore","title":"ThreatScore","text":"<pre><code>ThreatScore(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Threat score or critical success index (TS or CSI).</p> <p>Initializes threat score.</p> <p>thresholds: (Optional) Thresholds to use. Defaults to [0.5].   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes threat score.\n\n    Args:\n    ----\n      thresholds: (Optional) Thresholds to use. Defaults to [0.5].\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.ThreatScore.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    del tn\n    return _divide_only_positive_denominator(tp, tp + fn + fp)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions","title":"TotalAbsoluteAttributions","text":"<pre><code>TotalAbsoluteAttributions(\n    name: str = TOTAL_ABSOLUTE_ATTRIBUTIONS_NAME,\n)\n</code></pre> <p>               Bases: <code>AttributionsMetric</code></p> <p>Total absolute attributions metric.</p> <p>Initializes total absolute attributions metric.</p> <p>name: Attribution metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/attributions.py</code> <pre><code>def __init__(self, name: str = TOTAL_ABSOLUTE_ATTRIBUTIONS_NAME):\n    \"\"\"Initializes total absolute attributions metric.\n\n    Args:\n    ----\n      name: Attribution metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            functools.partial(_total_attributions, True)\n        ),\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAbsoluteAttributions.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions","title":"TotalAttributions","text":"<pre><code>TotalAttributions(name: str = TOTAL_ATTRIBUTIONS_NAME)\n</code></pre> <p>               Bases: <code>AttributionsMetric</code></p> <p>Total attributions metric.</p> <p>Initializes total attributions metric.</p> <p>name: Attribution metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/attributions.py</code> <pre><code>def __init__(self, name: str = TOTAL_ATTRIBUTIONS_NAME):\n    \"\"\"Initializes total attributions metric.\n\n    Args:\n    ----\n      name: Attribution metric name.\n    \"\"\"\n    super().__init__(\n        metric_util.merge_per_key_computations(\n            functools.partial(_total_attributions, False)\n        ),\n        name=name,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TotalAttributions.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives","title":"TrueNegatives","text":"<pre><code>TrueNegatives(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Calculates the number of true negatives.</p> <p>If <code>sample_weight</code> is given, calculates the sum of the weights of true negatives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes TrueNegatives metric.</p> <p>thresholds: (Optional) Defaults to [0.5]. A float value or a python     list/tuple of float threshold values in [0, 1]. A threshold is compared     with prediction values to determine the truth value of predictions     (i.e., above the threshold is <code>true</code>, below is <code>false</code>). One metric     value is generated for each threshold value.   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes TrueNegatives metric.\n\n    Args:\n    ----\n      thresholds: (Optional) Defaults to [0.5]. A float value or a python\n        list/tuple of float threshold values in [0, 1]. A threshold is compared\n        with prediction values to determine the truth value of predictions\n        (i.e., above the threshold is `true`, below is `false`). One metric\n        value is generated for each threshold value.\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TrueNegatives.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return tn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives","title":"TruePositives","text":"<pre><code>TruePositives(\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>ConfusionMatrixMetric</code></p> <p>Calculates the number of true positives.</p> <p>If <code>sample_weight</code> is given, calculates the sum of the weights of true positives. This metric creates one local variable, <code>true_positives</code> that is used to keep track of the number of true positives.</p> <p>If <code>sample_weight</code> is <code>None</code>, weights default to 1. Use <code>sample_weight</code> of 0 to mask values.</p> <p>Initializes TruePositives metric.</p> <p>thresholds: (Optional) Defaults to [0.5]. A float value or a python     list/tuple of float threshold values in [0, 1]. A threshold is compared     with prediction values to determine the truth value of predictions     (i.e., above the threshold is <code>true</code>, below is <code>false</code>). One metric     value is generated for each threshold value.   name: (Optional) Metric name.   top_k: (Optional) Used with a multi-class model to specify that the top-k     values should be used to compute the confusion matrix. The net effect is     that the non-top-k values are set to -inf and the matrix is then     constructed from the average TP, FP, TN, FN across the classes. When     top_k is used, metrics_specs.binarize settings must not be present. Only     one of class_id or top_k should be configured. When top_k is set, the     default thresholds are [float('-inf')].   class_id: (Optional) Used with a multi-class model to specify which class     to compute the confusion matrix for. When class_id is used,     metrics_specs.binarize settings must not be present. Only one of     class_id or top_k should be configured.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def __init__(\n    self,\n    thresholds: Optional[Union[float, List[float]]] = None,\n    name: Optional[str] = None,\n    top_k: Optional[int] = None,\n    class_id: Optional[int] = None,\n):\n    \"\"\"Initializes TruePositives metric.\n\n    Args:\n    ----\n      thresholds: (Optional) Defaults to [0.5]. A float value or a python\n        list/tuple of float threshold values in [0, 1]. A threshold is compared\n        with prediction values to determine the truth value of predictions\n        (i.e., above the threshold is `true`, below is `false`). One metric\n        value is generated for each threshold value.\n      name: (Optional) Metric name.\n      top_k: (Optional) Used with a multi-class model to specify that the top-k\n        values should be used to compute the confusion matrix. The net effect is\n        that the non-top-k values are set to -inf and the matrix is then\n        constructed from the average TP, FP, TN, FN across the classes. When\n        top_k is used, metrics_specs.binarize settings must not be present. Only\n        one of class_id or top_k should be configured. When top_k is set, the\n        default thresholds are [float('-inf')].\n      class_id: (Optional) Used with a multi-class model to specify which class\n        to compute the confusion matrix for. When class_id is used,\n        metrics_specs.binarize settings must not be present. Only one of\n        class_id or top_k should be configured.\n    \"\"\"\n    super().__init__(\n        thresholds=thresholds, name=name, top_k=top_k, class_id=class_id\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Whether to compute confidence intervals for this metric.</p> <p>Note that this may not completely remove the computational overhead involved in computing a given metric. This is only respected by the jackknife confidence interval method.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals for this metric.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    # Not all subclasses of ConfusionMatrixMetric support all the __init__\n    # parameters as part of their __init__, to avoid deserialization issues\n    # where an unsupported parameter is passed to the subclass, filter out any\n    # parameters that are None.\n    kwargs = copy.copy(self.kwargs)\n    for arg in (\"thresholds\", \"num_thresholds\", \"top_k\", \"class_id\"):\n        if kwargs[arg] is None:\n            del kwargs[arg]\n    return kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.TruePositives.result","title":"result","text":"<pre><code>result(tp: float, tn: float, fp: float, fn: float) -&gt; float\n</code></pre> <p>Function for computing metric value from TP, TN, FP, FN values.</p> Source code in <code>tensorflow_model_analysis/metrics/confusion_matrix_metrics.py</code> <pre><code>def result(self, tp: float, tn: float, fp: float, fn: float) -&gt; float:\n    return tp\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount","title":"WeightedExampleCount","text":"<pre><code>WeightedExampleCount(\n    name: str = WEIGHTED_EXAMPLE_COUNT_NAME,\n)\n</code></pre> <p>               Bases: <code>ExampleCount</code></p> <p>Weighted example count (deprecated - use ExampleCount).</p> <p>Initializes weighted example count.</p> <p>name: Metric name.</p> Source code in <code>tensorflow_model_analysis/metrics/weighted_example_count.py</code> <pre><code>def __init__(self, name: str = WEIGHTED_EXAMPLE_COUNT_NAME):\n    \"\"\"Initializes weighted example count.\n\n    Args:\n    ----\n      name: Metric name.\n    \"\"\"\n    super().__init__(name=name)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.compute_confidence_interval","title":"compute_confidence_interval  <code>property</code>","text":"<pre><code>compute_confidence_interval: bool\n</code></pre> <p>Always disable confidence intervals for ExampleCount.</p> <p>Confidence intervals capture uncertainty in a metric if it were computed on more examples. For ExampleCount, this sort of uncertainty is not meaningful, so confidence intervals are disabled.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.compute_confidence_interval--returns","title":"Returns","text":"<p>Whether to compute confidence intervals.</p>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.create_computations_fn","title":"create_computations_fn  <code>instance-attribute</code>","text":"<pre><code>create_computations_fn = create_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.computations","title":"computations","text":"<pre><code>computations(\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations\n</code></pre> <p>Creates computations associated with metric.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def computations(\n    self,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    sub_keys: Optional[List[Optional[SubKey]]] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    query_key: Optional[str] = None,\n) -&gt; MetricComputations:\n    \"\"\"Creates computations associated with metric.\"\"\"\n    updated_kwargs = validate_and_update_create_computations_fn_kwargs(\n        self._args,\n        self.kwargs.copy(),\n        eval_config,\n        schema,\n        model_names,\n        output_names,\n        sub_keys,\n        aggregation_type,\n        class_weights,\n        example_weighted,\n        query_key,\n    )\n    return self.create_computations_fn(**updated_kwargs)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: Dict[str, Any]) -&gt; Metric\n</code></pre> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>@classmethod\ndef from_config(cls, config: Dict[str, Any]) -&gt; \"Metric\":\n    # `fn` key is unnecessary for wrapper due to\n    # `create_computation_fn` key serialization.\n    config.pop(\"fn\", None)\n    return cls(**config)\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.WeightedExampleCount.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict[str, Any]\n</code></pre> <p>Returns serializable config.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns serializable config.\"\"\"\n    return self.kwargs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.CombinedFeaturePreprocessor","title":"CombinedFeaturePreprocessor","text":"<pre><code>CombinedFeaturePreprocessor(\n    feature_keys: Iterable[str],\n    include_default_inputs: bool = True,\n    model_names: Optional[Iterable[str]] = None,\n    output_names: Optional[Iterable[str]] = None,\n) -&gt; StandardMetricInputsPreprocessor\n</code></pre> <p>Returns preprocessor for incl combined features in StandardMetricInputs.</p> <p>feature_keys: List of feature keys. An empty list means all.   include_default_inputs: True to include default inputs (labels, predictions,     example weights) in addition to the transformed features.   model_names: Optional model names (required if transformed_features used     with multi-model evaluations).   output_names: Optional output names. Only used if include_default_inputs is     True. If unset all outputs will be included with the default inputs.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def CombinedFeaturePreprocessor(  # pylint: disable=invalid-name\n    feature_keys: Iterable[str],\n    include_default_inputs: bool = True,\n    model_names: Optional[Iterable[str]] = None,\n    output_names: Optional[Iterable[str]] = None,\n) -&gt; StandardMetricInputsPreprocessor:\n    \"\"\"Returns preprocessor for incl combined features in StandardMetricInputs.\n\n    Args:\n    ----\n      feature_keys: List of feature keys. An empty list means all.\n      include_default_inputs: True to include default inputs (labels, predictions,\n        example weights) in addition to the transformed features.\n      model_names: Optional model names (required if transformed_features used\n        with multi-model evaluations).\n      output_names: Optional output names. Only used if include_default_inputs is\n        True. If unset all outputs will be included with the default inputs.\n    \"\"\"\n    if feature_keys:\n        include_features = {k: {} for k in feature_keys}\n    else:\n        include_features = {}\n    if model_names:\n        include_features = {name: include_features for name in model_names}\n    return StandardMetricInputsPreprocessor(\n        include_filter={\n            constants.TRANSFORMED_FEATURES_KEY: include_features,\n            constants.FEATURES_KEY: include_features,\n        },\n        include_default_inputs=include_default_inputs,\n        model_names=model_names,\n        output_names=output_names,\n        name=_DEFAULT_COMBINED_FEATURE_PREPROCESSOR_NAME,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.FeaturePreprocessor","title":"FeaturePreprocessor","text":"<pre><code>FeaturePreprocessor(\n    feature_keys: Iterable[str],\n    include_default_inputs: bool = True,\n    model_names: Optional[Iterable[str]] = None,\n    output_names: Optional[Iterable[str]] = None,\n) -&gt; StandardMetricInputsPreprocessor\n</code></pre> <p>Returns preprocessor for including features in StandardMetricInputs.</p> <p>feature_keys: List of feature keys. An empty list means all.   include_default_inputs: True to include default inputs (labels, predictions,     example weights) in addition to the features.   model_names: Optional model names. Only used if include_default_inputs is     True. If unset all models will be included with the default inputs.   output_names: Optional output names. Only used if include_default_inputs is     True. If unset all outputs will be included with the default inputs.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_types.py</code> <pre><code>def FeaturePreprocessor(  # pylint: disable=invalid-name\n    feature_keys: Iterable[str],\n    include_default_inputs: bool = True,\n    model_names: Optional[Iterable[str]] = None,\n    output_names: Optional[Iterable[str]] = None,\n) -&gt; StandardMetricInputsPreprocessor:\n    \"\"\"Returns preprocessor for including features in StandardMetricInputs.\n\n    Args:\n    ----\n      feature_keys: List of feature keys. An empty list means all.\n      include_default_inputs: True to include default inputs (labels, predictions,\n        example weights) in addition to the features.\n      model_names: Optional model names. Only used if include_default_inputs is\n        True. If unset all models will be included with the default inputs.\n      output_names: Optional output names. Only used if include_default_inputs is\n        True. If unset all outputs will be included with the default inputs.\n    \"\"\"\n    if feature_keys:\n        include_features = {k: {} for k in feature_keys}\n    else:\n        include_features = {}\n    return StandardMetricInputsPreprocessor(\n        include_filter={constants.FEATURES_KEY: include_features},\n        include_default_inputs=include_default_inputs,\n        model_names=model_names,\n        output_names=output_names,\n        name=_DEFAULT_FEATURE_PREPROCESSOR_NAME,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.default_binary_classification_specs","title":"default_binary_classification_specs","text":"<pre><code>default_binary_classification_specs(\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    binarize: Optional[BinarizationOptions] = None,\n    aggregate: Optional[AggregationOptions] = None,\n    include_loss: bool = True,\n) -&gt; List[MetricsSpec]\n</code></pre> <p>Returns default metric specs for binary classification problems.</p> <p>model_names: Optional model names (if multi-model evaluation).   output_names: Optional list of output names (if multi-output model).   output_weights: Optional output weights for creating overall metric     aggregated across outputs (if multi-output model). If a weight is not     provided for an output, it's weight defaults to 0.0 (i.e. output ignored).   binarize: Optional settings for binarizing multi-class/multi-label metrics.   aggregate: Optional settings for aggregating multi-class/multi-label     metrics.   include_loss: True to include loss.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_specs.py</code> <pre><code>def default_binary_classification_specs(\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    binarize: Optional[config_pb2.BinarizationOptions] = None,\n    aggregate: Optional[config_pb2.AggregationOptions] = None,\n    include_loss: bool = True,\n) -&gt; List[config_pb2.MetricsSpec]:\n    \"\"\"Returns default metric specs for binary classification problems.\n\n    Args:\n    ----\n      model_names: Optional model names (if multi-model evaluation).\n      output_names: Optional list of output names (if multi-output model).\n      output_weights: Optional output weights for creating overall metric\n        aggregated across outputs (if multi-output model). If a weight is not\n        provided for an output, it's weight defaults to 0.0 (i.e. output ignored).\n      binarize: Optional settings for binarizing multi-class/multi-label metrics.\n      aggregate: Optional settings for aggregating multi-class/multi-label\n        metrics.\n      include_loss: True to include loss.\n    \"\"\"\n    metrics = [\n        confusion_matrix_metrics.BinaryAccuracy(name=\"binary_accuracy\"),\n        confusion_matrix_metrics.AUC(\n            name=\"auc\", num_thresholds=binary_confusion_matrices.DEFAULT_NUM_THRESHOLDS\n        ),\n        confusion_matrix_metrics.AUC(\n            name=\"auc_precison_recall\",  # Matches default name used by estimator.\n            curve=\"PR\",\n            num_thresholds=binary_confusion_matrices.DEFAULT_NUM_THRESHOLDS,\n        ),\n        confusion_matrix_metrics.Precision(name=\"precision\"),\n        confusion_matrix_metrics.Recall(name=\"recall\"),\n        calibration.MeanLabel(name=\"mean_label\"),\n        calibration.MeanPrediction(name=\"mean_prediction\"),\n        calibration.Calibration(name=\"calibration\"),\n        confusion_matrix_plot.ConfusionMatrixPlot(name=\"confusion_matrix_plot\"),\n        calibration_plot.CalibrationPlot(name=\"calibration_plot\"),\n    ]\n    if include_loss:\n        metrics.append(tf_keras.metrics.BinaryCrossentropy(name=\"loss\"))\n\n    return specs_from_metrics(\n        metrics,\n        model_names=model_names,\n        output_names=output_names,\n        output_weights=output_weights,\n        binarize=binarize,\n        aggregate=aggregate,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.default_multi_class_classification_specs","title":"default_multi_class_classification_specs","text":"<pre><code>default_multi_class_classification_specs(\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    binarize: Optional[BinarizationOptions] = None,\n    aggregate: Optional[AggregationOptions] = None,\n    sparse: bool = True,\n) -&gt; List[MetricsSpec]\n</code></pre> <p>Returns default metric specs for multi-class classification problems.</p> <p>model_names: Optional model names if multi-model evaluation.   output_names: Optional list of output names (if multi-output model).   output_weights: Optional output weights for creating overall metric     aggregated across outputs (if multi-output model). If a weight is not     provided for an output, it's weight defaults to 0.0 (i.e. output ignored).   binarize: Optional settings for binarizing multi-class/multi-label metrics.   aggregate: Optional settings for aggregating multi-class/multi-label     metrics.   sparse: True if the labels are sparse.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_specs.py</code> <pre><code>def default_multi_class_classification_specs(\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    binarize: Optional[config_pb2.BinarizationOptions] = None,\n    aggregate: Optional[config_pb2.AggregationOptions] = None,\n    sparse: bool = True,\n) -&gt; List[config_pb2.MetricsSpec]:\n    \"\"\"Returns default metric specs for multi-class classification problems.\n\n    Args:\n    ----\n      model_names: Optional model names if multi-model evaluation.\n      output_names: Optional list of output names (if multi-output model).\n      output_weights: Optional output weights for creating overall metric\n        aggregated across outputs (if multi-output model). If a weight is not\n        provided for an output, it's weight defaults to 0.0 (i.e. output ignored).\n      binarize: Optional settings for binarizing multi-class/multi-label metrics.\n      aggregate: Optional settings for aggregating multi-class/multi-label\n        metrics.\n      sparse: True if the labels are sparse.\n    \"\"\"\n    if sparse:\n        metrics = [\n            tf_keras.metrics.SparseCategoricalCrossentropy(name=\"loss\"),\n            tf_keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n        ]\n    else:\n        metrics = [\n            tf_keras.metrics.CategoricalCrossentropy(name=\"loss\"),\n            tf_keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n        ]\n    metrics.append(multi_class_confusion_matrix_plot.MultiClassConfusionMatrixPlot())\n    if binarize is not None:\n        for top_k in binarize.top_k_list.values:\n            metrics.extend(\n                [\n                    confusion_matrix_metrics.Precision(name=\"precision\", top_k=top_k),\n                    confusion_matrix_metrics.Recall(name=\"recall\", top_k=top_k),\n                ]\n            )\n        binarize_without_top_k = config_pb2.BinarizationOptions()\n        binarize_without_top_k.CopyFrom(binarize)\n        binarize_without_top_k.ClearField(\"top_k_list\")\n        binarize = binarize_without_top_k\n    multi_class_metrics = specs_from_metrics(\n        metrics,\n        model_names=model_names,\n        output_names=output_names,\n        output_weights=output_weights,\n    )\n    if aggregate is None:\n        aggregate = config_pb2.AggregationOptions(micro_average=True)\n    multi_class_metrics.extend(\n        default_binary_classification_specs(\n            model_names=model_names,\n            output_names=output_names,\n            output_weights=output_weights,\n            binarize=binarize,\n            aggregate=aggregate,\n        )\n    )\n    return multi_class_metrics\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.default_regression_specs","title":"default_regression_specs","text":"<pre><code>default_regression_specs(\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    loss_functions: Optional[\n        List[Union[Metric, Loss]]\n    ] = None,\n    min_value: Optional[float] = None,\n    max_value: Optional[float] = None,\n) -&gt; List[MetricsSpec]\n</code></pre> <p>Returns default metric specs for for regression problems.</p> <p>model_names: Optional model names (if multi-model evaluation).   output_names: Optional list of output names (if multi-output model).   output_weights: Optional output weights for creating overall metric     aggregated across outputs (if multi-output model). If a weight is not     provided for an output, it's weight defaults to 0.0 (i.e. output ignored).   loss_functions: Loss functions to use (if None MSE is used).   min_value: Min value for calibration plot (if None no plot will be created).   max_value: Max value for calibration plot (if None no plot will be created).</p> Source code in <code>tensorflow_model_analysis/metrics/metric_specs.py</code> <pre><code>def default_regression_specs(\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    loss_functions: Optional[\n        List[Union[tf_keras.metrics.Metric, tf_keras.losses.Loss]]\n    ] = None,\n    min_value: Optional[float] = None,\n    max_value: Optional[float] = None,\n) -&gt; List[config_pb2.MetricsSpec]:\n    \"\"\"Returns default metric specs for for regression problems.\n\n    Args:\n    ----\n      model_names: Optional model names (if multi-model evaluation).\n      output_names: Optional list of output names (if multi-output model).\n      output_weights: Optional output weights for creating overall metric\n        aggregated across outputs (if multi-output model). If a weight is not\n        provided for an output, it's weight defaults to 0.0 (i.e. output ignored).\n      loss_functions: Loss functions to use (if None MSE is used).\n      min_value: Min value for calibration plot (if None no plot will be created).\n      max_value: Max value for calibration plot (if None no plot will be created).\n    \"\"\"\n    if loss_functions is None:\n        loss_functions = [tf_keras.metrics.MeanSquaredError(name=\"mse\")]\n\n    metrics = [\n        tf_keras.metrics.Accuracy(name=\"accuracy\"),\n        calibration.MeanLabel(name=\"mean_label\"),\n        calibration.MeanPrediction(name=\"mean_prediction\"),\n        calibration.Calibration(name=\"calibration\"),\n    ]\n    for fn in loss_functions:\n        metrics.append(fn)\n    if min_value is not None and max_value is not None:\n        metrics.append(\n            calibration_plot.CalibrationPlot(\n                name=\"calibration_plot\", left=min_value, right=max_value\n            )\n        )\n\n    return specs_from_metrics(\n        metrics,\n        model_names=model_names,\n        output_names=output_names,\n        output_weights=output_weights,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.has_attributions_metrics","title":"has_attributions_metrics","text":"<pre><code>has_attributions_metrics(\n    metrics_specs: Iterable[MetricsSpec],\n) -&gt; bool\n</code></pre> <p>Returns true if any of the metrics_specs have attributions metrics.</p> Source code in <code>tensorflow_model_analysis/metrics/attributions.py</code> <pre><code>def has_attributions_metrics(metrics_specs: Iterable[config_pb2.MetricsSpec]) -&gt; bool:\n    \"\"\"Returns true if any of the metrics_specs have attributions metrics.\"\"\"\n    tfma_metric_classes = metric_types.registered_metrics()\n    for metrics_spec in metrics_specs:\n        for metric_config in metrics_spec.metrics:\n            instance = metric_specs.metric_instance(metric_config, tfma_metric_classes)\n            if isinstance(instance, AttributionsMetric):\n                return True\n    return False\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.merge_per_key_computations","title":"merge_per_key_computations","text":"<pre><code>merge_per_key_computations(\n    create_computations_fn: Callable[\n        ..., MetricComputations\n    ],\n) -&gt; Callable[..., MetricComputations]\n</code></pre> <p>Wraps create_computations_fn to be called separately for each key.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_util.py</code> <pre><code>def merge_per_key_computations(\n    create_computations_fn: Callable[..., metric_types.MetricComputations],\n) -&gt; Callable[..., metric_types.MetricComputations]:\n    \"\"\"Wraps create_computations_fn to be called separately for each key.\"\"\"\n\n    def merge_computations_fn(\n        eval_config: Optional[config_pb2.EvalConfig] = None,\n        schema: Optional[schema_pb2.Schema] = None,\n        model_names: Optional[List[str]] = None,\n        output_names: Optional[List[str]] = None,\n        sub_keys: Optional[List[Optional[metric_types.SubKey]]] = None,\n        aggregation_type: Optional[metric_types.AggregationType] = None,\n        class_weights: Optional[Dict[int, float]] = None,\n        example_weighted: bool = False,\n        query_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; metric_types.MetricComputations:\n        \"\"\"Merge computations function.\"\"\"\n        if model_names is None:\n            model_names = [\"\"]\n        if output_names is None:\n            output_names = [\"\"]\n        if sub_keys is None:\n            sub_keys = [None]\n        computations = []\n        for model_name in model_names:\n            for output_name in output_names:\n                for sub_key in sub_keys:\n                    if hasattr(inspect, \"getfullargspec\"):\n                        args = inspect.getfullargspec(create_computations_fn).args\n                    else:\n                        args = inspect.getargspec(create_computations_fn).args  # pylint: disable=deprecated-method\n                    updated_kwargs = (\n                        metric_types.validate_and_update_create_computations_fn_kwargs(\n                            args,\n                            kwargs.copy(),\n                            eval_config,\n                            schema,\n                            model_names,\n                            output_names,\n                            sub_keys,\n                            aggregation_type,\n                            class_weights,\n                            example_weighted,\n                            query_key,\n                        )\n                    )\n                    if \"model_name\" in args:\n                        updated_kwargs[\"model_name\"] = model_name\n                    if \"output_name\" in args:\n                        updated_kwargs[\"output_name\"] = output_name\n                    if \"sub_key\" in args:\n                        updated_kwargs[\"sub_key\"] = sub_key\n                    computations.extend(create_computations_fn(**updated_kwargs))\n        return computations\n\n    return merge_computations_fn\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.metric_thresholds_from_metrics_specs","title":"metric_thresholds_from_metrics_specs","text":"<pre><code>metric_thresholds_from_metrics_specs(\n    metrics_specs: Iterable[MetricsSpec],\n    eval_config: Optional[EvalConfig] = None,\n) -&gt; Dict[MetricKey, Iterable[_SliceAndThreshold]]\n</code></pre> <p>Returns thresholds associated with given metrics specs.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_specs.py</code> <pre><code>def metric_thresholds_from_metrics_specs(\n    metrics_specs: Iterable[config_pb2.MetricsSpec],\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n) -&gt; Dict[metric_types.MetricKey, Iterable[_SliceAndThreshold]]:\n    \"\"\"Returns thresholds associated with given metrics specs.\"\"\"\n    if eval_config is None:\n        eval_config = config_pb2.EvalConfig()\n    result = collections.defaultdict(list)\n    existing = collections.defaultdict(dict)\n\n    def add_if_not_exists(\n        key: metric_types.MetricKey,\n        slice_spec: Optional[\n            Union[config_pb2.SlicingSpec, config_pb2.CrossSlicingSpec]\n        ],\n        threshold: Union[\n            config_pb2.GenericChangeThreshold, config_pb2.GenericValueThreshold\n        ],\n    ):\n        \"\"\"Adds value to results if it doesn't already exist.\"\"\"\n        hashable_slice_spec = None\n        if slice_spec:\n            hashable_slice_spec = slicer.deserialize_slice_spec(slice_spec)\n        # Note that hashing by SerializeToString() is only safe if used within the\n        # same process.\n        threshold_hash = threshold.SerializeToString()\n        if not (\n            key in existing\n            and hashable_slice_spec in existing[key]\n            and threshold_hash in existing[key][hashable_slice_spec]\n        ):\n            if hashable_slice_spec not in existing[key]:\n                existing[key][hashable_slice_spec] = {}\n            existing[key][hashable_slice_spec][threshold_hash] = True\n            result[key].append((slice_spec, threshold))\n\n    def add_threshold(\n        key: metric_types.MetricKey,\n        slice_spec: Union[\n            Optional[config_pb2.SlicingSpec], Optional[config_pb2.CrossSlicingSpec]\n        ],\n        threshold: config_pb2.MetricThreshold,\n    ):\n        \"\"\"Adds thresholds to results.\"\"\"\n        if threshold.HasField(\"value_threshold\"):\n            add_if_not_exists(key, slice_spec, threshold.value_threshold)\n        if threshold.HasField(\"change_threshold\"):\n            key = key.make_diff_key()\n            add_if_not_exists(key, slice_spec, threshold.change_threshold)\n\n    for spec in metrics_specs:\n        for aggregation_type, sub_keys in _create_sub_keys(spec).items():\n            # Add thresholds for metrics computed in-graph.\n            for metric_name, threshold in spec.thresholds.items():\n                for key in _keys_for_metric(\n                    metric_name, spec, aggregation_type, sub_keys, [None]\n                ):\n                    add_threshold(key, None, threshold)\n            for metric_name, per_slice_thresholds in spec.per_slice_thresholds.items():\n                for key in _keys_for_metric(\n                    metric_name, spec, aggregation_type, sub_keys, [None]\n                ):\n                    for per_slice_threshold in per_slice_thresholds.thresholds:\n                        for slice_spec in per_slice_threshold.slicing_specs:\n                            add_threshold(\n                                key, slice_spec, per_slice_threshold.threshold\n                            )\n            for (\n                metric_name,\n                cross_slice_thresholds,\n            ) in spec.cross_slice_thresholds.items():\n                for key in _keys_for_metric(\n                    metric_name, spec, aggregation_type, sub_keys, [None]\n                ):\n                    for cross_slice_threshold in cross_slice_thresholds.thresholds:\n                        for (\n                            cross_slice_spec\n                        ) in cross_slice_threshold.cross_slicing_specs:\n                            add_threshold(\n                                key, cross_slice_spec, cross_slice_threshold.threshold\n                            )\n\n    # Add thresholds for post export metrics defined in MetricConfigs.\n    for key, metric_config, _ in keys_and_metrics_from_specs(\n        eval_config, metrics_specs\n    ):\n        if metric_config.HasField(\"threshold\"):\n            add_threshold(key, None, metric_config.threshold)\n        for per_slice_threshold in metric_config.per_slice_thresholds:\n            for slice_spec in per_slice_threshold.slicing_specs:\n                add_threshold(key, slice_spec, per_slice_threshold.threshold)\n        for cross_slice_threshold in metric_config.cross_slice_thresholds:\n            for cross_slice_spec in cross_slice_threshold.cross_slicing_specs:\n                add_threshold(key, cross_slice_spec, cross_slice_threshold.threshold)\n\n    return result\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.specs_from_metrics","title":"specs_from_metrics","text":"<pre><code>specs_from_metrics(\n    metrics: Optional[_MetricsOrLosses] = None,\n    unweighted_metrics: Optional[_MetricsOrLosses] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    binarize: Optional[BinarizationOptions] = None,\n    aggregate: Optional[AggregationOptions] = None,\n    query_key: Optional[str] = None,\n    include_example_count: Optional[bool] = None,\n    include_weighted_example_count: Optional[bool] = None,\n) -&gt; List[MetricsSpec]\n</code></pre> <p>Returns specs for tf_keras.metrics/losses or tfma.metrics classes.</p> <p>Examples:</p> <p>metrics_specs = specs_from_metrics(     [         tf_keras.metrics.BinaryAccuracy(),         tfma.metrics.AUC(),         tfma.metrics.MeanLabel(),         tfma.metrics.MeanPrediction()         ...     ],     unweighted=[         tfma.metrics.Precision(),         tfma.metrics.Recall()     ])</p> <p>metrics_specs = specs_from_metrics({     'output1': [         tf_keras.metrics.BinaryAccuracy(),         tfma.metrics.AUC(),         tfma.metrics.MeanLabel(),         tfma.metrics.MeanPrediction()         ...     ],     'output2': [         tfma.metrics.Precision(),         tfma.metrics.Recall(),     ]   })</p> <p>metrics: List of tfma.metrics.Metric, tf_keras.metrics.Metric, or     tf_keras.losses.Loss. For multi-output models a dict of dicts may be     passed where the first dict is indexed by the output_name. Whether these     metrics are weighted or not will be determined based on whether the     ModelSpec associated with the metrics contains example weight key settings     or not.   unweighted_metrics: Same as metrics only these metrics will not be weighted     by example_weight regardless of the example weight key settings.   model_names: Optional model names (if multi-model evaluation).   output_names: Optional output names (if multi-output models). If the metrics     are a dict this should not be set.   output_weights: Optional output weights for creating overall metric     aggregated across outputs (if multi-output model). If a weight is not     provided for an output, it's weight defaults to 0.0 (i.e. output ignored).   binarize: Optional settings for binarizing multi-class/multi-label metrics.   aggregate: Optional settings for aggregating multi-class/multi-label     metrics.   query_key: Optional query key for query/ranking based metrics.   include_example_count: True to add example_count metric. Default is True.   include_weighted_example_count: True to add weighted example_count metric.     Default is True. A weighted example count will be added per output for     multi-output models.</p> <p>MetricsSpecs based on options provided. A separate spec is returned for   weighted vs unweighted metrics. A separate spec is also returned for each   output if a dict of metrics per output is passed.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_specs.py</code> <pre><code>def specs_from_metrics(\n    metrics: Optional[_MetricsOrLosses] = None,\n    unweighted_metrics: Optional[_MetricsOrLosses] = None,\n    model_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    output_weights: Optional[Dict[str, float]] = None,\n    binarize: Optional[config_pb2.BinarizationOptions] = None,\n    aggregate: Optional[config_pb2.AggregationOptions] = None,\n    query_key: Optional[str] = None,\n    include_example_count: Optional[bool] = None,\n    include_weighted_example_count: Optional[bool] = None,\n) -&gt; List[config_pb2.MetricsSpec]:\n    \"\"\"Returns specs for tf_keras.metrics/losses or tfma.metrics classes.\n\n    Examples:\n    --------\n      metrics_specs = specs_from_metrics(\n        [\n            tf_keras.metrics.BinaryAccuracy(),\n            tfma.metrics.AUC(),\n            tfma.metrics.MeanLabel(),\n            tfma.metrics.MeanPrediction()\n            ...\n        ],\n        unweighted=[\n            tfma.metrics.Precision(),\n            tfma.metrics.Recall()\n        ])\n\n      metrics_specs = specs_from_metrics({\n        'output1': [\n            tf_keras.metrics.BinaryAccuracy(),\n            tfma.metrics.AUC(),\n            tfma.metrics.MeanLabel(),\n            tfma.metrics.MeanPrediction()\n            ...\n        ],\n        'output2': [\n            tfma.metrics.Precision(),\n            tfma.metrics.Recall(),\n        ]\n      })\n\n    Args:\n    ----\n      metrics: List of tfma.metrics.Metric, tf_keras.metrics.Metric, or\n        tf_keras.losses.Loss. For multi-output models a dict of dicts may be\n        passed where the first dict is indexed by the output_name. Whether these\n        metrics are weighted or not will be determined based on whether the\n        ModelSpec associated with the metrics contains example weight key settings\n        or not.\n      unweighted_metrics: Same as metrics only these metrics will not be weighted\n        by example_weight regardless of the example weight key settings.\n      model_names: Optional model names (if multi-model evaluation).\n      output_names: Optional output names (if multi-output models). If the metrics\n        are a dict this should not be set.\n      output_weights: Optional output weights for creating overall metric\n        aggregated across outputs (if multi-output model). If a weight is not\n        provided for an output, it's weight defaults to 0.0 (i.e. output ignored).\n      binarize: Optional settings for binarizing multi-class/multi-label metrics.\n      aggregate: Optional settings for aggregating multi-class/multi-label\n        metrics.\n      query_key: Optional query key for query/ranking based metrics.\n      include_example_count: True to add example_count metric. Default is True.\n      include_weighted_example_count: True to add weighted example_count metric.\n        Default is True. A weighted example count will be added per output for\n        multi-output models.\n\n    Returns:\n    -------\n      MetricsSpecs based on options provided. A separate spec is returned for\n      weighted vs unweighted metrics. A separate spec is also returned for each\n      output if a dict of metrics per output is passed.\n    \"\"\"\n    if isinstance(metrics, dict) and output_names:\n        raise ValueError(\n            \"metrics cannot be a dict when output_names is used: \"\n            f\"metrics={metrics}, output_names={output_names}\"\n        )\n    if (\n        metrics\n        and unweighted_metrics\n        and isinstance(metrics, dict) != isinstance(unweighted_metrics, dict)\n    ):\n        raise ValueError(\n            \"metrics and unweighted_metrics must both be either dicts or lists: \"\n            f\"metrics={metrics}, unweighted_metrics={unweighted_metrics}\"\n        )\n\n    if isinstance(metrics, dict) or isinstance(unweighted_metrics, dict):\n        metrics_dict = metrics if isinstance(metrics, dict) else {}\n        unweighted_metrics_dict = (\n            unweighted_metrics if isinstance(unweighted_metrics, dict) else {}\n        )\n        specs = []\n        output_names = set(metrics_dict) | set(unweighted_metrics_dict)\n        for output_name in sorted(output_names):\n            specs.extend(\n                specs_from_metrics(\n                    metrics_dict.get(output_name),\n                    unweighted_metrics=unweighted_metrics_dict.get(output_name),\n                    model_names=model_names,\n                    output_names=[output_name],\n                    binarize=binarize,\n                    aggregate=aggregate,\n                    include_example_count=include_example_count,\n                    include_weighted_example_count=include_weighted_example_count,\n                )\n            )\n            include_example_count = False\n        return specs\n\n    if include_example_count is None:\n        include_example_count = True\n    if include_weighted_example_count is None:\n        include_weighted_example_count = True\n\n    # Add the computations for the example counts and weights since they are\n    # independent of the model and class ID.\n    specs = example_count_specs(\n        model_names=model_names,\n        output_names=output_names,\n        output_weights=output_weights,\n        include_example_count=include_example_count,\n        include_weighted_example_count=include_weighted_example_count,\n    )\n\n    if metrics:\n        specs.append(\n            config_pb2.MetricsSpec(\n                metrics=[config_from_metric(metric) for metric in metrics],\n                model_names=model_names,\n                output_names=output_names,\n                output_weights=output_weights,\n                binarize=binarize,\n                aggregate=aggregate,\n                example_weights=None,\n                query_key=query_key,\n            )\n        )\n    if unweighted_metrics:\n        specs.append(\n            config_pb2.MetricsSpec(\n                metrics=[config_from_metric(metric) for metric in unweighted_metrics],\n                model_names=model_names,\n                output_names=output_names,\n                output_weights=output_weights,\n                binarize=binarize,\n                aggregate=aggregate,\n                example_weights=config_pb2.ExampleWeightOptions(unweighted=True),\n                query_key=query_key,\n            )\n        )\n\n    return specs\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.to_label_prediction_example_weight","title":"to_label_prediction_example_weight","text":"<pre><code>to_label_prediction_example_weight(\n    inputs: StandardMetricInputs,\n    eval_config: Optional[EvalConfig] = None,\n    model_name: str = \"\",\n    output_name: str = \"\",\n    sub_key: Optional[SubKey] = None,\n    aggregation_type: Optional[AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    fractional_labels: bool = False,\n    flatten: bool = True,\n    squeeze: bool = True,\n    allow_none: bool = False,\n    require_single_example_weight: bool = False,\n) -&gt; Iterator[Tuple[ndarray, ndarray, ndarray]]\n</code></pre> <p>Yields label, prediction, and example weights for use in calculations.</p> <p>Where applicable this function will perform model and output name lookups as well as any required class ID, top K, etc conversions. It will also apply prediction keys and label vocabularies given the necessary information is provided as part of the EvalConfig (or standard estimator based naming is used). The sparseness of labels will be inferred from the shapes of the labels and predictions (i.e. if the shapes are different then the labels will be assumed to be sparse).</p> <p>If successful, the final output of calling this function will be a tuple of numpy arrays representing the label, prediction, and example weight respectively. Labels and predictions will be returned in the same shape provided (default behavior) unless (1) flatten is True in which case a series of values (one per class ID) will be returned with last dimension of size 1 or (2) a sub_key is used in which case the last dimension may be re-shaped to match the new number of outputs (1 for class_id or k, top_k for top k with aggregation).</p> <p>Note that for top_k without aggregation, the non-top_k prediction values will be set to float('-inf'), but for top_k with aggregation the values will be truncated to only return the top k values.</p> <p>Examples:</p> <p># default behavior   #   # Binary classification   Input  : labels=[1] predictions=[0.6]   Output : (np.array([1]), np.array([0.6]), np.array([1.0]))   # Multi-class classification w/ sparse labels   Input : labels=[2] predictions=[0.3, 0.6, 0.1]   Output: (np.array([2]), np.array([0.3, 0.6, 0.1]), np.array([1.0]))   # Multi-class / multi-label classification w/ dense labels   Input  : labels=[0, 1, 1] predictions=[0.3, 0.6, 0.1]   Output : (np.array([0, 1, 1]), np.array([0.3, 0.6, 0.1]), np.array([1.0]))</p> <p># flatten=True   #   # Multi-class classification w/ sparse labels   Input  : labels=[2], predictions=[0.3, 0.6, 0.1]   Output : (np.array([0]), np.array([0.3]), np.array([1.0])),            (np.array([0]), np.array([0.6]), np.array([1.0])),            (np.array([1]), np.array([0.1]), np.array([1.0]))   # Multi-class/multi-label classification w/ dense labels   Input  : labels=[0, 0, 1], predictions=[0.3, 0.6, 0.1]   Output : (np.array([0]), np.array([0.3]), np.array([1.0])),            (np.array([0]), np.array([0.6]), np.array([1.0])),            (np.array([1]), np.array([0.1]), np.array([1.0]))</p> <p># sub_key.class_id=[2]   #   # Multi-class classification w/ sparse labels   Input  : labels=[2] predictions=[0.3, 0.6, 0.1]   Output : (np.array([1]), np.array([0.1]), np.array([1.0]))   # Multi-class classification w/ dense labels   Input  : labels=[0, 0, 1] predictions=[0.3, 0.6, 0.1]   Output : (np.array([1]), np.array([0.1]), np.array([1.0]))</p> <p># sub_key.top_k=2 and aggregation_type is None (i.e. binarization of top 2).   #   # Multi-class classification w/ sparse labels   Input  : labels=[2] predictions=[0.3, 0.6, 0.1]   Output : (np.array([0, 0, 1]), np.array([0.3, 0.6, -inf]), np.array([1.0]))   # Multi-class classification w/ dense labels   Input  : labels=[0, 0, 1] predictions=[0.3, 0.1, 0.6]   Output : (np.array([0, 0, 1]), np.array([0.3, -inf, 0.6]), np.array([1.0]))</p> <p># sub_key.top_k=2 and aggregation_type is not None (i.e. aggregate top 2).   #   # Multi-class classification w/ sparse labels   Input  : labels=[2] predictions=[0.3, 0.6, 0.1]   Output : (np.array([0, 1]), np.array([0.3, 0.6]), np.array([1.0]))   # Multi-class classification w/ dense labels   Input  : labels=[0, 0, 1] predictions=[0.3, 0.1, 0.6]   Output : (np.array([0, 0]), np.array([0.3, 0.6]), np.array([1.0]))</p> <p># sub_key.k=2 (i.e. binarization by choosing 2nd largest predicted value).   #   # Multi-class classification w/ sparse labels   Input  : labels=[0] predictions=[0.3, 0.6, 0.1]   Output : (np.array([1]), np.array([0.3]), np.array([1.0]))   # Multi-class classification w/ dense labels   Input  : labels=[0] predictions=[0.3]   Output : (np.array([0]), np.array([0.3]), np.array([1.0]))</p> <p>inputs: Standard metric inputs.   eval_config: Eval config   model_name: Optional model name (if multi-model evaluation).   output_name: Optional output name (if multi-output model type).   sub_key: Optional sub key.   aggregation_type: Optional aggregation type.   class_weights: Optional class weights to apply to multi-class / multi-label     labels and predictions. If used, flatten must also be True.   example_weighted: True if example weights should be applied.   fractional_labels: If true, each incoming tuple of (label, prediction, and     example weight) will be split into two tuples as follows (where l, p, w     represent the resulting label, prediction, and example weight values): (1)     l = 0.0, p = prediction, and w = example_weight * (1.0 - label) (2) l =     1.0, p = prediction, and w = example_weight * label If enabled, an     exception will be raised if labels are not within [0, 1]. The     implementation is such that tuples associated with a weight of zero are     not yielded. This means it is safe to enable fractional_labels even when     the labels only take on the values of 0.0 or 1.0.   flatten: True to flatten the final label and prediction outputs so that the     yielded values are always arrays of size 1. For example, multi-class /     multi-label outputs would be converted into label and prediction pairs     that could then be processed by a binary classification metric in order to     compute a micro average over all classes. If the example weight is not a     scalar, then they will be flattened as well, otherwise the same example     weight value will be output for each pair of labels and predictions.   squeeze: True to squeeze any outputs that have rank &gt; 1. This transforms     outputs such as np.array([[1]]) to np.array([1]).   allow_none: True to allow labels or predictions with None values to be     returned. When used, the values will be returned as empty np.ndarrays. The     example weight will always be non-empty.   require_single_example_weight: True to require that the example_weight be a     single value.</p> <p>Tuple of (label, prediction, example_weight).</p> Source code in <code>tensorflow_model_analysis/metrics/metric_util.py</code> <pre><code>def to_label_prediction_example_weight(\n    inputs: metric_types.StandardMetricInputs,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    model_name: str = \"\",\n    output_name: str = \"\",\n    sub_key: Optional[metric_types.SubKey] = None,\n    aggregation_type: Optional[metric_types.AggregationType] = None,\n    class_weights: Optional[Dict[int, float]] = None,\n    example_weighted: bool = False,\n    fractional_labels: bool = False,\n    flatten: bool = True,\n    squeeze: bool = True,\n    allow_none: bool = False,\n    require_single_example_weight: bool = False,\n) -&gt; Iterator[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"Yields label, prediction, and example weights for use in calculations.\n\n    Where applicable this function will perform model and output name lookups as\n    well as any required class ID, top K, etc conversions. It will also apply\n    prediction keys and label vocabularies given the necessary information is\n    provided as part of the EvalConfig (or standard estimator based naming is\n    used). The sparseness of labels will be inferred from the shapes of the labels\n    and predictions (i.e. if the shapes are different then the labels will be\n    assumed to be sparse).\n\n    If successful, the final output of calling this function will be a tuple of\n    numpy arrays representing the label, prediction, and example weight\n    respectively. Labels and predictions will be returned in the same shape\n    provided (default behavior) unless (1) flatten is True in which case a series\n    of values (one per class ID) will be returned with last dimension of size 1 or\n    (2) a sub_key is used in which case the last dimension may be re-shaped to\n    match the new number of outputs (1 for class_id or k, top_k for top k with\n    aggregation).\n\n    Note that for top_k without aggregation, the non-top_k prediction values will\n    be set to float('-inf'), but for top_k with aggregation the values will be\n    truncated to only return the top k values.\n\n    Examples:\n    --------\n      # default behavior\n      #\n      # Binary classification\n      Input  : labels=[1] predictions=[0.6]\n      Output : (np.array([1]), np.array([0.6]), np.array([1.0]))\n      # Multi-class classification w/ sparse labels\n      Input : labels=[2] predictions=[0.3, 0.6, 0.1]\n      Output: (np.array([2]), np.array([0.3, 0.6, 0.1]), np.array([1.0]))\n      # Multi-class / multi-label classification w/ dense labels\n      Input  : labels=[0, 1, 1] predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([0, 1, 1]), np.array([0.3, 0.6, 0.1]), np.array([1.0]))\n\n      # flatten=True\n      #\n      # Multi-class classification w/ sparse labels\n      Input  : labels=[2], predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([0]), np.array([0.3]), np.array([1.0])),\n               (np.array([0]), np.array([0.6]), np.array([1.0])),\n               (np.array([1]), np.array([0.1]), np.array([1.0]))\n      # Multi-class/multi-label classification w/ dense labels\n      Input  : labels=[0, 0, 1], predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([0]), np.array([0.3]), np.array([1.0])),\n               (np.array([0]), np.array([0.6]), np.array([1.0])),\n               (np.array([1]), np.array([0.1]), np.array([1.0]))\n\n      # sub_key.class_id=[2]\n      #\n      # Multi-class classification w/ sparse labels\n      Input  : labels=[2] predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([1]), np.array([0.1]), np.array([1.0]))\n      # Multi-class classification w/ dense labels\n      Input  : labels=[0, 0, 1] predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([1]), np.array([0.1]), np.array([1.0]))\n\n      # sub_key.top_k=2 and aggregation_type is None (i.e. binarization of top 2).\n      #\n      # Multi-class classification w/ sparse labels\n      Input  : labels=[2] predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([0, 0, 1]), np.array([0.3, 0.6, -inf]), np.array([1.0]))\n      # Multi-class classification w/ dense labels\n      Input  : labels=[0, 0, 1] predictions=[0.3, 0.1, 0.6]\n      Output : (np.array([0, 0, 1]), np.array([0.3, -inf, 0.6]), np.array([1.0]))\n\n      # sub_key.top_k=2 and aggregation_type is not None (i.e. aggregate top 2).\n      #\n      # Multi-class classification w/ sparse labels\n      Input  : labels=[2] predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([0, 1]), np.array([0.3, 0.6]), np.array([1.0]))\n      # Multi-class classification w/ dense labels\n      Input  : labels=[0, 0, 1] predictions=[0.3, 0.1, 0.6]\n      Output : (np.array([0, 0]), np.array([0.3, 0.6]), np.array([1.0]))\n\n      # sub_key.k=2 (i.e. binarization by choosing 2nd largest predicted value).\n      #\n      # Multi-class classification w/ sparse labels\n      Input  : labels=[0] predictions=[0.3, 0.6, 0.1]\n      Output : (np.array([1]), np.array([0.3]), np.array([1.0]))\n      # Multi-class classification w/ dense labels\n      Input  : labels=[0] predictions=[0.3]\n      Output : (np.array([0]), np.array([0.3]), np.array([1.0]))\n\n    Args:\n    ----\n      inputs: Standard metric inputs.\n      eval_config: Eval config\n      model_name: Optional model name (if multi-model evaluation).\n      output_name: Optional output name (if multi-output model type).\n      sub_key: Optional sub key.\n      aggregation_type: Optional aggregation type.\n      class_weights: Optional class weights to apply to multi-class / multi-label\n        labels and predictions. If used, flatten must also be True.\n      example_weighted: True if example weights should be applied.\n      fractional_labels: If true, each incoming tuple of (label, prediction, and\n        example weight) will be split into two tuples as follows (where l, p, w\n        represent the resulting label, prediction, and example weight values): (1)\n        l = 0.0, p = prediction, and w = example_weight * (1.0 - label) (2) l =\n        1.0, p = prediction, and w = example_weight * label If enabled, an\n        exception will be raised if labels are not within [0, 1]. The\n        implementation is such that tuples associated with a weight of zero are\n        not yielded. This means it is safe to enable fractional_labels even when\n        the labels only take on the values of 0.0 or 1.0.\n      flatten: True to flatten the final label and prediction outputs so that the\n        yielded values are always arrays of size 1. For example, multi-class /\n        multi-label outputs would be converted into label and prediction pairs\n        that could then be processed by a binary classification metric in order to\n        compute a micro average over all classes. If the example weight is not a\n        scalar, then they will be flattened as well, otherwise the same example\n        weight value will be output for each pair of labels and predictions.\n      squeeze: True to squeeze any outputs that have rank &gt; 1. This transforms\n        outputs such as np.array([[1]]) to np.array([1]).\n      allow_none: True to allow labels or predictions with None values to be\n        returned. When used, the values will be returned as empty np.ndarrays. The\n        example weight will always be non-empty.\n      require_single_example_weight: True to require that the example_weight be a\n        single value.\n\n    Yields:\n    ------\n      Tuple of (label, prediction, example_weight).\n    \"\"\"\n\n    def fn_call_str():\n        return (\n            f\"to_label_prediction_example_weight(inputs={inputs}, \"\n            f\"eval_config={eval_config}, model_name={model_name}, \"\n            f\"output_name={output_name}, sub_key={sub_key}, \"\n            f\"aggregation_type={aggregation_type}, \"\n            f\"class_weights={class_weights}, \"\n            f\"fractional_labels={fractional_labels}, flatten={flatten}, \"\n            f\"squeeze={squeeze}, allow_none={allow_none})\"\n        )\n\n    def optionally_get_by_keys(value: Any, keys: List[str]) -&gt; Any:\n        class NotFound:\n            pass\n\n        if isinstance(value, Mapping):\n            new_value = util.get_by_keys(value, keys, default_value=NotFound())\n            if not isinstance(new_value, NotFound):\n                # Might be None if that's what is in the dict\n                return new_value\n        return value\n\n    try:\n        prediction_key = \"\"\n        label_key = \"\"\n        if eval_config and eval_config.model_specs:\n            for spec in eval_config.model_specs:\n                # To maintain consistency between settings where single models are used,\n                # always use '' as the model name regardless of whether a name is passed\n                spec_name = spec.name if len(eval_config.model_specs) &gt; 1 else \"\"\n                if spec_name == model_name:\n                    prediction_key = spec.prediction_key\n                    label_key = spec.label_key\n                    break\n\n        label = inputs.label\n        if label_key:\n            # This is to support a custom EvalSavedModel where the labels are a dict\n            # but the keys are not output_names.\n            label = optionally_get_by_keys(label, [label_key])\n        prediction = inputs.prediction\n        example_weight = inputs.example_weight\n        if model_name:\n            if prediction is not None:\n                prediction = util.get_by_keys(prediction, [model_name])\n            # Labels and weights can optionally be keyed by model name.\n            label = optionally_get_by_keys(label, [model_name])\n            example_weight = optionally_get_by_keys(example_weight, [model_name])\n        if output_name:\n            if prediction is not None:\n                prediction = util.get_by_keys(prediction, [output_name])\n            # Labels and example weights can optionally be keyed by output name.\n            label = optionally_get_by_keys(label, [output_name])\n            example_weight = optionally_get_by_keys(example_weight, [output_name])\n\n        if not example_weighted or example_weight is None:\n            example_weight = np.array(1.0, dtype=np.float32)  # tf-ranking needs float32\n\n        if isinstance(label, Mapping):\n            raise ValueError(\n                \"unable to prepare label for metric computation because the label is \"\n                \"a dict with unrecognized keys. If a multi-output model was used \"\n                f\"check that an output name was provided in all the relevant \"\n                \"settings (ModelSpec.label_keys, MetricsSpec.output_names, etc): \"\n                f\"label={label}, output_name={output_name}\"\n            )\n        if isinstance(example_weight, Mapping):\n            raise ValueError(\n                \"unable to prepare example_weight for metric computation because the \"\n                \"example_weight is a dict with unrecognized keys. If a multi-output \"\n                \"model was used check that an output name was provided in all the \"\n                \"relevant settings (ModelSpec.example_weight_keys, \"\n                f\"MetricsSpec.output_names, etc): example_weight={example_weight}, \"\n                f\"output_name={output_name}\"\n            )\n\n        label, prediction = prepare_labels_and_predictions(\n            label, prediction, prediction_key\n        )\n\n        if not allow_none:\n            for txt, value in zip((\"label\", \"prediction\"), (label, prediction)):\n                if value is None:\n                    raise ValueError(\n                        f\"no value provided for {txt}\\n\\n\"\n                        \"This may be caused by a configuration error (i.e. label, \"\n                        \"and/or prediction keys were not specified) or an \"\n                        \"error in the pipeline.\"\n                    )\n\n        example_weight = util.to_numpy(example_weight)\n        if require_single_example_weight and example_weight.size &gt; 1:\n            example_weight = example_weight.flatten()\n            if not np.all(example_weight == example_weight[0]):\n                raise ValueError(\n                    \"if example_weight size &gt; 0, the values must all be the same: \"\n                    f\"example_weight={example_weight}\\n\\n\"\n                    \"This is most likely a configuration error.\"\n                )\n            example_weight = np.array(example_weight[0])\n\n        if sub_key is not None and label is not None and prediction is not None:\n            if sub_key.k is not None:\n                indices = top_k_indices(sub_key.k, prediction)\n                if len(prediction.shape) == 1:\n                    indices = indices[0]  # 1D\n                else:\n                    # 2D, take kth values\n                    indices = (indices[0][0 :: sub_key.k], indices[1][0 :: sub_key.k])\n                if label.shape != prediction.shape:\n                    label = one_hot(label, prediction)\n                label = select_indices(label, indices)\n                prediction = select_indices(prediction, indices)\n            else:\n                if sub_key.top_k is not None:\n                    # Set all non-top-k predictions to -inf. Note that we do not sort.\n                    indices = top_k_indices(sub_key.top_k, prediction)\n                    if aggregation_type is None:\n                        top_k_predictions = np.full(prediction.shape, float(\"-inf\"))\n                        top_k_predictions[indices] = prediction[indices]\n                        prediction = top_k_predictions\n                    else:\n                        if label.shape != prediction.shape:\n                            label = one_hot(label, prediction)\n                        label = select_indices(label, indices)\n                        prediction = select_indices(prediction, indices)\n                if sub_key.class_id is not None:\n                    label, prediction = select_class_id(\n                        sub_key.class_id, label, prediction\n                    )\n\n        # For consistency, make sure all outputs are arrays (i.e. convert scalars)\n        if label is not None and not label.shape:\n            label = label.reshape((1,))\n        if prediction is not None and not prediction.shape:\n            prediction = prediction.reshape((1,))\n        if not example_weight.shape:\n            example_weight = example_weight.reshape((1,))\n\n        label = label if label is not None else np.array([])\n        prediction = prediction if prediction is not None else np.array([])\n\n        flatten_size = prediction.size or label.size\n        if flatten:\n            if example_weight.size == 1:\n                example_weight = np.array(\n                    [float(example_weight) for i in range(flatten_size)]\n                )\n            elif example_weight.size != flatten_size:\n                raise ValueError(\n                    \"example_weight size does not match the size of labels and \"\n                    f\"predictions: label={label}, prediction={prediction}, example_weight={example_weight}\"\n                )\n\n        if class_weights:\n            if not flatten:\n                raise ValueError(\n                    \"class_weights can only be used when flatten is also used: \"\n                    f\"class_weights={class_weights}, flatten={flatten}\\n\\n\"\n                    \"This is likely caused by a configuration error (i.e. micro \"\n                    \"averaging being applied to metrics that don't support micro \"\n                    \"averaging\"\n                )\n            example_weight = np.array(\n                [\n                    example_weight[i] * class_weights[i] if i in class_weights else 0.0\n                    for i in range(flatten_size)\n                ]\n            )\n\n        def yield_results(label, prediction, example_weight):\n            if (\n                not flatten\n                or (label.size == 0 and prediction.size == 0)\n                or (\n                    label.size == 1\n                    and prediction.size == 1\n                    and example_weight.size == 1\n                )\n            ):\n                if squeeze:\n                    yield (\n                        _squeeze(label),\n                        _squeeze(prediction),\n                        _squeeze(example_weight),\n                    )\n                else:\n                    yield label, prediction, example_weight\n            elif label.size == 0:\n                for p, w in zip(prediction.flatten(), example_weight.flatten()):\n                    yield label, np.array([p]), np.array([w])\n            elif prediction.size == 0:\n                for l, w in zip(label.flatten(), example_weight.flatten()):\n                    yield np.array([l]), prediction, np.array([w])\n            elif label.size == prediction.size and label.size == example_weight.size:\n                for l, p, w in zip(\n                    label.flatten(), prediction.flatten(), example_weight.flatten()\n                ):\n                    yield np.array([l]), np.array([p]), np.array([w])\n            elif label.shape[-1] == 1 and prediction.size == example_weight.size:\n                label = one_hot(label, prediction)\n                for l, p, w in zip(\n                    label.flatten(), prediction.flatten(), example_weight.flatten()\n                ):\n                    yield np.array([l]), np.array([p]), np.array([w])\n            else:\n                raise ValueError(\n                    \"unable to pair labels, predictions, and example weights: \"\n                    f\"label={label}, prediction={prediction}, \"\n                    f\"example_weight={example_weight}\\n\\n\"\n                    \"This is most likely a configuration error.\"\n                )\n\n        for result in yield_results(label, prediction, example_weight):\n            if fractional_labels and label.size:\n                for new_result in _yield_fractional_labels(*result):\n                    yield new_result\n            else:\n                yield result\n    except Exception as e:\n        import sys  # pylint: disable=g-import-not-at-top\n\n        raise type(e)(str(e) + f\"\\n\\n{fn_call_str()}\").with_traceback(sys.exc_info()[2])\n</code></pre>"},{"location":"api_docs/python/tfma-metrics/#tensorflow_model_analysis.metrics.to_standard_metric_inputs","title":"to_standard_metric_inputs","text":"<pre><code>to_standard_metric_inputs(\n    extracts: Extracts,\n    include_labels: bool = True,\n    include_predictions: bool = True,\n    include_features: bool = False,\n    include_transformed_features: bool = False,\n    include_any_feature: bool = False,\n    include_attributions: bool = False,\n) -&gt; StandardMetricInputs\n</code></pre> <p>Verifies extract keys and converts extracts to StandardMetricInputs.</p> Source code in <code>tensorflow_model_analysis/metrics/metric_util.py</code> <pre><code>def to_standard_metric_inputs(\n    extracts: types.Extracts,\n    include_labels: bool = True,\n    include_predictions: bool = True,\n    include_features: bool = False,\n    include_transformed_features: bool = False,\n    include_any_feature: bool = False,\n    include_attributions: bool = False,\n) -&gt; metric_types.StandardMetricInputs:\n    \"\"\"Verifies extract keys and converts extracts to StandardMetricInputs.\"\"\"\n    if include_labels and constants.LABELS_KEY not in extracts:\n        raise ValueError(\n            f'\"{constants.LABELS_KEY}\" key not found in extracts. '\n            \"Check that the configuration is setup properly to \"\n            \"specify the name of label input and that the proper \"\n            \"extractor has been configured to extract the labels from \"\n            f\"the inputs. Existing keys: {extracts.keys()}\"\n        )\n    if include_predictions and constants.PREDICTIONS_KEY not in extracts:\n        raise ValueError(\n            f'\"{constants.PREDICTIONS_KEY}\" key not found in '\n            \"extracts. Check that the proper extractor has been \"\n            \"configured to perform model inference.\"\n        )\n    if include_features and constants.FEATURES_KEY not in extracts:\n        raise ValueError(\n            f'\"{constants.FEATURES_KEY}\" key not found in extracts. '\n            \"Check that the proper extractor has been configured to \"\n            \"extract the features from the inputs. Existing keys: \"\n            f\"{extracts.keys()}\"\n        )\n    if (\n        include_transformed_features\n        and constants.TRANSFORMED_FEATURES_KEY not in extracts\n    ):\n        raise ValueError(\n            f'\"{constants.TRANSFORMED_FEATURES_KEY}\" key not found in '\n            \"extracts. Check that the proper extractor has been \"\n            \"configured to extract the transformed features from the \"\n            f\"inputs. Existing keys: {extracts.keys()}\"\n        )\n    if (\n        include_any_feature\n        and constants.FEATURES_KEY not in extracts\n        and constants.TRANSFORMED_FEATURES_KEY not in extracts\n    ):\n        raise ValueError(\n            f'\"{constants.FEATURES_KEY}\" or {constants.TRANSFORMED_FEATURES_KEY} '\n            \"key not found in extracts. Check that the proper extractor has been \"\n            \"configured to extract the attributions from the inputs.\"\n            f\"Existing keys: {extracts.keys()}\"\n        )\n    if include_attributions and constants.ATTRIBUTIONS_KEY not in extracts:\n        raise ValueError(\n            f'\"{constants.ATTRIBUTIONS_KEY}\" key not found in '\n            \"extracts. Check that the proper extractor has been \"\n            \"configured to extract the attributions from the inputs.\"\n            f\"Existing keys: {extracts.keys()}\"\n        )\n    return metric_types.StandardMetricInputs(extracts)\n</code></pre>"},{"location":"api_docs/python/tfma-post_export_metrics/","title":"TFMA Post_Export_Metrics","text":""},{"location":"api_docs/python/tfma-post_export_metrics/#tensorflow_model_analysis.post_export_metrics","title":"tensorflow_model_analysis.post_export_metrics","text":"<p>Init module for TensorFlow Model Analysis post export metrics.</p> <p>WARNING: This is a legacy API that is no longer available in OSS and no longer under active development.</p>"},{"location":"api_docs/python/tfma-sdk/","title":"TFMA SDK","text":""},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk","title":"tensorflow_model_analysis.sdk","text":"<p>SDK for TensorFlow Model Analysis.</p>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.ANALYSIS_KEY","title":"ANALYSIS_KEY  <code>module-attribute</code>","text":"<pre><code>ANALYSIS_KEY = 'analysis'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.ARROW_INPUT_COLUMN","title":"ARROW_INPUT_COLUMN  <code>module-attribute</code>","text":"<pre><code>ARROW_INPUT_COLUMN = '__raw_record__'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.ARROW_RECORD_BATCH_KEY","title":"ARROW_RECORD_BATCH_KEY  <code>module-attribute</code>","text":"<pre><code>ARROW_RECORD_BATCH_KEY = 'arrow_record_batch'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.ATTRIBUTIONS_KEY","title":"ATTRIBUTIONS_KEY  <code>module-attribute</code>","text":"<pre><code>ATTRIBUTIONS_KEY = 'attributions'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.BASELINE_KEY","title":"BASELINE_KEY  <code>module-attribute</code>","text":"<pre><code>BASELINE_KEY = 'baseline'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.BASELINE_SCORE_KEY","title":"BASELINE_SCORE_KEY  <code>module-attribute</code>","text":"<pre><code>BASELINE_SCORE_KEY = 'baseline_score'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.CANDIDATE_KEY","title":"CANDIDATE_KEY  <code>module-attribute</code>","text":"<pre><code>CANDIDATE_KEY = 'candidate'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.DATA_CENTRIC_MODE","title":"DATA_CENTRIC_MODE  <code>module-attribute</code>","text":"<pre><code>DATA_CENTRIC_MODE = 'data_centric_mode'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.EXAMPLE_SCORE_KEY","title":"EXAMPLE_SCORE_KEY  <code>module-attribute</code>","text":"<pre><code>EXAMPLE_SCORE_KEY = 'example_score'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.EXAMPLE_WEIGHTS_KEY","title":"EXAMPLE_WEIGHTS_KEY  <code>module-attribute</code>","text":"<pre><code>EXAMPLE_WEIGHTS_KEY = 'example_weights'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.FEATURES_KEY","title":"FEATURES_KEY  <code>module-attribute</code>","text":"<pre><code>FEATURES_KEY = 'features'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.FEATURES_PREDICTIONS_LABELS_KEY","title":"FEATURES_PREDICTIONS_LABELS_KEY  <code>module-attribute</code>","text":"<pre><code>FEATURES_PREDICTIONS_LABELS_KEY = '_fpl'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.INPUT_KEY","title":"INPUT_KEY  <code>module-attribute</code>","text":"<pre><code>INPUT_KEY = 'input'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.LABELS_KEY","title":"LABELS_KEY  <code>module-attribute</code>","text":"<pre><code>LABELS_KEY = 'labels'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.METRICS_KEY","title":"METRICS_KEY  <code>module-attribute</code>","text":"<pre><code>METRICS_KEY = 'metrics'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.MODEL_CENTRIC_MODE","title":"MODEL_CENTRIC_MODE  <code>module-attribute</code>","text":"<pre><code>MODEL_CENTRIC_MODE = 'model_centric_mode'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.PLOTS_KEY","title":"PLOTS_KEY  <code>module-attribute</code>","text":"<pre><code>PLOTS_KEY = 'plots'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.PREDICTIONS_KEY","title":"PREDICTIONS_KEY  <code>module-attribute</code>","text":"<pre><code>PREDICTIONS_KEY = 'predictions'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.SLICE_KEY_TYPES_KEY","title":"SLICE_KEY_TYPES_KEY  <code>module-attribute</code>","text":"<pre><code>SLICE_KEY_TYPES_KEY = '_slice_key_types'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.TFMA_EVAL","title":"TFMA_EVAL  <code>module-attribute</code>","text":"<pre><code>TFMA_EVAL = 'tfma_eval'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.TF_ESTIMATOR","title":"TF_ESTIMATOR  <code>module-attribute</code>","text":"<pre><code>TF_ESTIMATOR = 'tf_estimator'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.TF_GENERIC","title":"TF_GENERIC  <code>module-attribute</code>","text":"<pre><code>TF_GENERIC = 'tf_generic'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.TF_JS","title":"TF_JS  <code>module-attribute</code>","text":"<pre><code>TF_JS = 'tf_js'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.TF_KERAS","title":"TF_KERAS  <code>module-attribute</code>","text":"<pre><code>TF_KERAS = 'tf_keras'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.TF_LITE","title":"TF_LITE  <code>module-attribute</code>","text":"<pre><code>TF_LITE = 'tf_lite'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.VALIDATIONS_KEY","title":"VALIDATIONS_KEY  <code>module-attribute</code>","text":"<pre><code>VALIDATIONS_KEY = 'validations'\n</code></pre>"},{"location":"api_docs/python/tfma-sdk/#tensorflow_model_analysis.sdk.VERSION_STRING","title":"VERSION_STRING  <code>module-attribute</code>","text":"<pre><code>VERSION_STRING = '0.49.0.dev'\n</code></pre>"},{"location":"api_docs/python/tfma-types/","title":"TFMA Types","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types","title":"tensorflow_model_analysis.types","text":"<p>Types.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.AddMetricsCallbackType","title":"AddMetricsCallbackType  <code>module-attribute</code>","text":"<pre><code>AddMetricsCallbackType = Callable[\n    [\n        TensorTypeMaybeDict,\n        TensorTypeMaybeDict,\n        TensorTypeMaybeDict,\n    ],\n    Dict[str, Tuple[TensorType, TensorType]],\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ConcreteStructuredMetricValue","title":"ConcreteStructuredMetricValue  <code>module-attribute</code>","text":"<pre><code>ConcreteStructuredMetricValue = TypeVar(\n    \"ConcreteStructuredMetricValue\",\n    bound=\"StructuredMetricValue\",\n)\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.DictOfFetchedTensorValues","title":"DictOfFetchedTensorValues  <code>module-attribute</code>","text":"<pre><code>DictOfFetchedTensorValues = Dict[\n    FPLKeyType, Dict[str, TensorValue]\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.DictOfTensorType","title":"DictOfTensorType  <code>module-attribute</code>","text":"<pre><code>DictOfTensorType = Dict[str, TensorType]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.DictOfTensorTypeMaybeDict","title":"DictOfTensorTypeMaybeDict  <code>module-attribute</code>","text":"<pre><code>DictOfTensorTypeMaybeDict = Dict[str, TensorTypeMaybeDict]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.DictOfTensorValue","title":"DictOfTensorValue  <code>module-attribute</code>","text":"<pre><code>DictOfTensorValue = Dict[str, TensorValue]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.DictOfTensorValueMaybeDict","title":"DictOfTensorValueMaybeDict  <code>module-attribute</code>","text":"<pre><code>DictOfTensorValueMaybeDict = Dict[str, TensorValueMaybeDict]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.DictOfTypeSpec","title":"DictOfTypeSpec  <code>module-attribute</code>","text":"<pre><code>DictOfTypeSpec = Dict[str, TypeSpec]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.DictOfTypeSpecMaybeDict","title":"DictOfTypeSpecMaybeDict  <code>module-attribute</code>","text":"<pre><code>DictOfTypeSpecMaybeDict = Dict[str, TypeSpecMaybeDict]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.Extracts","title":"Extracts  <code>module-attribute</code>","text":"<pre><code>Extracts = MutableMapping[str, Any]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.FPLKeyType","title":"FPLKeyType  <code>module-attribute</code>","text":"<pre><code>FPLKeyType = Union[str, Tuple[str, ...]]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.MaybeMultipleEvalSharedModels","title":"MaybeMultipleEvalSharedModels  <code>module-attribute</code>","text":"<pre><code>MaybeMultipleEvalSharedModels = Union[\n    EvalSharedModel,\n    List[EvalSharedModel],\n    Dict[str, EvalSharedModel],\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.MetricValueType","title":"MetricValueType  <code>module-attribute</code>","text":"<pre><code>MetricValueType = Union[\n    PrimitiveMetricValueType, ndarray, StructuredMetricValue\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.MetricVariablesType","title":"MetricVariablesType  <code>module-attribute</code>","text":"<pre><code>MetricVariablesType = List[Any]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.PrimitiveMetricValueType","title":"PrimitiveMetricValueType  <code>module-attribute</code>","text":"<pre><code>PrimitiveMetricValueType = Union[float, int, number]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TensorOrOperationType","title":"TensorOrOperationType  <code>module-attribute</code>","text":"<pre><code>TensorOrOperationType = Union[TensorType, Operation]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TensorType","title":"TensorType  <code>module-attribute</code>","text":"<pre><code>TensorType = Union[Tensor, SparseTensor, RaggedTensor]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TensorTypeMaybeDict","title":"TensorTypeMaybeDict  <code>module-attribute</code>","text":"<pre><code>TensorTypeMaybeDict = Union[TensorType, DictOfTensorType]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TensorTypeMaybeMultiLevelDict","title":"TensorTypeMaybeMultiLevelDict  <code>module-attribute</code>","text":"<pre><code>TensorTypeMaybeMultiLevelDict = Union[\n    TensorTypeMaybeDict, DictOfTensorTypeMaybeDict\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TensorValue","title":"TensorValue  <code>module-attribute</code>","text":"<pre><code>TensorValue = Union[\n    ndarray,\n    SparseTensorValue,\n    RaggedTensorValue,\n    SparseTensorValue,\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TensorValueMaybeDict","title":"TensorValueMaybeDict  <code>module-attribute</code>","text":"<pre><code>TensorValueMaybeDict = Union[TensorValue, DictOfTensorValue]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TensorValueMaybeMultiLevelDict","title":"TensorValueMaybeMultiLevelDict  <code>module-attribute</code>","text":"<pre><code>TensorValueMaybeMultiLevelDict = Union[\n    TensorValueMaybeDict, DictOfTensorValueMaybeDict\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TypeSpecMaybeDict","title":"TypeSpecMaybeDict  <code>module-attribute</code>","text":"<pre><code>TypeSpecMaybeDict = Union[TypeSpec, DictOfTypeSpec]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.TypeSpecMaybeMultiLevelDict","title":"TypeSpecMaybeMultiLevelDict  <code>module-attribute</code>","text":"<pre><code>TypeSpecMaybeMultiLevelDict = Union[\n    TypeSpecMaybeDict, DictOfTypeSpecMaybeDict\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.EvalSharedModel","title":"EvalSharedModel","text":"<p>               Bases: <code>NamedTuple('EvalSharedModel', [('model_path', str), ('add_metrics_callbacks', List[Callable]), ('include_default_metrics', bool), ('example_weight_key', Union[str, Dict[str, str]]), ('additional_fetches', List[str]), ('model_loader', ModelLoader), ('model_name', str), ('model_type', str), ('rubber_stamp', bool), ('is_baseline', bool), ('resource_hints', Optional[Dict[str, Any]]), ('backend_config', Optional[Any])])</code></p> <p>Shared model used during extraction and evaluation.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.EvalSharedModel--attributes","title":"Attributes","text":"<p>model_path: Path to EvalSavedModel (containing the saved_model.pb file).   add_metrics_callbacks: Optional list of callbacks for adding additional     metrics to the graph. The names of the metrics added by the callbacks     should not conflict with existing metrics. See below for more details     about what each callback should do. The callbacks are only used during     evaluation.   include_default_metrics: True to include the default metrics that are part     of the saved model graph during evaluation.   example_weight_key: Example weight key (single-output model) or dict of     example weight keys (multi-output model) keyed by output_name.   additional_fetches: Prefixes of additional tensors stored in     signature_def.inputs that should be fetched at prediction time. The     \"features\" and \"labels\" tensors are handled automatically and should not     be included in this list.   model_loader: Model loader.   model_name: Model name (should align with ModelSpecs.name).   model_type: Model type (tfma.TF_KERAS, tfma.TF_LITE, tfma.TF_ESTIMATOR, ..).   rubber_stamp: True if this model is being rubber stamped. When a     model is rubber stamped diff thresholds will be ignored if an associated     baseline model is not passed.   is_baseline: The model is the baseline for comparison or not.   resource_hints: The beam resource hints to apply to the PTransform which     runs inference for this model.   backend_config: The backend config for running model inference.</p> <p>More details on add_metrics_callbacks:</p> <p>Each add_metrics_callback should have the following prototype:     def add_metrics_callback(features_dict, predictions_dict, labels_dict):</p> <p>Note that features_dict, predictions_dict and labels_dict are not   necessarily dictionaries - they might also be Tensors, depending on what the   model's eval_input_receiver_fn returns.</p> <p>It should create and return a metric_ops dictionary, such that   metric_ops['metric_name'] = (value_op, update_op), just as in the Trainer.</p> <p>Short example:</p> <p>def add_metrics_callback(features_dict, predictions_dict, labels):     metrics_ops = {}     metric_ops['mean_label'] = tf.metrics.mean(labels)     metric_ops['mean_probability'] = tf.metrics.mean(tf.slice(       predictions_dict['probabilities'], [0, 1], [2, 1]))     return metric_ops</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.FeaturesPredictionsLabels","title":"FeaturesPredictionsLabels","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.FeaturesPredictionsLabels-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.FeaturesPredictionsLabels.features","title":"features  <code>instance-attribute</code>","text":"<pre><code>features: DictOfFetchedTensorValues\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.FeaturesPredictionsLabels.input_ref","title":"input_ref  <code>instance-attribute</code>","text":"<pre><code>input_ref: int\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.FeaturesPredictionsLabels.labels","title":"labels  <code>instance-attribute</code>","text":"<pre><code>labels: DictOfFetchedTensorValues\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.FeaturesPredictionsLabels.predictions","title":"predictions  <code>instance-attribute</code>","text":"<pre><code>predictions: DictOfFetchedTensorValues\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.MaterializedColumn","title":"MaterializedColumn","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.MaterializedColumn-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.MaterializedColumn.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.MaterializedColumn.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: Union[\n    List[bytes], List[int], List[float], bytes, int, float\n]\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ModelLoader","title":"ModelLoader","text":"<pre><code>ModelLoader(\n    construct_fn: Callable[[], Any],\n    tags: Optional[List[str]] = None,\n)\n</code></pre> <p>Model loader is responsible for loading shared model types.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ModelLoader--attributes","title":"Attributes","text":"<p>construct_fn: A callable which creates the model instance. The callable     should take no args as input (typically a closure is used to capture     necessary parameters).   tags: Optional model tags (e.g. 'serve' for serving or 'eval' for     EvalSavedModel).</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def __init__(\n    self, construct_fn: Callable[[], Any], tags: Optional[List[str]] = None\n):\n    self.construct_fn = construct_fn\n    self.tags = tags\n    self._shared_handle = shared.Shared()\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ModelLoader-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ModelLoader.construct_fn","title":"construct_fn  <code>instance-attribute</code>","text":"<pre><code>construct_fn = construct_fn\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ModelLoader.tags","title":"tags  <code>instance-attribute</code>","text":"<pre><code>tags = tags\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ModelLoader-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ModelLoader.load","title":"load","text":"<pre><code>load(\n    model_load_time_callback: Optional[\n        Callable[[int], None]\n    ] = None,\n) -&gt; Any\n</code></pre> <p>Returns loaded model.</p> <p>model_load_time_callback: Optional callback to track load time.</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def load(\n    self, model_load_time_callback: Optional[Callable[[int], None]] = None\n) -&gt; Any:\n    \"\"\"Returns loaded model.\n\n    Args:\n    ----\n      model_load_time_callback: Optional callback to track load time.\n    \"\"\"\n    if model_load_time_callback:\n        construct_fn = self._construct_fn_with_load_time(model_load_time_callback)\n    else:\n        construct_fn = self.construct_fn\n    return self._shared_handle.acquire(construct_fn)\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.RaggedTensorValue","title":"RaggedTensorValue","text":"<p>               Bases: <code>NamedTuple('RaggedTensorValue', [('values', ndarray), ('nested_row_splits', List[ndarray])])</code></p> <p>RaggedTensorValue encapsulates a batch of ragged tensor values.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.RaggedTensorValue--attributes","title":"Attributes","text":"<p>values: A np.ndarray of values.   nested_row_splits: A list of np.ndarray values representing the row splits     (one per dimension including the batch dimension).</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.SparseTensorValue","title":"SparseTensorValue","text":"<p>               Bases: <code>NamedTuple('SparseTensorValue', [('values', ndarray), ('indices', ndarray), ('dense_shape', ndarray)])</code></p> <p>SparseTensorValue encapsulates a batch of sparse tensor values.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.SparseTensorValue--attributes","title":"Attributes","text":"<p>values: A np.ndarray of values.   indices: A np.ndarray of indices.   dense_shape: A np.ndarray representing the dense shape.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.StructuredMetricValue","title":"StructuredMetricValue","text":"<p>               Bases: <code>ABC</code></p> <p>The base class for all structured metrics used within TFMA.</p> <p>This class allows custom metrics to control how proto serialization happens, and how to handle basic algebraic operations used in computing confidence intervals and model diffs. By implementing the _apply_binary_op methods, subclasses can then be treated like primitive numeric types.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.StructuredMetricValue-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.StructuredMetricValue.to_proto","title":"to_proto  <code>abstractmethod</code>","text":"<pre><code>to_proto() -&gt; MetricValue\n</code></pre> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>@abc.abstractmethod\ndef to_proto(self) -&gt; metrics_for_slice_pb2.MetricValue: ...\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.ValueWithTDistribution","title":"ValueWithTDistribution","text":"<p>               Bases: <code>NamedTuple('ValueWithTDistribution', [('sample_mean', MetricValueType), ('sample_standard_deviation', MetricValueType), ('sample_degrees_of_freedom', int), ('unsampled_value', MetricValueType)])</code></p> <p>Represents the t-distribution value.</p> <p>It includes sample_mean, sample_standard_deviation, sample_degrees_of_freedom. And also unsampled_value is also stored here to record the value calculated without bootstrapping. The sample_standard_deviation is calculated as: \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^{N}{(x_i - \\bar{x})^2} }</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.VarLenTensorValue","title":"VarLenTensorValue","text":"<p>               Bases: <code>NamedTuple('VarLenTensorValue', [('values', ndarray), ('indices', ndarray), ('dense_shape', ndarray)])</code></p> <p>VarLenTensorValue encapsulates a batch of varlen dense tensor values.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.VarLenTensorValue--attributes","title":"Attributes","text":"<p>values: A np.ndarray of values.   indices: A np.ndarray of indices.   dense_shape: A np.ndarray representing the dense shape of the entire tensor.     Note that each row (i.e. set of values sharing the same value for the     first / batch dimension) is considered to have its own shape based on the     presence of values.</p>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.VarLenTensorValue-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.VarLenTensorValue.DenseRowIterator","title":"DenseRowIterator","text":"<pre><code>DenseRowIterator(tensor)\n</code></pre> <p>An Iterator over rows of a VarLenTensorValue as dense np.arrays.</p> <p>Because the VarLenTensorValue was created from a set of variable length (dense) arrays, we can invert this process to turn a VarLenTensorValue back into the original dense arrays.</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def __init__(self, tensor):\n    self._tensor = tensor\n    self._offset = 0\n</code></pre> Functions\u00b6"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.VarLenTensorValue-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.VarLenTensorValue.dense_rows","title":"dense_rows","text":"<pre><code>dense_rows()\n</code></pre> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def dense_rows(self):\n    return self.DenseRowIterator(self)\n</code></pre>"},{"location":"api_docs/python/tfma-types/#tensorflow_model_analysis.types.VarLenTensorValue.from_dense_rows","title":"from_dense_rows  <code>classmethod</code>","text":"<pre><code>from_dense_rows(\n    dense_rows: Iterable[ndarray],\n) -&gt; VarLenTensorValue\n</code></pre> <p>Converts a collection of variable length dense arrays into a tensor.</p> <p>dense_rows: A sequence of possibly variable length 1D arrays.</p> <p>A new VarLenTensorValue containing the sparse representation of the   vertically stacked dense rows. The dense_shape attribute on the result   will be (num_rows, max_row_len).</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>@classmethod\ndef from_dense_rows(cls, dense_rows: Iterable[np.ndarray]) -&gt; \"VarLenTensorValue\":\n    \"\"\"Converts a collection of variable length dense arrays into a tensor.\n\n    Args:\n    ----\n      dense_rows: A sequence of possibly variable length 1D arrays.\n\n    Returns:\n    -------\n      A new VarLenTensorValue containing the sparse representation of the\n      vertically stacked dense rows. The dense_shape attribute on the result\n      will be (num_rows, max_row_len).\n    \"\"\"\n    rows = []\n    index_arrays = []\n    max_row_len = 0\n    num_rows = 0\n    for i, row in enumerate(dense_rows):\n        num_rows += 1\n        if row.size:\n            if row.ndim &lt;= 1:\n                # Add a dimension for unsized numpy array. This will solve the problem\n                # where scalar numpy arrays like np.array(None), np.array(0) can not\n                # be merged with other numpy arrays.\n                row = row.reshape(-1)\n                rows.append(row)\n            else:\n                raise ValueError(\n                    \"Each non-empty dense row should be 1D or scalar but\"\n                    f\" found row with shape {row.shape}.\"\n                )\n            index_arrays.append(np.array([[i, j] for j in range(len(row))]))\n        max_row_len = max(max_row_len, row.size)\n    if index_arrays:\n        values = np.concatenate(rows, axis=0)\n        indices = np.concatenate(index_arrays, axis=0)\n    else:\n        # empty case\n        values = np.array([])\n        indices = np.empty((0, 2))\n    dense_shape = np.array([num_rows, max_row_len])\n    return cls.__new__(cls, values=values, indices=indices, dense_shape=dense_shape)\n</code></pre>"},{"location":"api_docs/python/tfma-utils/","title":"TFMA Utils","text":""},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils","title":"tensorflow_model_analysis.utils","text":"<p>Init module for TensorFlow Model Analysis utils.</p>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.CombineFnWithModels","title":"CombineFnWithModels","text":"<pre><code>CombineFnWithModels(model_loaders: Dict[str, ModelLoader])\n</code></pre> <p>               Bases: <code>CombineFn</code></p> <p>Abstract class for CombineFns that need the shared models.</p> <p>Initializes CombineFn using dict of loaders keyed by model location.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def __init__(self, model_loaders: Dict[str, types.ModelLoader]):\n    \"\"\"Initializes CombineFn using dict of loaders keyed by model location.\"\"\"\n    self._model_loaders = model_loaders\n    self._loaded_models = None\n    self._model_load_seconds = None\n    self._model_load_seconds_distribution = beam.metrics.Metrics.distribution(\n        constants.METRICS_NAMESPACE, \"model_load_seconds\"\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.CombineFnWithModels-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.CombineFnWithModels.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def setup(self):\n    if self._loaded_models is None:\n        self._loaded_models = {}\n        for model_name, model_loader in self._model_loaders.items():\n            self._loaded_models[model_name] = model_loader.load(\n                model_load_time_callback=self._set_model_load_seconds\n            )\n        if self._model_load_seconds is not None:\n            self._model_load_seconds_distribution.update(self._model_load_seconds)\n            self._model_load_seconds = None\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.DoFnWithModels","title":"DoFnWithModels","text":"<pre><code>DoFnWithModels(model_loaders: Dict[str, ModelLoader])\n</code></pre> <p>               Bases: <code>DoFn</code></p> <p>Abstract class for DoFns that need the shared models.</p> <p>Initializes DoFn using dict of model loaders keyed by model location.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def __init__(self, model_loaders: Dict[str, types.ModelLoader]):\n    \"\"\"Initializes DoFn using dict of model loaders keyed by model location.\"\"\"\n    self._model_loaders = model_loaders\n    self._loaded_models = None\n    self._model_load_seconds = None\n    self._model_load_seconds_distribution = beam.metrics.Metrics.distribution(\n        constants.METRICS_NAMESPACE, \"model_load_seconds\"\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.DoFnWithModels-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.DoFnWithModels.finish_bundle","title":"finish_bundle","text":"<pre><code>finish_bundle()\n</code></pre> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def finish_bundle(self):\n    # Must update distribution in finish_bundle instead of setup\n    # because Beam metrics are not supported in setup.\n    if self._model_load_seconds is not None:\n        self._model_load_seconds_distribution.update(self._model_load_seconds)\n        self._model_load_seconds = None\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.DoFnWithModels.process","title":"process","text":"<pre><code>process(elem)\n</code></pre> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def process(self, elem):\n    raise NotImplementedError(\"Subclasses are expected to override this.\")\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.DoFnWithModels.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def setup(self):\n    self._loaded_models = {}\n    for model_name, model_loader in self._model_loaders.items():\n        self._loaded_models[model_name] = model_loader.load(\n            model_load_time_callback=self._set_model_load_seconds\n        )\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.calculate_confidence_interval","title":"calculate_confidence_interval","text":"<pre><code>calculate_confidence_interval(\n    t_distribution_value: ValueWithTDistribution,\n)\n</code></pre> <p>Calculate confidence intervals based 95% confidence level.</p> Source code in <code>tensorflow_model_analysis/utils/math_util.py</code> <pre><code>def calculate_confidence_interval(t_distribution_value: types.ValueWithTDistribution):\n    \"\"\"Calculate confidence intervals based 95% confidence level.\"\"\"\n    alpha = 0.05\n    std_err = t_distribution_value.sample_standard_deviation\n    t_stat = stats.t.ppf(\n        1 - (alpha / 2.0), t_distribution_value.sample_degrees_of_freedom\n    )\n    # The order of operands matters here because we want to use the\n    # std_err.__mul__ operator below, rather than the t_stat.__mul__.\n    # TODO(b/197669322): make StructuredMetricValues robust to operand ordering.\n    upper_bound = t_distribution_value.sample_mean + std_err * t_stat\n    lower_bound = t_distribution_value.sample_mean - std_err * t_stat\n    return t_distribution_value.sample_mean, lower_bound, upper_bound\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.compound_key","title":"compound_key","text":"<pre><code>compound_key(\n    keys: Sequence[str], separator: str = KEY_SEPARATOR\n) -&gt; str\n</code></pre> <p>Returns a compound key based on a list of keys.</p> <p>keys: Keys used to make up compound key.   separator: Separator between keys. To ensure the keys can be parsed out of     any compound key created, any use of a separator within a key will be     replaced by two separators.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def compound_key(keys: Sequence[str], separator: str = KEY_SEPARATOR) -&gt; str:\n    \"\"\"Returns a compound key based on a list of keys.\n\n    Args:\n    ----\n      keys: Keys used to make up compound key.\n      separator: Separator between keys. To ensure the keys can be parsed out of\n        any compound key created, any use of a separator within a key will be\n        replaced by two separators.\n    \"\"\"\n    return separator.join([key.replace(separator, separator * 2) for key in keys])\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.create_keys_key","title":"create_keys_key","text":"<pre><code>create_keys_key(key: str) -&gt; str\n</code></pre> <p>Creates secondary key representing the sparse keys associated with key.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def create_keys_key(key: str) -&gt; str:\n    \"\"\"Creates secondary key representing the sparse keys associated with key.\"\"\"\n    return \"_\".join([key, KEYS_SUFFIX])\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.create_values_key","title":"create_values_key","text":"<pre><code>create_values_key(key: str) -&gt; str\n</code></pre> <p>Creates secondary key representing sparse values associated with key.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def create_values_key(key: str) -&gt; str:\n    \"\"\"Creates secondary key representing sparse values associated with key.\"\"\"\n    return \"_\".join([key, VALUES_SUFFIX])\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.get_baseline_model_spec","title":"get_baseline_model_spec","text":"<pre><code>get_baseline_model_spec(\n    eval_config: EvalConfig,\n) -&gt; Optional[ModelSpec]\n</code></pre> <p>Returns baseline model spec.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def get_baseline_model_spec(\n    eval_config: config_pb2.EvalConfig,\n) -&gt; Optional[config_pb2.ModelSpec]:\n    \"\"\"Returns baseline model spec.\"\"\"\n    for spec in eval_config.model_specs:\n        if spec.is_baseline:\n            return spec\n    return None\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.get_by_keys","title":"get_by_keys","text":"<pre><code>get_by_keys(\n    data: Mapping[str, Any],\n    keys: Sequence[Any],\n    default_value=None,\n    optional: bool = False,\n) -&gt; Any\n</code></pre> <p>Returns value with given key(s) in (possibly multi-level) dict.</p> <p>The keys represent multiple levels of indirection into the data. For example if 3 keys are passed then the data is expected to be a dict of dict of dict. For compatibily with data that uses prefixing to create separate the keys in a single dict, lookups will also be searched for under the keys separated by '/'. For example, the keys 'head1' and 'probabilities' could be stored in a a single dict as 'head1/probabilties'.</p> <p>data: Dict to get value from.   keys: Sequence of keys to lookup in data. None keys will be ignored.   default_value: Default value if not found.   optional: Whether the key is optional or not. If default value is None and     optional is False then a ValueError will be raised if key not found.</p> <p>ValueError: If (non-optional) key is not found.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def get_by_keys(\n    data: Mapping[str, Any],\n    keys: Sequence[Any],\n    default_value=None,\n    optional: bool = False,\n) -&gt; Any:\n    \"\"\"Returns value with given key(s) in (possibly multi-level) dict.\n\n    The keys represent multiple levels of indirection into the data. For example\n    if 3 keys are passed then the data is expected to be a dict of dict of dict.\n    For compatibily with data that uses prefixing to create separate the keys in a\n    single dict, lookups will also be searched for under the keys separated by\n    '/'. For example, the keys 'head1' and 'probabilities' could be stored in a\n    a single dict as 'head1/probabilties'.\n\n    Args:\n    ----\n      data: Dict to get value from.\n      keys: Sequence of keys to lookup in data. None keys will be ignored.\n      default_value: Default value if not found.\n      optional: Whether the key is optional or not. If default value is None and\n        optional is False then a ValueError will be raised if key not found.\n\n    Raises:\n    ------\n      ValueError: If (non-optional) key is not found.\n    \"\"\"\n    if not keys:\n        raise ValueError(\"no keys provided to get_by_keys: %s\" % data)\n\n    format_keys = lambda keys: \"-&gt;\".join([str(k) for k in keys if k is not None])\n\n    value = data\n    keys_matched = 0\n    for i, key in enumerate(keys):\n        if key is None:\n            keys_matched += 1\n            continue\n\n        if not isinstance(value, Mapping):\n            raise ValueError(\n                'expected dict for \"%s\" but found %s: %s'\n                % (format_keys(keys[: i + 1]), type(value), data)\n            )\n\n        if key in value:\n            value = value[key]\n            keys_matched += 1\n            continue\n\n        # If values have prefixes matching the key, return those values (stripped\n        # of the prefix) instead.\n        prefix_matches = {}\n        for k, v in value.items():\n            if k.startswith(key + \"/\"):\n                prefix_matches[k[len(key) + 1 :]] = v\n        if prefix_matches:\n            value = prefix_matches\n            keys_matched += 1\n            continue\n\n        break\n\n    if keys_matched &lt; len(keys) or isinstance(value, Mapping) and not value:\n        if default_value is not None:\n            return default_value\n        if optional:\n            return None\n        raise ValueError(\n            '\"%s\" key not found (or value is empty dict): %s'\n            % (format_keys(keys[: keys_matched + 1]), data)\n        )\n    return value\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.get_model_spec","title":"get_model_spec","text":"<pre><code>get_model_spec(\n    eval_config: EvalConfig, model_name: str\n) -&gt; Optional[ModelSpec]\n</code></pre> <p>Returns model spec with given model name.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def get_model_spec(\n    eval_config: config_pb2.EvalConfig, model_name: str\n) -&gt; Optional[config_pb2.ModelSpec]:\n    \"\"\"Returns model spec with given model name.\"\"\"\n    if len(eval_config.model_specs) == 1 and not model_name:\n        return eval_config.model_specs[0]\n    for spec in eval_config.model_specs:\n        if spec.name == model_name:\n            return spec\n    return None\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.get_model_type","title":"get_model_type","text":"<pre><code>get_model_type(\n    model_spec: Optional[ModelSpec],\n    model_path: Optional[str] = \"\",\n    tags: Optional[List[str]] = None,\n) -&gt; str\n</code></pre> <p>Returns model type for given model spec taking into account defaults.</p> <p>The defaults are chosen such that if a model_path is provided and the model can be loaded as a keras model then TF_KERAS is assumed. Next, if tags are provided and the tags contains 'eval' then TF_ESTIMATOR is assumed. Lastly, if the model spec contains an 'eval' signature TF_ESTIMATOR is assumed otherwise TF_GENERIC is assumed.</p> <p>model_spec: Model spec.   model_path: Optional model path to verify if keras model.   tags: Options tags to verify if eval is used.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def get_model_type(\n    model_spec: Optional[config_pb2.ModelSpec],\n    model_path: Optional[str] = \"\",\n    tags: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"Returns model type for given model spec taking into account defaults.\n\n    The defaults are chosen such that if a model_path is provided and the model\n    can be loaded as a keras model then TF_KERAS is assumed. Next, if tags\n    are provided and the tags contains 'eval' then TF_ESTIMATOR is assumed.\n    Lastly, if the model spec contains an 'eval' signature TF_ESTIMATOR is assumed\n    otherwise TF_GENERIC is assumed.\n\n    Args:\n    ----\n      model_spec: Model spec.\n      model_path: Optional model path to verify if keras model.\n      tags: Options tags to verify if eval is used.\n    \"\"\"\n    if model_spec and model_spec.model_type:\n        return model_spec.model_type\n\n    if model_path:\n        try:\n            keras_model = tf.keras.models.load_model(model_path)\n            # In some cases, tf.keras.models.load_model can successfully load a\n            # saved_model but it won't actually be a keras model.\n            if isinstance(keras_model, tf.keras.models.Model):\n                return constants.TF_KERAS\n        except Exception:  # pylint: disable=broad-except\n            pass\n\n    signature_name = None\n    if model_spec:\n        if model_spec.signature_name:\n            signature_name = model_spec.signature_name\n        else:\n            signature_name = tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n\n    return constants.TF_GENERIC\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.get_non_baseline_model_specs","title":"get_non_baseline_model_specs","text":"<pre><code>get_non_baseline_model_specs(\n    eval_config: EvalConfig,\n) -&gt; Iterable[ModelSpec]\n</code></pre> <p>Returns non-baseline model specs.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def get_non_baseline_model_specs(\n    eval_config: config_pb2.EvalConfig,\n) -&gt; Iterable[config_pb2.ModelSpec]:\n    \"\"\"Returns non-baseline model specs.\"\"\"\n    return [spec for spec in eval_config.model_specs if not spec.is_baseline]\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.has_change_threshold","title":"has_change_threshold","text":"<pre><code>has_change_threshold(eval_config: EvalConfig) -&gt; bool\n</code></pre> <p>Checks whether the eval_config has any change thresholds.</p> <p>eval_config: the TFMA eval_config.</p> <p>True when there are change thresholds otherwise False.</p> Source code in <code>tensorflow_model_analysis/utils/config_util.py</code> <pre><code>def has_change_threshold(eval_config: config_pb2.EvalConfig) -&gt; bool:\n    \"\"\"Checks whether the eval_config has any change thresholds.\n\n    Args:\n    ----\n      eval_config: the TFMA eval_config.\n\n    Returns:\n    -------\n      True when there are change thresholds otherwise False.\n    \"\"\"\n    for metrics_spec in eval_config.metrics_specs:\n        for metric in metrics_spec.metrics:\n            if metric.threshold.change_threshold.ByteSize():\n                return True\n            for per_slice_threshold in metric.per_slice_thresholds:\n                if per_slice_threshold.threshold.change_threshold.ByteSize():\n                    return True\n            for cross_slice_threshold in metric.cross_slice_thresholds:\n                if cross_slice_threshold.threshold.change_threshold.ByteSize():\n                    return True\n        for threshold in metrics_spec.thresholds.values():\n            if threshold.change_threshold.ByteSize():\n                return True\n        for per_slice_thresholds in metrics_spec.per_slice_thresholds.values():\n            for per_slice_threshold in per_slice_thresholds.thresholds:\n                if per_slice_threshold.threshold.change_threshold.ByteSize():\n                    return True\n        for cross_slice_thresholds in metrics_spec.cross_slice_thresholds.values():\n            for cross_slice_threshold in cross_slice_thresholds.thresholds:\n                if cross_slice_threshold.threshold.change_threshold.ByteSize():\n                    return True\n    return False\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.merge_extracts","title":"merge_extracts","text":"<pre><code>merge_extracts(\n    extracts: List[Extracts],\n    squeeze_two_dim_vector: bool = True,\n) -&gt; Extracts\n</code></pre> <p>Merges list of extracts into a single extract with multidimensional data.</p> Running split_extracts followed by merge extracts with default options <p>will not reproduce the exact shape of the original extracts. Arrays in shape (x,1) will be flattened to (x,). To maintain the original shape of extract values of array shape (x,1), you must run with these options: split_extracts(extracts, expand_zero_dims=False) merge_extracts(extracts, squeeze_two_dim_vector=False)</p> <p>extracts: Batched TFMA Extracts.   squeeze_two_dim_vector: Determines how the function will handle arrays of     shape (x,1). If squeeze_two_dim_vector is True, the array will be squeezed     to shape (x,).</p> <p>A single Extracts whose values have been grouped into batches.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def merge_extracts(\n    extracts: List[types.Extracts], squeeze_two_dim_vector: bool = True\n) -&gt; types.Extracts:\n    \"\"\"Merges list of extracts into a single extract with multidimensional data.\n\n    Note: Running split_extracts followed by merge extracts with default options\n      will not reproduce the exact shape of the original extracts. Arrays in shape\n      (x,1) will be flattened to (x,). To maintain the original shape of extract\n      values of array shape (x,1), you must run with these options:\n      split_extracts(extracts, expand_zero_dims=False)\n      merge_extracts(extracts, squeeze_two_dim_vector=False)\n\n    Args:\n    ----\n      extracts: Batched TFMA Extracts.\n      squeeze_two_dim_vector: Determines how the function will handle arrays of\n        shape (x,1). If squeeze_two_dim_vector is True, the array will be squeezed\n        to shape (x,).\n\n    Returns:\n    -------\n      A single Extracts whose values have been grouped into batches.\n    \"\"\"\n\n    def merge_with_lists(\n        target: types.Extracts, index: int, key: str, value: Any, num_extracts: int\n    ):\n        \"\"\"Merges key and value into the target extracts as a list of values.\n\n        Args:\n        ----\n         target: The extract to store all merged all the data.\n         index: The index at which the value should be stored. It is in accordance\n           with the order of extracts in the batch.\n         key: The key of the key-value pair to store in the target.\n         value: The value of the key-value pair to store in the target.\n         num_extracts: The total number of extracts to be merged in this target.\n        \"\"\"\n        if isinstance(value, Mapping):\n            if key not in target:\n                target[key] = {}\n            target = target[key]\n            for k, v in value.items():\n                merge_with_lists(target, index, k, v, num_extracts)\n        else:\n            # If key is newly found, we create a list with length of extracts,\n            # so that every value of the i th extracts will go to the i th position.\n            # And the extracts without this key will have value np.array([]).\n            if key not in target:\n                target[key] = [np.array([])] * num_extracts\n            target[key][index] = value\n\n    def merge_lists(target: types.Extracts) -&gt; types.Extracts:\n        \"\"\"Converts target's leaves which are lists to batched np.array's, etc.\"\"\"\n        if isinstance(target, Mapping):\n            result = {}\n            for key, value in target.items():\n                try:\n                    result[key] = merge_lists(value)\n                except Exception as e:\n                    raise RuntimeError(\n                        f\"Failed to convert value for key: {key} and value: {value}\"\n                    ) from e\n            return {k: merge_lists(v) for k, v in target.items()}\n        elif (\n            target\n            and np.any([isinstance(t, tf.compat.v1.SparseTensorValue) for t in target])\n            or np.any([isinstance(target[0], types.SparseTensorValue) for _ in target])\n        ):\n            t = tf.compat.v1.sparse_concat(\n                0,\n                [tf.sparse.expand_dims(to_tensorflow_tensor(t), 0) for t in target],\n                expand_nonconcat_dim=True,\n            )\n            return to_tensor_value(t)\n        elif target and np.any(\n            [isinstance(t, types.RaggedTensorValue) for t in target]\n        ):\n            t = tf.concat(\n                [tf.expand_dims(to_tensorflow_tensor(t), 0) for t in target], 0\n            )\n            return to_tensor_value(t)\n        elif (\n            all(isinstance(t, np.ndarray) for t in target)\n            and len({t.shape for t in target}) &gt; 1\n        ):\n            target = (t.squeeze() for t in target)\n            return types.VarLenTensorValue.from_dense_rows(target)\n        # If all value in the target are scalar numpy array, we stack them.\n        # This is to avoid np.array([np.array(b'abc'), np.array(b'abcd')])\n        # and stack to np.array([b'abc', b'abcd'])\n        elif all(isinstance(t, np.ndarray) and t.shape == () for t in target):  # pylint: disable=g-explicit-bool-comparison\n            return np.stack(target)\n        elif all(t is None for t in target):\n            return None\n        else:\n            # Compatibility shim for NumPy 1.24. See:\n            # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n            try:\n                arr = np.array(target)\n            except ValueError:\n                arr = np.array(target, dtype=object)\n            # Flatten values that were originally single item lists into a single list\n            # e.g. [[1], [2], [3]] -&gt; [1, 2, 3]\n            if squeeze_two_dim_vector and len(arr.shape) == 2 and arr.shape[1] == 1:\n                return arr.squeeze(axis=1)\n            return arr\n\n    result = {}\n    num_extracts = len(extracts)\n    for i, x in enumerate(extracts):\n        if x:\n            for k, v in x.items():\n                merge_with_lists(result, i, k, v, num_extracts)\n    return merge_lists(result)\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.model_construct_fn","title":"model_construct_fn","text":"<pre><code>model_construct_fn(\n    eval_saved_model_path: Optional[str] = None,\n    add_metrics_callbacks: Optional[\n        List[AddMetricsCallbackType]\n    ] = None,\n    include_default_metrics: Optional[bool] = None,\n    additional_fetches: Optional[List[str]] = None,\n    blacklist_feature_fetches: Optional[List[str]] = None,\n    tags: Optional[List[str]] = None,\n    model_type: Optional[str] = TFMA_EVAL,\n) -&gt; Callable[[], Any]\n</code></pre> <p>Returns function for constructing shared models.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def model_construct_fn(  # pylint: disable=invalid-name\n    eval_saved_model_path: Optional[str] = None,\n    add_metrics_callbacks: Optional[List[types.AddMetricsCallbackType]] = None,\n    include_default_metrics: Optional[bool] = None,\n    additional_fetches: Optional[List[str]] = None,\n    blacklist_feature_fetches: Optional[List[str]] = None,\n    tags: Optional[List[str]] = None,\n    model_type: Optional[str] = constants.TFMA_EVAL,\n) -&gt; Callable[[], Any]:\n    \"\"\"Returns function for constructing shared models.\"\"\"\n    if tags is None:\n        raise ValueError(\"Model tags must be specified.\")\n\n    def construct_fn():  # pylint: disable=invalid-name\n        \"\"\"Function for constructing shared models.\"\"\"\n        # If we are evaluating on TPU, initialize the TPU.\n        # TODO(b/143484017): Add model warmup for TPU.\n        if tf.saved_model.TPU in tags:\n            tf.tpu.experimental.initialize_tpu_system()\n\n        if model_type == constants.TF_KERAS:\n            model = tf.keras.models.load_model(eval_saved_model_path)\n        elif model_type == constants.TF_LITE:\n            # The tf.lite.Interpreter is not thread-safe so we only load the model\n            # file's contents and leave construction of the Interpreter up to the\n            # PTransform using it.\n            model_filename = os.path.join(eval_saved_model_path, _TFLITE_FILE_NAME)\n            with tf.io.gfile.GFile(model_filename, \"rb\") as model_file:\n                model_bytes = model_file.read()\n\n            # If a SavedModel is present in the same directory, load it as well.\n            # This allows the SavedModel to be used for computing the\n            # Transformed Features and Labels.\n            if tf.io.gfile.exists(\n                os.path.join(\n                    eval_saved_model_path, tf.saved_model.SAVED_MODEL_FILENAME_PB\n                )\n            ) or tf.io.gfile.exists(\n                os.path.join(\n                    eval_saved_model_path, tf.saved_model.SAVED_MODEL_FILENAME_PBTXT\n                )\n            ):\n                model = tf.compat.v1.saved_model.load_v2(\n                    eval_saved_model_path, tags=tags\n                )\n                model.contents = model_bytes\n            else:\n                model = ModelContents(model_bytes)\n\n        elif model_type == constants.TF_JS:\n            # We invoke TFJS models via a subprocess call. So this call is no-op.\n            return None\n        else:\n            model = tf.compat.v1.saved_model.load_v2(eval_saved_model_path, tags=tags)\n        return model\n\n    return construct_fn\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.unique_key","title":"unique_key","text":"<pre><code>unique_key(\n    key: str,\n    current_keys: List[str],\n    update_keys: Optional[bool] = False,\n) -&gt; str\n</code></pre> <p>Returns a unique key given a list of current keys.</p> <p>If the key exists in current_keys then a new key with _1, _2, ..., etc appended will be returned, otherwise the key will be returned as passed.</p> <p>key: desired key name.   current_keys: List of current key names.   update_keys: True to append the new key to current_keys.</p> Source code in <code>tensorflow_model_analysis/utils/util.py</code> <pre><code>def unique_key(\n    key: str, current_keys: List[str], update_keys: Optional[bool] = False\n) -&gt; str:\n    \"\"\"Returns a unique key given a list of current keys.\n\n    If the key exists in current_keys then a new key with _1, _2, ..., etc\n    appended will be returned, otherwise the key will be returned as passed.\n\n    Args:\n    ----\n      key: desired key name.\n      current_keys: List of current key names.\n      update_keys: True to append the new key to current_keys.\n    \"\"\"\n    index = 1\n    k = key\n    while k in current_keys:\n        k = \"%s_%d\" % (key, index)\n        index += 1\n    if update_keys:\n        current_keys.append(k)\n    return k\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.update_eval_config_with_defaults","title":"update_eval_config_with_defaults","text":"<pre><code>update_eval_config_with_defaults(\n    eval_config: EvalConfig,\n    maybe_add_baseline: Optional[bool] = None,\n    maybe_remove_baseline: Optional[bool] = None,\n    has_baseline: Optional[bool] = False,\n    rubber_stamp: Optional[bool] = False,\n) -&gt; EvalConfig\n</code></pre> <p>Returns a new config with default settings applied.</p> <p>a) Add or remove a model_spec according to \"has_baseline\". b) Fix the model names (model_spec.name) to tfma.CANDIDATE_KEY and    tfma.BASELINE_KEY. c) Update the metrics_specs with the fixed model name.</p> <p>eval_config: Original eval config.   maybe_add_baseline: DEPRECATED. True to add a baseline ModelSpec to the     config as a copy of the candidate ModelSpec that should already be     present. This is only applied if a single ModelSpec already exists in the     config and that spec doesn't have a name associated with it. When applied     the model specs will use the names tfma.CANDIDATE_KEY and     tfma.BASELINE_KEY. Only one of maybe_add_baseline or maybe_remove_baseline     should be used.   maybe_remove_baseline: DEPRECATED. True to remove a baseline ModelSpec from     the config if it already exists. Removal of the baseline also removes any     change thresholds. Only one of maybe_add_baseline or maybe_remove_baseline     should be used.   has_baseline: True to add a baseline ModelSpec to the config as a copy of     the candidate ModelSpec that should already be present. This is only     applied if a single ModelSpec already exists in the config and that spec     doesn't have a name associated with it. When applied the model specs will     use the names tfma.CANDIDATE_KEY and tfma.BASELINE_KEY. False to remove a     baseline ModelSpec from the config if it already exists. Removal of the     baseline also removes any change thresholds. Only one of has_baseline or     maybe_remove_baseline should be used.   rubber_stamp: True if this model is being rubber stamped. When a model is     rubber stamped diff thresholds will be ignored if an associated baseline     model is not passed.</p> <p>RuntimeError: on missing baseline model for non-rubberstamp cases.</p> Source code in <code>tensorflow_model_analysis/utils/config_util.py</code> <pre><code>def update_eval_config_with_defaults(\n    eval_config: config_pb2.EvalConfig,\n    maybe_add_baseline: Optional[bool] = None,\n    maybe_remove_baseline: Optional[bool] = None,\n    has_baseline: Optional[bool] = False,\n    rubber_stamp: Optional[bool] = False,\n) -&gt; config_pb2.EvalConfig:\n    \"\"\"Returns a new config with default settings applied.\n\n    a) Add or remove a model_spec according to \"has_baseline\".\n    b) Fix the model names (model_spec.name) to tfma.CANDIDATE_KEY and\n       tfma.BASELINE_KEY.\n    c) Update the metrics_specs with the fixed model name.\n\n    Args:\n    ----\n      eval_config: Original eval config.\n      maybe_add_baseline: DEPRECATED. True to add a baseline ModelSpec to the\n        config as a copy of the candidate ModelSpec that should already be\n        present. This is only applied if a single ModelSpec already exists in the\n        config and that spec doesn't have a name associated with it. When applied\n        the model specs will use the names tfma.CANDIDATE_KEY and\n        tfma.BASELINE_KEY. Only one of maybe_add_baseline or maybe_remove_baseline\n        should be used.\n      maybe_remove_baseline: DEPRECATED. True to remove a baseline ModelSpec from\n        the config if it already exists. Removal of the baseline also removes any\n        change thresholds. Only one of maybe_add_baseline or maybe_remove_baseline\n        should be used.\n      has_baseline: True to add a baseline ModelSpec to the config as a copy of\n        the candidate ModelSpec that should already be present. This is only\n        applied if a single ModelSpec already exists in the config and that spec\n        doesn't have a name associated with it. When applied the model specs will\n        use the names tfma.CANDIDATE_KEY and tfma.BASELINE_KEY. False to remove a\n        baseline ModelSpec from the config if it already exists. Removal of the\n        baseline also removes any change thresholds. Only one of has_baseline or\n        maybe_remove_baseline should be used.\n      rubber_stamp: True if this model is being rubber stamped. When a model is\n        rubber stamped diff thresholds will be ignored if an associated baseline\n        model is not passed.\n\n    Raises:\n    ------\n      RuntimeError: on missing baseline model for non-rubberstamp cases.\n    \"\"\"\n    if not has_baseline and has_change_threshold(eval_config) and not rubber_stamp:\n        # TODO(b/173657964): Raise an error instead of logging an error.\n        raise RuntimeError(\n            \"There are change thresholds, but the baseline is missing. \"\n            \"This is allowed only when rubber stamping (first run).\"\n        )\n\n    updated_config = config_pb2.EvalConfig()\n    updated_config.CopyFrom(eval_config)\n    # if user requests CIs but doesn't set method, use JACKKNIFE\n    if (\n        eval_config.options.compute_confidence_intervals.value\n        and eval_config.options.confidence_intervals.method\n        == config_pb2.ConfidenceIntervalOptions.UNKNOWN_CONFIDENCE_INTERVAL_METHOD\n    ):\n        updated_config.options.confidence_intervals.method = (\n            config_pb2.ConfidenceIntervalOptions.JACKKNIFE\n        )\n    if maybe_add_baseline and maybe_remove_baseline:\n        raise ValueError(\n            \"only one of maybe_add_baseline and maybe_remove_baseline \" \"should be used\"\n        )\n    if maybe_add_baseline or maybe_remove_baseline:\n        logging.warning(\n            \"\"\"\"maybe_add_baseline\" and \"maybe_remove_baseline\" are deprecated,\n        please use \"has_baseline\" instead.\"\"\"\n        )\n        if has_baseline:\n            raise ValueError(\n                \"\"\"\"maybe_add_baseline\" and \"maybe_remove_baseline\" are ignored if\n          \"has_baseline\" is set.\"\"\"\n            )\n    if has_baseline is not None:\n        if has_baseline:\n            maybe_add_baseline = True\n        else:\n            maybe_remove_baseline = True\n\n    # Has a baseline model.\n    if (\n        maybe_add_baseline\n        and len(updated_config.model_specs) == 1\n        and not updated_config.model_specs[0].name\n    ):\n        baseline = updated_config.model_specs.add()\n        baseline.CopyFrom(updated_config.model_specs[0])\n        baseline.name = constants.BASELINE_KEY\n        baseline.is_baseline = True\n        updated_config.model_specs[0].name = constants.CANDIDATE_KEY\n        logging.info(\n            \"Adding default baseline ModelSpec based on the candidate ModelSpec \"\n            'provided. The candidate model will be called \"%s\" and the baseline '\n            'will be called \"%s\": updated_config=\\n%s',\n            constants.CANDIDATE_KEY,\n            constants.BASELINE_KEY,\n            updated_config,\n        )\n\n    # Does not have a baseline.\n    if maybe_remove_baseline:\n        tmp_model_specs = []\n        for model_spec in updated_config.model_specs:\n            if not model_spec.is_baseline:\n                tmp_model_specs.append(model_spec)\n        del updated_config.model_specs[:]\n        updated_config.model_specs.extend(tmp_model_specs)\n        for metrics_spec in updated_config.metrics_specs:\n            for metric in metrics_spec.metrics:\n                if metric.threshold.ByteSize():\n                    metric.threshold.ClearField(\"change_threshold\")\n                for per_slice_threshold in metric.per_slice_thresholds:\n                    if per_slice_threshold.threshold.ByteSize():\n                        per_slice_threshold.threshold.ClearField(\"change_threshold\")\n                for cross_slice_threshold in metric.cross_slice_thresholds:\n                    if cross_slice_threshold.threshold.ByteSize():\n                        cross_slice_threshold.threshold.ClearField(\"change_threshold\")\n            for threshold in metrics_spec.thresholds.values():\n                if threshold.ByteSize():\n                    threshold.ClearField(\"change_threshold\")\n            for per_slice_thresholds in metrics_spec.per_slice_thresholds.values():\n                for per_slice_threshold in per_slice_thresholds.thresholds:\n                    if per_slice_threshold.threshold.ByteSize():\n                        per_slice_threshold.threshold.ClearField(\"change_threshold\")\n            for cross_slice_thresholds in metrics_spec.cross_slice_thresholds.values():\n                for cross_slice_threshold in cross_slice_thresholds.thresholds:\n                    if cross_slice_threshold.threshold.ByteSize():\n                        cross_slice_threshold.threshold.ClearField(\"change_threshold\")\n        logging.info(\n            \"Request was made to ignore the baseline ModelSpec and any change \"\n            \"thresholds. This is likely because a baseline model was not provided: \"\n            \"updated_config=\\n%s\",\n            updated_config,\n        )\n\n    if not updated_config.model_specs:\n        updated_config.model_specs.add()\n\n    model_names = []\n    for spec in updated_config.model_specs:\n        model_names.append(spec.name)\n    if len(model_names) == 1 and model_names[0]:\n        logging.info(\n            'ModelSpec name \"%s\" is being ignored and replaced by \"\" because a '\n            \"single ModelSpec is being used\",\n            model_names[0],\n        )\n        updated_config.model_specs[0].name = \"\"\n        model_names = [\"\"]\n    for spec in updated_config.metrics_specs:\n        if not spec.model_names:\n            spec.model_names.extend(model_names)\n        elif len(model_names) == 1:\n            del spec.model_names[:]\n            spec.model_names.append(\"\")\n\n    return updated_config\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.verify_and_update_eval_shared_models","title":"verify_and_update_eval_shared_models","text":"<pre><code>verify_and_update_eval_shared_models(\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ],\n) -&gt; Optional[List[EvalSharedModel]]\n</code></pre> <p>Verifies eval shared models and normnalizes to produce a single list.</p> <p>The output is normalized such that if a list or dict contains a single entry, the model name will always be empty.</p> <p>eval_shared_model: None, a single model, a list of models, or a dict of     models keyed by model name.</p> <p>A list of models or None.</p> <p>ValueError if dict is passed and keys don't match model names or a   multi-item list is passed without model names.</p> Source code in <code>tensorflow_model_analysis/utils/model_util.py</code> <pre><code>def verify_and_update_eval_shared_models(\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels],\n) -&gt; Optional[List[types.EvalSharedModel]]:\n    \"\"\"Verifies eval shared models and normnalizes to produce a single list.\n\n    The output is normalized such that if a list or dict contains a single entry,\n    the model name will always be empty.\n\n    Args:\n    ----\n      eval_shared_model: None, a single model, a list of models, or a dict of\n        models keyed by model name.\n\n    Returns:\n    -------\n      A list of models or None.\n\n    Raises:\n    ------\n      ValueError if dict is passed and keys don't match model names or a\n      multi-item list is passed without model names.\n    \"\"\"\n    if not eval_shared_model:\n        return None\n    eval_shared_models = []\n    if isinstance(eval_shared_model, dict):\n        for k, v in eval_shared_model.items():\n            if v.model_name and k and k != v.model_name:\n                raise ValueError(\n                    \"keys for EvalSharedModel dict do not match \"\n                    f\"model_names: dict={eval_shared_model}\"\n                )\n            if not v.model_name and k:\n                v = v._replace(model_name=k)\n            eval_shared_models.append(v)\n    elif isinstance(eval_shared_model, list):\n        # Ensure we don't modify the input list when updating model_name, below.\n        eval_shared_models = eval_shared_model.copy()\n    else:\n        eval_shared_models = [eval_shared_model]\n    if len(eval_shared_models) &gt; 1:\n        for v in eval_shared_models:\n            if not v.model_name:\n                raise ValueError(\n                    \"model_name is required when passing multiple EvalSharedModels: \"\n                    f\"eval_shared_models={eval_shared_models}\"\n                )\n    # To maintain consistency between settings where single models are used,\n    # always use '' as the model name regardless of whether a name is passed.\n    elif len(eval_shared_models) == 1 and eval_shared_models[0].model_name:\n        eval_shared_models[0] = eval_shared_models[0]._replace(model_name=\"\")\n    # Normalizes model types to TFMA_EVAL when appropriate.\n    for i, model in enumerate(eval_shared_models):\n        assert isinstance(model, types.EvalSharedModel)\n    return eval_shared_models  # pytype: disable=bad-return-type  # py310-upgrade\n</code></pre>"},{"location":"api_docs/python/tfma-utils/#tensorflow_model_analysis.utils.verify_eval_config","title":"verify_eval_config","text":"<pre><code>verify_eval_config(\n    eval_config: EvalConfig,\n    baseline_required: Optional[bool] = None,\n)\n</code></pre> <p>Verifies eval config.</p> Source code in <code>tensorflow_model_analysis/utils/config_util.py</code> <pre><code>def verify_eval_config(\n    eval_config: config_pb2.EvalConfig, baseline_required: Optional[bool] = None\n):\n    \"\"\"Verifies eval config.\"\"\"\n    if not eval_config.model_specs:\n        raise ValueError(\n            f\"At least one model_spec is required: eval_config=\\n{eval_config}\"\n        )\n\n    model_specs_by_name = {}\n    baseline = None\n    for spec in eval_config.model_specs:\n        if spec.label_key and spec.label_keys:\n            raise ValueError(\n                \"only one of label_key or label_keys should be used at \"\n                f\"a time: model_spec=\\n{spec}\"\n            )\n        if spec.prediction_key and spec.prediction_keys:\n            raise ValueError(\n                \"only one of prediction_key or prediction_keys should be used at \"\n                f\"a time: model_spec=\\n{spec}\"\n            )\n        if spec.example_weight_key and spec.example_weight_keys:\n            raise ValueError(\n                \"only one of example_weight_key or example_weight_keys should be \"\n                f\"used at a time: model_spec=\\n{spec}\"\n            )\n        if spec.name in eval_config.model_specs:\n            raise ValueError(\n                f'more than one model_spec found for model \"{spec.name}\": {[spec, model_specs_by_name[spec.name]]}'\n            )\n        model_specs_by_name[spec.name] = spec\n        if spec.is_baseline:\n            if baseline is not None:\n                raise ValueError(\n                    \"only one model_spec may be a baseline, found: \"\n                    f\"{spec} and {baseline}\"\n                )\n            baseline = spec\n\n    if len(model_specs_by_name) &gt; 1 and \"\" in model_specs_by_name:\n        raise ValueError(\n            \"A name is required for all ModelSpecs when multiple \"\n            f\"models are used: eval_config=\\n{eval_config}\"\n        )\n\n    if baseline_required and not baseline:\n        raise ValueError(\n            f\"A baseline ModelSpec is required: eval_config=\\n{eval_config}\"\n        )\n\n    # Raise exception if per_slice_thresholds has no slicing_specs.\n    for metric_spec in eval_config.metrics_specs:\n        for name, per_slice_thresholds in metric_spec.per_slice_thresholds.items():\n            for per_slice_threshold in per_slice_thresholds.thresholds:\n                if not per_slice_threshold.slicing_specs:\n                    raise ValueError(\n                        \"slicing_specs must be set on per_slice_thresholds but found \"\n                        f\"per_slice_threshold=\\n{per_slice_threshold}\\n\"\n                        f\"for metric name {name} in metric_spec:\\n{metric_spec}\"\n                    )\n        for metric_config in metric_spec.metrics:\n            for per_slice_threshold in metric_config.per_slice_thresholds:\n                if not per_slice_threshold.slicing_specs:\n                    raise ValueError(\n                        \"slicing_specs must be set on per_slice_thresholds but found \"\n                        f\"per_slice_threshold=\\n{per_slice_threshold}\\n\"\n                        f\"for metric config:\\t{metric_config}\"\n                    )\n</code></pre>"},{"location":"api_docs/python/tfma-validators/","title":"TFMA Validators","text":""},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators","title":"tensorflow_model_analysis.validators","text":"<p>Init module for TensorFlow Model Analysis validators.</p>"},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators.Validation","title":"Validation  <code>module-attribute</code>","text":"<pre><code>Validation = Dict[str, PCollection]\n</code></pre>"},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators.Validator","title":"Validator","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators.Validator-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators.Validator.ptransform","title":"ptransform  <code>instance-attribute</code>","text":"<pre><code>ptransform: PTransform\n</code></pre>"},{"location":"api_docs/python/tfma-validators/#tensorflow_model_analysis.validators.Validator.stage_name","title":"stage_name  <code>instance-attribute</code>","text":"<pre><code>stage_name: str\n</code></pre>"},{"location":"api_docs/python/tfma-version/","title":"TFMA Version","text":""},{"location":"api_docs/python/tfma-version/#tensorflow_model_analysis.version","title":"tensorflow_model_analysis.version","text":"<p>Contains the version string for this release of TFMA.</p>"},{"location":"api_docs/python/tfma-version/#tensorflow_model_analysis.version-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-version/#tensorflow_model_analysis.version.VERSION","title":"VERSION  <code>module-attribute</code>","text":"<pre><code>VERSION = '0.49.0.dev'\n</code></pre>"},{"location":"api_docs/python/tfma-view/","title":"TFMA View","text":""},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view","title":"tensorflow_model_analysis.view","text":"<p>Initializes TFMA's view rendering api.</p>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.SlicedMetrics","title":"SlicedMetrics","text":"<p>               Bases: <code>NamedTuple('SlicedMetrics', [('slice', SliceKeyType), ('metrics', MetricsByOutputName)])</code></p> <p>A tuple containing the metrics belonging to a slice.</p> <p>The metrics are stored in a nested dictionary with the following levels:</p> <ol> <li>output_name: Optional output name associated with metric (for multi-output  models). '' by default.</li> <li>sub_key: Optional sub key associated with metric (for multi-class models).  '' by default. See <code>tfma.metrics.SubKey</code> for more info.</li> <li>metric_name: Name of the metric (<code>auc</code>, <code>accuracy</code>, etc).</li> <li>metric_value: A dictionary containing the metric's value. See  <code>tfma.proto.metrics_for_slice_pb2.MetricValue</code>  for more info.</li> </ol> <p>Below is a sample SlicedMetrics:</p> <pre><code>(\n  (('color', 'green')),\n  {\n    '': {  # default for single-output models\n      '': {  # default sub_key for non-multiclass-classification models\n        'auc': {\n          'doubleValue': 0.7243943810462952\n        },\n        'accuracy': {\n          'doubleValue': 0.6488351225852966\n        }\n      }\n    }\n  }\n)\n</code></pre>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.SlicedMetrics--attributes","title":"Attributes","text":"<p>slice: A 2-element tuple representing a slice. The first element is the key     of a feature (ex: 'color'), and the second element is the value (ex:       'green'). An empty tuple represents an 'overall' slice (i.e. one that       encompasses the entire dataset.   metrics: A nested dictionary containing metric names and values.</p>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.SlicedPlots","title":"SlicedPlots","text":"<p>               Bases: <code>NamedTuple('SlicedPlots', [('slice', SliceKeyType), ('plot', PlotsByOutputName)])</code></p> <p>A tuple containing the plots belonging to a slice.</p>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.SlicedPlots--attributes","title":"Attributes","text":"<p>slice: A 2-element tuple representing a slice. The first element is the key     of a feature (ex: 'color'), and the second element is the value (ex:       'green'). An empty tuple represents an 'overall' slice (i.e. one that       encompasses the entire dataset.   plot: A dict mapping <code>output_name</code> and <code>sub_key_id</code> to plot data. The data     contains histograms and confusion matrices, which can be rendered with the     <code>tfma.view.render_plot</code> function.</p>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.render_plot","title":"render_plot","text":"<pre><code>render_plot(\n    result: EvalResult,\n    slicing_spec: Optional[\n        Union[SingleSliceSpec, SlicingSpec]\n    ] = None,\n    output_name: Optional[str] = None,\n    class_id: Optional[int] = None,\n    top_k: Optional[int] = None,\n    k: Optional[int] = None,\n    label: Optional[str] = None,\n) -&gt; Optional[PlotViewer]\n</code></pre> <p>Renders the plot view as widget.</p> <p>result: An tfma.EvalResult.   slicing_spec: The tfma.SlicingSpec to identify the slice. Show overall if     unset.   output_name: A string representing the output name.   class_id: A number representing the class id if multi class.   top_k: The k used to compute prediction in the top k position.   k: The k used to compute prediciton at the kth position.   label: A partial label used to match a set of plots in the results.</p> <p>A PlotViewer object if in Jupyter notebook; None if in Colab.</p> Source code in <code>tensorflow_model_analysis/view/widget_view.py</code> <pre><code>def render_plot(\n    result: view_types.EvalResult,\n    slicing_spec: Optional[\n        Union[slicer.SingleSliceSpec, config_pb2.SlicingSpec]\n    ] = None,\n    output_name: Optional[str] = None,\n    class_id: Optional[int] = None,\n    top_k: Optional[int] = None,\n    k: Optional[int] = None,\n    label: Optional[str] = None,\n) -&gt; Optional[visualization.PlotViewer]:  # pytype: disable=invalid-annotation\n    \"\"\"Renders the plot view as widget.\n\n    Args:\n    ----\n      result: An tfma.EvalResult.\n      slicing_spec: The tfma.SlicingSpec to identify the slice. Show overall if\n        unset.\n      output_name: A string representing the output name.\n      class_id: A number representing the class id if multi class.\n      top_k: The k used to compute prediction in the top k position.\n      k: The k used to compute prediciton at the kth position.\n      label: A partial label used to match a set of plots in the results.\n\n    Returns:\n    -------\n      A PlotViewer object if in Jupyter notebook; None if in Colab.\n    \"\"\"\n    if slicing_spec and isinstance(slicing_spec, config_pb2.SlicingSpec):\n        slicing_spec = slicer.SingleSliceSpec(spec=slicing_spec)\n    slice_spec_to_use = slicing_spec if slicing_spec else slicer.SingleSliceSpec()\n    data, cfg = util.get_plot_data_and_config(\n        result.plots, slice_spec_to_use, output_name, class_id, top_k, k, label\n    )\n    return visualization.render_plot(data, cfg)\n</code></pre>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.render_slicing_attributions","title":"render_slicing_attributions","text":"<pre><code>render_slicing_attributions(\n    result: EvalResult,\n    slicing_column: Optional[str] = None,\n    slicing_spec: Optional[\n        Union[SingleSliceSpec, SlicingSpec]\n    ] = None,\n    metric_name: Optional[str] = None,\n    weighted_example_column: Optional[str] = None,\n    event_handlers: Optional[\n        Callable[[Dict[str, Union[str, float]]], None]\n    ] = None,\n) -&gt; Optional[SlicingMetricsViewer]\n</code></pre> <p>Renders the slicing metrics view as widget.</p> <p>result: An tfma.EvalResult.   slicing_column: The column to slice on.   slicing_spec: The tfma.SlicingSpec to filter results. If neither column nor     spec is set, show overall.   metric_name: Name of attributions metric to show attributions for. Optional     if only one metric used.   weighted_example_column: Override for the weighted example column. This can     be used when different weights are applied in different aprts of the model     (eg: multi-head).   event_handlers: The event handlers</p> <p>A SlicingMetricsViewer object if in Jupyter notebook; None if in Colab.</p> Source code in <code>tensorflow_model_analysis/view/widget_view.py</code> <pre><code>def render_slicing_attributions(\n    result: view_types.EvalResult,\n    slicing_column: Optional[str] = None,\n    slicing_spec: Optional[\n        Union[slicer.SingleSliceSpec, config_pb2.SlicingSpec]\n    ] = None,\n    metric_name: Optional[str] = None,\n    weighted_example_column: Optional[str] = None,\n    event_handlers: Optional[Callable[[Dict[str, Union[str, float]]], None]] = None,\n) -&gt; Optional[visualization.SlicingMetricsViewer]:  # pytype: disable=invalid-annotation\n    \"\"\"Renders the slicing metrics view as widget.\n\n    Args:\n    ----\n      result: An tfma.EvalResult.\n      slicing_column: The column to slice on.\n      slicing_spec: The tfma.SlicingSpec to filter results. If neither column nor\n        spec is set, show overall.\n      metric_name: Name of attributions metric to show attributions for. Optional\n        if only one metric used.\n      weighted_example_column: Override for the weighted example column. This can\n        be used when different weights are applied in different aprts of the model\n        (eg: multi-head).\n      event_handlers: The event handlers\n\n    Returns:\n    -------\n      A SlicingMetricsViewer object if in Jupyter notebook; None if in Colab.\n    \"\"\"\n    if slicing_spec and isinstance(slicing_spec, config_pb2.SlicingSpec):\n        slicing_spec = slicer.SingleSliceSpec(spec=slicing_spec)\n    data = util.get_slicing_metrics(result.attributions, slicing_column, slicing_spec)\n    # Attributions have one additional level of indirection for the metric_name.\n    # Filter this out using the metric_name provided.\n    for d in data:\n        updated_data = {}\n        for output_name, per_output_items in d[\n            \"metrics\"\n        ].items():  # pytype: disable=attribute-error\n            updated_data[output_name] = {}\n            for sub_key, per_sub_key_items in per_output_items.items():\n                updated_data[output_name][sub_key] = {}\n                if metric_name:\n                    if metric_name not in per_sub_key_items:\n                        raise ValueError(\n                            f\"metric_name={metric_name} not found in {per_sub_key_items.keys()}\"\n                        )\n                    updated_data[output_name][sub_key] = per_sub_key_items[metric_name]\n                elif len(per_sub_key_items) == 1:\n                    updated_data[output_name][sub_key] = list(\n                        per_sub_key_items.values()\n                    )[0]\n                else:\n                    raise ValueError(\n                        f\"metric_name must be one of the following: {per_sub_key_items.keys()}\"\n                    )\n        d[\"metrics\"] = updated_data\n\n    cfg = util.get_slicing_config(result.config, weighted_example_column)\n\n    return visualization.render_slicing_metrics(\n        data, cfg, event_handlers=event_handlers\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.render_slicing_metrics","title":"render_slicing_metrics","text":"<pre><code>render_slicing_metrics(\n    result: EvalResult,\n    slicing_column: Optional[str] = None,\n    slicing_spec: Optional[\n        Union[SingleSliceSpec, SlicingSpec]\n    ] = None,\n    weighted_example_column: Optional[str] = None,\n    event_handlers: Optional[\n        Callable[[Dict[str, Union[str, float]]], None]\n    ] = None,\n) -&gt; Optional[SlicingMetricsViewer]\n</code></pre> <p>Renders the slicing metrics view as widget.</p> <p>result: An tfma.EvalResult.   slicing_column: The column to slice on.   slicing_spec: The tfma.SlicingSpec to filter results. If neither column nor     spec is set, show overall.   weighted_example_column: Override for the weighted example column. This can     be used when different weights are applied in different aprts of the model     (eg: multi-head).   event_handlers: The event handlers</p> <p>A SlicingMetricsViewer object if in Jupyter notebook; None if in Colab.</p> Source code in <code>tensorflow_model_analysis/view/widget_view.py</code> <pre><code>def render_slicing_metrics(\n    result: view_types.EvalResult,\n    slicing_column: Optional[str] = None,\n    slicing_spec: Optional[\n        Union[slicer.SingleSliceSpec, config_pb2.SlicingSpec]\n    ] = None,\n    weighted_example_column: Optional[str] = None,\n    event_handlers: Optional[Callable[[Dict[str, Union[str, float]]], None]] = None,\n) -&gt; Optional[visualization.SlicingMetricsViewer]:  # pytype: disable=invalid-annotation\n    \"\"\"Renders the slicing metrics view as widget.\n\n    Args:\n    ----\n      result: An tfma.EvalResult.\n      slicing_column: The column to slice on.\n      slicing_spec: The tfma.SlicingSpec to filter results. If neither column nor\n        spec is set, show overall.\n      weighted_example_column: Override for the weighted example column. This can\n        be used when different weights are applied in different aprts of the model\n        (eg: multi-head).\n      event_handlers: The event handlers\n\n    Returns:\n    -------\n      A SlicingMetricsViewer object if in Jupyter notebook; None if in Colab.\n    \"\"\"\n    if slicing_spec and isinstance(slicing_spec, config_pb2.SlicingSpec):\n        slicing_spec = slicer.SingleSliceSpec(spec=slicing_spec)\n    data = util.get_slicing_metrics(\n        result.slicing_metrics, slicing_column, slicing_spec\n    )\n    cfg = util.get_slicing_config(result.config, weighted_example_column)\n\n    return visualization.render_slicing_metrics(\n        data, cfg, event_handlers=event_handlers\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-view/#tensorflow_model_analysis.view.render_time_series","title":"render_time_series","text":"<pre><code>render_time_series(\n    results: EvalResults,\n    slicing_spec: Optional[\n        Union[SingleSliceSpec, SlicingSpec]\n    ] = None,\n    display_full_path: bool = False,\n) -&gt; Optional[TimeSeriesViewer]\n</code></pre> <p>Renders the time series view as widget.</p> <p>results: An tfma.EvalResults.   slicing_spec: A tfma.SlicingSpec determining the slice to show time series     on. Show overall if not set.   display_full_path: Whether to display the full path to model / data in the     visualization or just show file name.</p> <p>A TimeSeriesViewer object if in Jupyter notebook; None if in Colab.</p> Source code in <code>tensorflow_model_analysis/view/widget_view.py</code> <pre><code>def render_time_series(\n    results: view_types.EvalResults,\n    slicing_spec: Optional[\n        Union[slicer.SingleSliceSpec, config_pb2.SlicingSpec]\n    ] = None,\n    display_full_path: bool = False,\n) -&gt; Optional[visualization.TimeSeriesViewer]:  # pytype: disable=invalid-annotation\n    \"\"\"Renders the time series view as widget.\n\n    Args:\n    ----\n      results: An tfma.EvalResults.\n      slicing_spec: A tfma.SlicingSpec determining the slice to show time series\n        on. Show overall if not set.\n      display_full_path: Whether to display the full path to model / data in the\n        visualization or just show file name.\n\n    Returns:\n    -------\n      A TimeSeriesViewer object if in Jupyter notebook; None if in Colab.\n    \"\"\"\n    if slicing_spec and isinstance(slicing_spec, config_pb2.SlicingSpec):\n        slicing_spec = slicer.SingleSliceSpec(spec=slicing_spec)\n    slice_spec_to_use = slicing_spec if slicing_spec else slicer.SingleSliceSpec()\n    data = util.get_time_series(results, slice_spec_to_use, display_full_path)\n    cfg = {\"isModelCentric\": results.get_mode() == constants.MODEL_CENTRIC_MODE}\n\n    return visualization.render_time_series(data, cfg)\n</code></pre>"},{"location":"api_docs/python/tfma-writers/","title":"TFMA Writers","text":""},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers","title":"tensorflow_model_analysis.writers","text":"<p>Init module for TensorFlow Model Analysis writers.</p>"},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.Writer","title":"Writer","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.Writer-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.Writer.ptransform","title":"ptransform  <code>instance-attribute</code>","text":"<pre><code>ptransform: PTransform\n</code></pre>"},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.Writer.stage_name","title":"stage_name  <code>instance-attribute</code>","text":"<pre><code>stage_name: str\n</code></pre>"},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.EvalConfigWriter","title":"EvalConfigWriter","text":"<pre><code>EvalConfigWriter(\n    output_path: str,\n    eval_config: EvalConfig,\n    output_file_format: str = EVAL_CONFIG_FILE_FORMAT,\n    data_location: Optional[str] = None,\n    data_file_format: Optional[str] = None,\n    model_locations: Optional[Dict[str, str]] = None,\n    filename: Optional[str] = None,\n) -&gt; Writer\n</code></pre> <p>Returns eval config writer.</p> <p>output_path: Output path to write config to.   eval_config: EvalConfig.   output_file_format: Output file format. Currently on 'json' is supported.   data_location: Optional path indicating where data is read from. This is     only used for display purposes.   data_file_format: Optional format of the input examples. This is only used     for display purposes.   model_locations: Dict of model locations keyed by model name. This is only     used for display purposes.   filename: Name of file to store the config as.</p> Source code in <code>tensorflow_model_analysis/writers/eval_config_writer.py</code> <pre><code>def EvalConfigWriter(  # pylint: disable=invalid-name\n    output_path: str,\n    eval_config: config_pb2.EvalConfig,\n    output_file_format: str = EVAL_CONFIG_FILE_FORMAT,\n    data_location: Optional[str] = None,\n    data_file_format: Optional[str] = None,\n    model_locations: Optional[Dict[str, str]] = None,\n    filename: Optional[str] = None,\n) -&gt; writer.Writer:\n    \"\"\"Returns eval config writer.\n\n    Args:\n    ----\n      output_path: Output path to write config to.\n      eval_config: EvalConfig.\n      output_file_format: Output file format. Currently on 'json' is supported.\n      data_location: Optional path indicating where data is read from. This is\n        only used for display purposes.\n      data_file_format: Optional format of the input examples. This is only used\n        for display purposes.\n      model_locations: Dict of model locations keyed by model name. This is only\n        used for display purposes.\n      filename: Name of file to store the config as.\n    \"\"\"\n    if data_location is None:\n        data_location = \"&lt;user provided PCollection&gt;\"\n    if data_file_format is None:\n        data_file_format = \"&lt;unknown&gt;\"\n    if model_locations is None:\n        model_locations = {\"\": \"&lt;unknown&gt;\"}\n    if filename is None:\n        filename = EVAL_CONFIG_FILE + \".\" + output_file_format\n\n    return writer.Writer(\n        stage_name=\"WriteEvalConfig\",\n        ptransform=_WriteEvalConfig(  # pylint: disable=no-value-for-parameter\n            eval_config=eval_config,\n            output_path=output_path,\n            output_file_format=output_file_format,\n            data_location=data_location,\n            data_file_format=data_file_format,\n            model_locations=model_locations,\n            filename=filename,\n        ),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.MetricsPlotsAndValidationsWriter","title":"MetricsPlotsAndValidationsWriter","text":"<pre><code>MetricsPlotsAndValidationsWriter(\n    output_paths: Dict[str, str],\n    eval_config: EvalConfig,\n    add_metrics_callbacks: Optional[\n        List[AddMetricsCallbackType]\n    ] = None,\n    metrics_key: str = METRICS_KEY,\n    plots_key: str = PLOTS_KEY,\n    attributions_key: str = ATTRIBUTIONS_KEY,\n    validations_key: str = VALIDATIONS_KEY,\n    output_file_format: str = _TFRECORD_FORMAT,\n    rubber_stamp: Optional[bool] = False,\n    stage_name: str = METRICS_PLOTS_AND_VALIDATIONS_WRITER_STAGE_NAME,\n) -&gt; Writer\n</code></pre> <p>Returns metrics and plots writer.</p> <p>Note, sharding will be enabled by default if a output_file_format is provided. The files will be named -SSSSS-of-NNNNN. where SSSSS is the shard number and NNNNN is the number of shards. <p>output_paths: Output paths keyed by output key (e.g. 'metrics', 'plots',     'validation').   eval_config: Eval config.   add_metrics_callbacks: Optional list of metric callbacks (if used).   metrics_key: Name to use for metrics key in Evaluation output.   plots_key: Name to use for plots key in Evaluation output.   attributions_key: Name to use for attributions key in Evaluation output.   validations_key: Name to use for validations key in Evaluation output.   output_file_format: File format to use when saving files. Currently     'tfrecord' and 'parquet' are supported and 'tfrecord is the default'.     If using parquet, the output metrics and plots files will contain two     columns, 'slice_key' and 'serialized_value'. The 'slice_key' column will     be a structured column matching the metrics_for_slice_pb2.SliceKey proto.     The 'serialized_value' column will contain a serialized MetricsForSlice or     PlotsForSlice proto. The validation result file will contain a single     column 'serialized_value' which will contain a single serialized     ValidationResult proto.   rubber_stamp: True if this model is being rubber stamped. When a model is     rubber stamped diff thresholds will be ignored if an associated baseline     model is not passed.   stage_name: The stage name to use when this writer is added to the Beam     pipeline.</p> Source code in <code>tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py</code> <pre><code>def MetricsPlotsAndValidationsWriter(  # pylint: disable=invalid-name\n    output_paths: Dict[str, str],\n    eval_config: config_pb2.EvalConfig,\n    add_metrics_callbacks: Optional[List[types.AddMetricsCallbackType]] = None,\n    metrics_key: str = constants.METRICS_KEY,\n    plots_key: str = constants.PLOTS_KEY,\n    attributions_key: str = constants.ATTRIBUTIONS_KEY,\n    validations_key: str = constants.VALIDATIONS_KEY,\n    output_file_format: str = _TFRECORD_FORMAT,\n    rubber_stamp: Optional[bool] = False,\n    stage_name: str = METRICS_PLOTS_AND_VALIDATIONS_WRITER_STAGE_NAME,\n) -&gt; writer.Writer:\n    \"\"\"Returns metrics and plots writer.\n\n    Note, sharding will be enabled by default if a output_file_format is provided.\n    The files will be named &lt;output_path&gt;-SSSSS-of-NNNNN.&lt;output_file_format&gt;\n    where SSSSS is the shard number and NNNNN is the number of shards.\n\n    Args:\n    ----\n      output_paths: Output paths keyed by output key (e.g. 'metrics', 'plots',\n        'validation').\n      eval_config: Eval config.\n      add_metrics_callbacks: Optional list of metric callbacks (if used).\n      metrics_key: Name to use for metrics key in Evaluation output.\n      plots_key: Name to use for plots key in Evaluation output.\n      attributions_key: Name to use for attributions key in Evaluation output.\n      validations_key: Name to use for validations key in Evaluation output.\n      output_file_format: File format to use when saving files. Currently\n        'tfrecord' and 'parquet' are supported and 'tfrecord is the default'.\n        If using parquet, the output metrics and plots files will contain two\n        columns, 'slice_key' and 'serialized_value'. The 'slice_key' column will\n        be a structured column matching the metrics_for_slice_pb2.SliceKey proto.\n        The 'serialized_value' column will contain a serialized MetricsForSlice or\n        PlotsForSlice proto. The validation result file will contain a single\n        column 'serialized_value' which will contain a single serialized\n        ValidationResult proto.\n      rubber_stamp: True if this model is being rubber stamped. When a model is\n        rubber stamped diff thresholds will be ignored if an associated baseline\n        model is not passed.\n      stage_name: The stage name to use when this writer is added to the Beam\n        pipeline.\n    \"\"\"\n    return writer.Writer(\n        stage_name=stage_name,\n        ptransform=_WriteMetricsPlotsAndValidations(  # pylint: disable=no-value-for-parameter\n            output_paths=output_paths,\n            eval_config=eval_config,\n            add_metrics_callbacks=add_metrics_callbacks or [],\n            metrics_key=metrics_key,\n            plots_key=plots_key,\n            attributions_key=attributions_key,\n            validations_key=validations_key,\n            output_file_format=output_file_format,\n            rubber_stamp=rubber_stamp,\n        ),\n    )\n</code></pre>"},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.Write","title":"Write","text":"<pre><code>Write(\n    evaluation_or_validation: Union[Evaluation, Validation],\n    key: str,\n    ptransform: PTransform,\n) -&gt; Optional[PCollection]\n</code></pre> <p>Writes given Evaluation or Validation data using given writer PTransform.</p> <p>evaluation_or_validation: Evaluation or Validation data.   key: Key for Evaluation or Validation output to write. It is valid for the     key to not exist in the dict (in which case the write is a no-op).   ptransform: PTransform to use for writing.</p> <p>ValueError: If Evaluation or Validation is empty. The key does not need to     exist in the Evaluation or Validation, but the dict must not be empty.</p> <p>The result of the underlying beam write PTransform. This makes it possible   for interactive environments to execute your writer, as well as for   downstream Beam stages to make use of the files that are written.</p> Source code in <code>tensorflow_model_analysis/writers/writer.py</code> <pre><code>@beam.ptransform_fn\ndef Write(\n    evaluation_or_validation: Union[evaluator.Evaluation, validator.Validation],\n    key: str,\n    ptransform: beam.PTransform,\n) -&gt; Optional[beam.PCollection]:\n    \"\"\"Writes given Evaluation or Validation data using given writer PTransform.\n\n    Args:\n    ----\n      evaluation_or_validation: Evaluation or Validation data.\n      key: Key for Evaluation or Validation output to write. It is valid for the\n        key to not exist in the dict (in which case the write is a no-op).\n      ptransform: PTransform to use for writing.\n\n    Raises:\n    ------\n      ValueError: If Evaluation or Validation is empty. The key does not need to\n        exist in the Evaluation or Validation, but the dict must not be empty.\n\n    Returns:\n    -------\n      The result of the underlying beam write PTransform. This makes it possible\n      for interactive environments to execute your writer, as well as for\n      downstream Beam stages to make use of the files that are written.\n    \"\"\"\n    if not evaluation_or_validation:\n        raise ValueError(\"Evaluations and Validations cannot be empty\")\n    if key in evaluation_or_validation:\n        return evaluation_or_validation[key] | ptransform\n    return None\n</code></pre>"},{"location":"api_docs/python/tfma-writers/#tensorflow_model_analysis.writers.convert_slice_metrics_to_proto","title":"convert_slice_metrics_to_proto","text":"<pre><code>convert_slice_metrics_to_proto(\n    metrics: Tuple[\n        SliceKeyOrCrossSliceKeyType, MetricsDict\n    ],\n    add_metrics_callbacks: Optional[\n        List[AddMetricsCallbackType]\n    ],\n) -&gt; MetricsForSlice\n</code></pre> <p>Converts the given slice metrics into serialized proto MetricsForSlice.</p> <p>metrics: The slice metrics.   add_metrics_callbacks: A list of metric callbacks. This should be the same     list as the one passed to tfma.Evaluate().</p> <p>The MetricsForSlice proto.</p> <p>TypeError: If the type of the feature value in slice key cannot be     recognized.</p> Source code in <code>tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py</code> <pre><code>def convert_slice_metrics_to_proto(\n    metrics: Tuple[slicer.SliceKeyOrCrossSliceKeyType, metric_types.MetricsDict],\n    add_metrics_callbacks: Optional[List[types.AddMetricsCallbackType]],\n) -&gt; metrics_for_slice_pb2.MetricsForSlice:\n    \"\"\"Converts the given slice metrics into serialized proto MetricsForSlice.\n\n    Args:\n    ----\n      metrics: The slice metrics.\n      add_metrics_callbacks: A list of metric callbacks. This should be the same\n        list as the one passed to tfma.Evaluate().\n\n    Returns:\n    -------\n      The MetricsForSlice proto.\n\n    Raises:\n    ------\n      TypeError: If the type of the feature value in slice key cannot be\n        recognized.\n    \"\"\"\n    result = metrics_for_slice_pb2.MetricsForSlice()\n    slice_key, slice_metrics = metrics\n\n    if slicer.is_cross_slice_key(slice_key):\n        result.cross_slice_key.CopyFrom(slicer.serialize_cross_slice_key(slice_key))\n    else:\n        result.slice_key.CopyFrom(slicer.serialize_slice_key(slice_key))\n\n    slice_metrics = slice_metrics.copy()\n\n    if metric_keys.ERROR_METRIC in slice_metrics:\n        logging.warning(\n            \"Error for slice: %s with error message: %s \",\n            slice_key,\n            slice_metrics[metric_keys.ERROR_METRIC],\n        )\n        result.metrics[metric_keys.ERROR_METRIC].debug_message = slice_metrics[\n            metric_keys.ERROR_METRIC\n        ]\n        return result\n\n    # Convert the metrics from add_metrics_callbacks to the structured output if\n    # defined.\n    if add_metrics_callbacks and (\n        not any(isinstance(k, metric_types.MetricKey) for k in slice_metrics)\n    ):\n        for add_metrics_callback in add_metrics_callbacks:\n            if hasattr(add_metrics_callback, \"populate_stats_and_pop\"):\n                add_metrics_callback.populate_stats_and_pop(\n                    slice_key, slice_metrics, result.metrics\n                )\n    for key in sorted(slice_metrics):\n        value = slice_metrics[key]\n        if isinstance(value, types.ValueWithTDistribution):\n            unsampled_value = value.unsampled_value\n            _, lower_bound, upper_bound = math_util.calculate_confidence_interval(value)\n            confidence_interval = metrics_for_slice_pb2.ConfidenceInterval(\n                lower_bound=convert_metric_value_to_proto(lower_bound),\n                upper_bound=convert_metric_value_to_proto(upper_bound),\n                standard_error=convert_metric_value_to_proto(\n                    value.sample_standard_deviation\n                ),\n                degrees_of_freedom={\"value\": value.sample_degrees_of_freedom},\n            )\n            metric_value = convert_metric_value_to_proto(unsampled_value)\n            if isinstance(key, metric_types.MetricKey):\n                result.metric_keys_and_values.add(\n                    key=key.to_proto(),\n                    value=metric_value,\n                    confidence_interval=confidence_interval,\n                )\n            else:\n                # For v1 we continue to populate bounded_value for backwards\n                # compatibility. If metric can be stored to double_value metrics,\n                # replace it with a bounded_value.\n                # TODO(b/171992041): remove the string-typed metric key branch once v1\n                # code is removed.\n                if metric_value.WhichOneof(\"type\") == \"double_value\":\n                    # setting bounded_value clears double_value in the same oneof scope.\n                    metric_value.bounded_value.value.value = unsampled_value\n                    metric_value.bounded_value.lower_bound.value = lower_bound\n                    metric_value.bounded_value.upper_bound.value = upper_bound\n                    metric_value.bounded_value.methodology = (\n                        metrics_for_slice_pb2.BoundedValue.POISSON_BOOTSTRAP\n                    )\n                result.metrics[key].CopyFrom(metric_value)\n        elif isinstance(value, metrics_for_slice_pb2.BoundedValue):\n            metric_value = metrics_for_slice_pb2.MetricValue(\n                double_value=wrappers_pb2.DoubleValue(value=value.value.value)\n            )\n            confidence_interval = metrics_for_slice_pb2.ConfidenceInterval(\n                lower_bound=metrics_for_slice_pb2.MetricValue(\n                    double_value=wrappers_pb2.DoubleValue(value=value.lower_bound.value)\n                ),\n                upper_bound=metrics_for_slice_pb2.MetricValue(\n                    double_value=wrappers_pb2.DoubleValue(value=value.upper_bound.value)\n                ),\n            )\n            result.metric_keys_and_values.add(\n                key=key.to_proto(),\n                value=metric_value,\n                confidence_interval=confidence_interval,\n            )\n        else:\n            metric_value = convert_metric_value_to_proto(value)\n            if isinstance(key, metric_types.MetricKey):\n                result.metric_keys_and_values.add(\n                    key=key.to_proto(), value=metric_value\n                )\n            else:\n                # TODO(b/171992041): remove the string-typed metric key branch once v1\n                # code is removed.\n                result.metrics[key].CopyFrom(metric_value)\n    return result\n</code></pre>"},{"location":"api_docs/python/tfma/","title":"TFMA","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis","title":"tensorflow_model_analysis","text":"<p>Init module for TensorFlow Model Analysis.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ANALYSIS_KEY","title":"ANALYSIS_KEY  <code>module-attribute</code>","text":"<pre><code>ANALYSIS_KEY = 'analysis'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ARROW_INPUT_COLUMN","title":"ARROW_INPUT_COLUMN  <code>module-attribute</code>","text":"<pre><code>ARROW_INPUT_COLUMN = '__raw_record__'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ARROW_RECORD_BATCH_KEY","title":"ARROW_RECORD_BATCH_KEY  <code>module-attribute</code>","text":"<pre><code>ARROW_RECORD_BATCH_KEY = 'arrow_record_batch'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ATTRIBUTIONS_KEY","title":"ATTRIBUTIONS_KEY  <code>module-attribute</code>","text":"<pre><code>ATTRIBUTIONS_KEY = 'attributions'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.AddMetricsCallbackType","title":"AddMetricsCallbackType  <code>module-attribute</code>","text":"<pre><code>AddMetricsCallbackType = Callable[\n    [\n        TensorTypeMaybeDict,\n        TensorTypeMaybeDict,\n        TensorTypeMaybeDict,\n    ],\n    Dict[str, Tuple[TensorType, TensorType]],\n]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.AttributionsForSlice","title":"AttributionsForSlice  <code>module-attribute</code>","text":"<pre><code>AttributionsForSlice = AttributionsForSlice\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.BASELINE_KEY","title":"BASELINE_KEY  <code>module-attribute</code>","text":"<pre><code>BASELINE_KEY = 'baseline'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.BASELINE_SCORE_KEY","title":"BASELINE_SCORE_KEY  <code>module-attribute</code>","text":"<pre><code>BASELINE_SCORE_KEY = 'baseline_score'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.CANDIDATE_KEY","title":"CANDIDATE_KEY  <code>module-attribute</code>","text":"<pre><code>CANDIDATE_KEY = 'candidate'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.DATA_CENTRIC_MODE","title":"DATA_CENTRIC_MODE  <code>module-attribute</code>","text":"<pre><code>DATA_CENTRIC_MODE = 'data_centric_mode'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EXAMPLE_SCORE_KEY","title":"EXAMPLE_SCORE_KEY  <code>module-attribute</code>","text":"<pre><code>EXAMPLE_SCORE_KEY = 'example_score'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EXAMPLE_WEIGHTS_KEY","title":"EXAMPLE_WEIGHTS_KEY  <code>module-attribute</code>","text":"<pre><code>EXAMPLE_WEIGHTS_KEY = 'example_weights'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.Extracts","title":"Extracts  <code>module-attribute</code>","text":"<pre><code>Extracts = MutableMapping[str, Any]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FEATURES_KEY","title":"FEATURES_KEY  <code>module-attribute</code>","text":"<pre><code>FEATURES_KEY = 'features'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FEATURES_PREDICTIONS_LABELS_KEY","title":"FEATURES_PREDICTIONS_LABELS_KEY  <code>module-attribute</code>","text":"<pre><code>FEATURES_PREDICTIONS_LABELS_KEY = '_fpl'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.INPUT_KEY","title":"INPUT_KEY  <code>module-attribute</code>","text":"<pre><code>INPUT_KEY = 'input'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.LABELS_KEY","title":"LABELS_KEY  <code>module-attribute</code>","text":"<pre><code>LABELS_KEY = 'labels'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.METRICS_KEY","title":"METRICS_KEY  <code>module-attribute</code>","text":"<pre><code>METRICS_KEY = 'metrics'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.MODEL_CENTRIC_MODE","title":"MODEL_CENTRIC_MODE  <code>module-attribute</code>","text":"<pre><code>MODEL_CENTRIC_MODE = 'model_centric_mode'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.MaybeMultipleEvalSharedModels","title":"MaybeMultipleEvalSharedModels  <code>module-attribute</code>","text":"<pre><code>MaybeMultipleEvalSharedModels = Union[\n    EvalSharedModel,\n    List[EvalSharedModel],\n    Dict[str, EvalSharedModel],\n]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.MetricsForSlice","title":"MetricsForSlice  <code>module-attribute</code>","text":"<pre><code>MetricsForSlice = MetricsForSlice\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.PLOTS_KEY","title":"PLOTS_KEY  <code>module-attribute</code>","text":"<pre><code>PLOTS_KEY = 'plots'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.PREDICTIONS_KEY","title":"PREDICTIONS_KEY  <code>module-attribute</code>","text":"<pre><code>PREDICTIONS_KEY = 'predictions'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.PlotsForSlice","title":"PlotsForSlice  <code>module-attribute</code>","text":"<pre><code>PlotsForSlice = PlotsForSlice\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.SLICE_KEY_TYPES_KEY","title":"SLICE_KEY_TYPES_KEY  <code>module-attribute</code>","text":"<pre><code>SLICE_KEY_TYPES_KEY = '_slice_key_types'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TFMA_EVAL","title":"TFMA_EVAL  <code>module-attribute</code>","text":"<pre><code>TFMA_EVAL = 'tfma_eval'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TF_ESTIMATOR","title":"TF_ESTIMATOR  <code>module-attribute</code>","text":"<pre><code>TF_ESTIMATOR = 'tf_estimator'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TF_GENERIC","title":"TF_GENERIC  <code>module-attribute</code>","text":"<pre><code>TF_GENERIC = 'tf_generic'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TF_JS","title":"TF_JS  <code>module-attribute</code>","text":"<pre><code>TF_JS = 'tf_js'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TF_KERAS","title":"TF_KERAS  <code>module-attribute</code>","text":"<pre><code>TF_KERAS = 'tf_keras'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TF_LITE","title":"TF_LITE  <code>module-attribute</code>","text":"<pre><code>TF_LITE = 'tf_lite'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TensorType","title":"TensorType  <code>module-attribute</code>","text":"<pre><code>TensorType = Union[Tensor, SparseTensor, RaggedTensor]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TensorTypeMaybeDict","title":"TensorTypeMaybeDict  <code>module-attribute</code>","text":"<pre><code>TensorTypeMaybeDict = Union[TensorType, DictOfTensorType]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.TensorValue","title":"TensorValue  <code>module-attribute</code>","text":"<pre><code>TensorValue = Union[\n    ndarray,\n    SparseTensorValue,\n    RaggedTensorValue,\n    SparseTensorValue,\n]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VALIDATIONS_KEY","title":"VALIDATIONS_KEY  <code>module-attribute</code>","text":"<pre><code>VALIDATIONS_KEY = 'validations'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VERSION_STRING","title":"VERSION_STRING  <code>module-attribute</code>","text":"<pre><code>VERSION_STRING = '0.49.0.dev'\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ValidationResult","title":"ValidationResult  <code>module-attribute</code>","text":"<pre><code>ValidationResult = ValidationResult\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult","title":"EvalResult","text":"<p>               Bases: <code>NamedTuple('EvalResult', [('slicing_metrics', List[SlicedMetrics]), ('plots', List[SlicedPlots]), ('attributions', List[SlicedAttributions]), ('config', EvalConfig), ('data_location', str), ('file_format', str), ('model_location', str)])</code></p> <p>The result of a single model analysis run.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult--attributes","title":"Attributes","text":"<p>slicing_metrics: a list of <code>tfma.SlicedMetrics</code>, containing metric values     for each slice.   plots: List of slice-plot pairs.   attributions: List of SlicedAttributions containing attribution values for     each slice.   config: The config containing slicing and metrics specification.   data_location: Optional location for data used with config.   file_format: Optional format for data used with config.   model_location: Optional location(s) for model(s) used with config.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_attributions_for_all_slices","title":"get_attributions_for_all_slices","text":"<pre><code>get_attributions_for_all_slices(\n    metric_name: str = \"\",\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Dict[str, AttributionsByFeatureKey]\n</code></pre> <p>Get attribution feature keys and values for every slice.</p> <p>metric_name: Name of metric to get attributions for. Optional if only one     metric used.   output_name: The name of the output (optional, only used for multi-output     models).   class_id: Used with multi-class metrics to identify a specific class ID.   k: Used with multi-class metrics to identify the kth predicted value.   top_k: Used with multi-class and ranking metrics to identify top-k     predicted values.</p> <p>Dictionary mapping slices to attribution feature keys and values.</p> Source code in <code>tensorflow_model_analysis/view/view_types.py</code> <pre><code>def get_attributions_for_all_slices(\n    self,\n    metric_name: str = \"\",\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Dict[str, AttributionsByFeatureKey]:\n    \"\"\"Get attribution feature keys and values for every slice.\n\n    Args:\n    ----\n      metric_name: Name of metric to get attributions for. Optional if only one\n        metric used.\n      output_name: The name of the output (optional, only used for multi-output\n        models).\n      class_id: Used with multi-class metrics to identify a specific class ID.\n      k: Used with multi-class metrics to identify the kth predicted value.\n      top_k: Used with multi-class and ranking metrics to identify top-k\n        predicted values.\n\n    Returns:\n    -------\n      Dictionary mapping slices to attribution feature keys and values.\n    \"\"\"\n    if class_id or k or top_k:\n        sub_key = str(metric_types.SubKey(class_id, k, top_k))\n    else:\n        sub_key = \"\"\n\n    all_sliced_attributions = {}\n    for sliced_attributions in self.attributions:\n        slice_name = sliced_attributions[0]\n        attributions = sliced_attributions[1][output_name][sub_key]\n        if metric_name:\n            attributions = attributions[metric_name]\n        elif len(attributions) == 1:\n            attributions = list(attributions.values())[0]\n        else:\n            raise ValueError(\n                f\"metric_name must be one of the following: {attributions.keys()}\"\n            )\n        all_sliced_attributions[slice_name] = copy.copy(attributions)\n    return all_sliced_attributions  # pytype: disable=bad-return-type\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_attributions_for_slice","title":"get_attributions_for_slice","text":"<pre><code>get_attributions_for_slice(\n    slice_name: SliceKeyType = (),\n    metric_name: str = \"\",\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Union[AttributionsByFeatureKey, None]\n</code></pre> <p>Get attribution features names and values for a slice.</p> <p>slice_name: A tuple of the form (column, value), indicating which slice to     get attributions from. Optional; if excluded, use overall slice.   metric_name: Name of metric to get attributions for. Optional if only one     metric used.   output_name: The name of the output. Optional, only used for multi-output     models.   class_id: Used with multi-class models to identify a specific class ID.   k: Used with multi-class models to identify the kth predicted value.   top_k: Used with multi-class models to identify top-k attribution values.</p> <p>Dictionary containing feature keys and values for the specified slice.</p> <p>ValueError: If metric_name is required.</p> Source code in <code>tensorflow_model_analysis/view/view_types.py</code> <pre><code>def get_attributions_for_slice(\n    self,\n    slice_name: slicer.SliceKeyType = (),\n    metric_name: str = \"\",\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Union[AttributionsByFeatureKey, None]:\n    \"\"\"Get attribution features names and values for a slice.\n\n    Args:\n    ----\n      slice_name: A tuple of the form (column, value), indicating which slice to\n        get attributions from. Optional; if excluded, use overall slice.\n      metric_name: Name of metric to get attributions for. Optional if only one\n        metric used.\n      output_name: The name of the output. Optional, only used for multi-output\n        models.\n      class_id: Used with multi-class models to identify a specific class ID.\n      k: Used with multi-class models to identify the kth predicted value.\n      top_k: Used with multi-class models to identify top-k attribution values.\n\n    Returns:\n    -------\n      Dictionary containing feature keys and values for the specified slice.\n\n    Raises:\n    ------\n      ValueError: If metric_name is required.\n    \"\"\"\n    if class_id or k or top_k:\n        sub_key = str(metric_types.SubKey(class_id, k, top_k))\n    else:\n        sub_key = \"\"\n\n    def equals_slice_name(slice_key):\n        if not slice_key:\n            return not slice_name\n        else:\n            return slice_key == slice_name\n\n    for sliced_attributions in self.attributions:\n        slice_key = sliced_attributions[0]\n        slice_val = sliced_attributions[1]\n        if equals_slice_name(slice_key):\n            if metric_name:\n                return slice_val[output_name][sub_key][metric_name]\n            elif len(slice_val[output_name][sub_key]) == 1:\n                return list(slice_val[output_name][sub_key].values())[0]\n            else:\n                raise ValueError(\n                    f\"metric_name must be one of the following: {slice_val[output_name][sub_key].keys()}\"\n                )\n\n    # if slice could not be found, return None\n    return None\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_metric_names","title":"get_metric_names","text":"<pre><code>get_metric_names() -&gt; Sequence[str]\n</code></pre> <p>Get names of metrics.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_metric_names--returns","title":"Returns","text":"<p>List of metric names.</p> Source code in <code>tensorflow_model_analysis/view/view_types.py</code> <pre><code>def get_metric_names(self) -&gt; Sequence[str]:\n    \"\"\"Get names of metrics.\n\n    Returns\n    -------\n      List of metric names.\n    \"\"\"\n    metric_names = set()\n    for slicing_metric in self.slicing_metrics:\n        for output_name in slicing_metric[1]:\n            for metrics in slicing_metric[1][output_name].values():\n                metric_names.update(metrics)\n    return list(metric_names)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_metrics_for_all_slices","title":"get_metrics_for_all_slices","text":"<pre><code>get_metrics_for_all_slices(\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Dict[str, MetricsByTextKey]\n</code></pre> <p>Get metric names and values for every slice.</p> <p>output_name: The name of the output (optional, only used for multi-output     models).   class_id: Used with multi-class metrics to identify a specific class ID.   k: Used with multi-class metrics to identify the kth predicted value.   top_k: Used with multi-class and ranking metrics to identify top-k     predicted values.</p> <p>Dictionary mapping slices to metric names and values.</p> Source code in <code>tensorflow_model_analysis/view/view_types.py</code> <pre><code>def get_metrics_for_all_slices(\n    self,\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Dict[str, MetricsByTextKey]:\n    \"\"\"Get metric names and values for every slice.\n\n    Args:\n    ----\n      output_name: The name of the output (optional, only used for multi-output\n        models).\n      class_id: Used with multi-class metrics to identify a specific class ID.\n      k: Used with multi-class metrics to identify the kth predicted value.\n      top_k: Used with multi-class and ranking metrics to identify top-k\n        predicted values.\n\n    Returns:\n    -------\n      Dictionary mapping slices to metric names and values.\n    \"\"\"\n    if all(v is None for v in [class_id, k, top_k]):\n        sub_key = \"\"\n    else:\n        sub_key = str(metric_types.SubKey(class_id, k, top_k))\n\n    sliced_metrics = {}\n    for slicing_metric in self.slicing_metrics:\n        slice_name = slicing_metric[0]\n        metrics = slicing_metric[1][output_name][sub_key]\n        sliced_metrics[slice_name] = {\n            metric_name: metric_value\n            for metric_name, metric_value in metrics.items()\n        }\n    return sliced_metrics  # pytype: disable=bad-return-type\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_metrics_for_slice","title":"get_metrics_for_slice","text":"<pre><code>get_metrics_for_slice(\n    slice_name: SliceKeyType = (),\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Union[MetricsByTextKey, None]\n</code></pre> <p>Get metric names and values for a slice.</p> <p>slice_name: A tuple of the form (column, value), indicating which slice to     get metrics from. Optional; if excluded, return overall metrics.   output_name: The name of the output. Optional, only used for multi-output     models.   class_id: Used with multi-class metrics to identify a specific class ID.   k: Used with multi-class metrics to identify the kth predicted value.   top_k: Used with multi-class and ranking metrics to identify top-k     predicted values.</p> <p>Dictionary containing metric names and values for the specified slice.</p> Source code in <code>tensorflow_model_analysis/view/view_types.py</code> <pre><code>def get_metrics_for_slice(\n    self,\n    slice_name: slicer.SliceKeyType = (),\n    output_name: str = \"\",\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None,\n) -&gt; Union[MetricsByTextKey, None]:\n    \"\"\"Get metric names and values for a slice.\n\n    Args:\n    ----\n      slice_name: A tuple of the form (column, value), indicating which slice to\n        get metrics from. Optional; if excluded, return overall metrics.\n      output_name: The name of the output. Optional, only used for multi-output\n        models.\n      class_id: Used with multi-class metrics to identify a specific class ID.\n      k: Used with multi-class metrics to identify the kth predicted value.\n      top_k: Used with multi-class and ranking metrics to identify top-k\n        predicted values.\n\n    Returns:\n    -------\n      Dictionary containing metric names and values for the specified slice.\n    \"\"\"\n    if all(v is None for v in [class_id, k, top_k]):\n        sub_key = \"\"\n    else:\n        sub_key = str(metric_types.SubKey(class_id, k, top_k))\n\n    def equals_slice_name(slice_key):\n        if not slice_key:\n            return not slice_name\n        else:\n            return slice_key == slice_name\n\n    for slicing_metric in self.slicing_metrics:\n        slice_key = slicing_metric[0]\n        slice_val = slicing_metric[1]\n        if equals_slice_name(slice_key):\n            return slice_val[output_name][sub_key]\n\n    # if slice could not be found, return None\n    return None\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_slice_names","title":"get_slice_names","text":"<pre><code>get_slice_names() -&gt; Sequence[str]\n</code></pre> <p>Get names of slices.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalResult.get_slice_names--returns","title":"Returns","text":"<p>List of slice names.</p> Source code in <code>tensorflow_model_analysis/view/view_types.py</code> <pre><code>def get_slice_names(self) -&gt; Sequence[str]:\n    \"\"\"Get names of slices.\n\n    Returns\n    -------\n      List of slice names.\n    \"\"\"\n    return [\n        slicing_metric[0] for slicing_metric in self.slicing_metrics\n    ]  # pytype: disable=bad-return-type\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalSharedModel","title":"EvalSharedModel","text":"<p>               Bases: <code>NamedTuple('EvalSharedModel', [('model_path', str), ('add_metrics_callbacks', List[Callable]), ('include_default_metrics', bool), ('example_weight_key', Union[str, Dict[str, str]]), ('additional_fetches', List[str]), ('model_loader', ModelLoader), ('model_name', str), ('model_type', str), ('rubber_stamp', bool), ('is_baseline', bool), ('resource_hints', Optional[Dict[str, Any]]), ('backend_config', Optional[Any])])</code></p> <p>Shared model used during extraction and evaluation.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.EvalSharedModel--attributes","title":"Attributes","text":"<p>model_path: Path to EvalSavedModel (containing the saved_model.pb file).   add_metrics_callbacks: Optional list of callbacks for adding additional     metrics to the graph. The names of the metrics added by the callbacks     should not conflict with existing metrics. See below for more details     about what each callback should do. The callbacks are only used during     evaluation.   include_default_metrics: True to include the default metrics that are part     of the saved model graph during evaluation.   example_weight_key: Example weight key (single-output model) or dict of     example weight keys (multi-output model) keyed by output_name.   additional_fetches: Prefixes of additional tensors stored in     signature_def.inputs that should be fetched at prediction time. The     \"features\" and \"labels\" tensors are handled automatically and should not     be included in this list.   model_loader: Model loader.   model_name: Model name (should align with ModelSpecs.name).   model_type: Model type (tfma.TF_KERAS, tfma.TF_LITE, tfma.TF_ESTIMATOR, ..).   rubber_stamp: True if this model is being rubber stamped. When a     model is rubber stamped diff thresholds will be ignored if an associated     baseline model is not passed.   is_baseline: The model is the baseline for comparison or not.   resource_hints: The beam resource hints to apply to the PTransform which     runs inference for this model.   backend_config: The backend config for running model inference.</p> <p>More details on add_metrics_callbacks:</p> <p>Each add_metrics_callback should have the following prototype:     def add_metrics_callback(features_dict, predictions_dict, labels_dict):</p> <p>Note that features_dict, predictions_dict and labels_dict are not   necessarily dictionaries - they might also be Tensors, depending on what the   model's eval_input_receiver_fn returns.</p> <p>It should create and return a metric_ops dictionary, such that   metric_ops['metric_name'] = (value_op, update_op), just as in the Trainer.</p> <p>Short example:</p> <p>def add_metrics_callback(features_dict, predictions_dict, labels):     metrics_ops = {}     metric_ops['mean_label'] = tf.metrics.mean(labels)     metric_ops['mean_probability'] = tf.metrics.mean(tf.slice(       predictions_dict['probabilities'], [0, 1], [2, 1]))     return metric_ops</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FeaturesPredictionsLabels","title":"FeaturesPredictionsLabels","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FeaturesPredictionsLabels-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FeaturesPredictionsLabels.features","title":"features  <code>instance-attribute</code>","text":"<pre><code>features: DictOfFetchedTensorValues\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FeaturesPredictionsLabels.input_ref","title":"input_ref  <code>instance-attribute</code>","text":"<pre><code>input_ref: int\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FeaturesPredictionsLabels.labels","title":"labels  <code>instance-attribute</code>","text":"<pre><code>labels: DictOfFetchedTensorValues\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.FeaturesPredictionsLabels.predictions","title":"predictions  <code>instance-attribute</code>","text":"<pre><code>predictions: DictOfFetchedTensorValues\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.MaterializedColumn","title":"MaterializedColumn","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.MaterializedColumn-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.MaterializedColumn.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.MaterializedColumn.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: Union[\n    List[bytes], List[int], List[float], bytes, int, float\n]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ModelLoader","title":"ModelLoader","text":"<pre><code>ModelLoader(\n    construct_fn: Callable[[], Any],\n    tags: Optional[List[str]] = None,\n)\n</code></pre> <p>Model loader is responsible for loading shared model types.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ModelLoader--attributes","title":"Attributes","text":"<p>construct_fn: A callable which creates the model instance. The callable     should take no args as input (typically a closure is used to capture     necessary parameters).   tags: Optional model tags (e.g. 'serve' for serving or 'eval' for     EvalSavedModel).</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def __init__(\n    self, construct_fn: Callable[[], Any], tags: Optional[List[str]] = None\n):\n    self.construct_fn = construct_fn\n    self.tags = tags\n    self._shared_handle = shared.Shared()\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ModelLoader-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ModelLoader.construct_fn","title":"construct_fn  <code>instance-attribute</code>","text":"<pre><code>construct_fn = construct_fn\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ModelLoader.tags","title":"tags  <code>instance-attribute</code>","text":"<pre><code>tags = tags\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ModelLoader-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ModelLoader.load","title":"load","text":"<pre><code>load(\n    model_load_time_callback: Optional[\n        Callable[[int], None]\n    ] = None,\n) -&gt; Any\n</code></pre> <p>Returns loaded model.</p> <p>model_load_time_callback: Optional callback to track load time.</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def load(\n    self, model_load_time_callback: Optional[Callable[[int], None]] = None\n) -&gt; Any:\n    \"\"\"Returns loaded model.\n\n    Args:\n    ----\n      model_load_time_callback: Optional callback to track load time.\n    \"\"\"\n    if model_load_time_callback:\n        construct_fn = self._construct_fn_with_load_time(model_load_time_callback)\n    else:\n        construct_fn = self.construct_fn\n    return self._shared_handle.acquire(construct_fn)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.RaggedTensorValue","title":"RaggedTensorValue","text":"<p>               Bases: <code>NamedTuple('RaggedTensorValue', [('values', ndarray), ('nested_row_splits', List[ndarray])])</code></p> <p>RaggedTensorValue encapsulates a batch of ragged tensor values.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.RaggedTensorValue--attributes","title":"Attributes","text":"<p>values: A np.ndarray of values.   nested_row_splits: A list of np.ndarray values representing the row splits     (one per dimension including the batch dimension).</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.SparseTensorValue","title":"SparseTensorValue","text":"<p>               Bases: <code>NamedTuple('SparseTensorValue', [('values', ndarray), ('indices', ndarray), ('dense_shape', ndarray)])</code></p> <p>SparseTensorValue encapsulates a batch of sparse tensor values.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.SparseTensorValue--attributes","title":"Attributes","text":"<p>values: A np.ndarray of values.   indices: A np.ndarray of indices.   dense_shape: A np.ndarray representing the dense shape.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VarLenTensorValue","title":"VarLenTensorValue","text":"<p>               Bases: <code>NamedTuple('VarLenTensorValue', [('values', ndarray), ('indices', ndarray), ('dense_shape', ndarray)])</code></p> <p>VarLenTensorValue encapsulates a batch of varlen dense tensor values.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VarLenTensorValue--attributes","title":"Attributes","text":"<p>values: A np.ndarray of values.   indices: A np.ndarray of indices.   dense_shape: A np.ndarray representing the dense shape of the entire tensor.     Note that each row (i.e. set of values sharing the same value for the     first / batch dimension) is considered to have its own shape based on the     presence of values.</p>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VarLenTensorValue-classes","title":"Classes","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VarLenTensorValue.DenseRowIterator","title":"DenseRowIterator","text":"<pre><code>DenseRowIterator(tensor)\n</code></pre> <p>An Iterator over rows of a VarLenTensorValue as dense np.arrays.</p> <p>Because the VarLenTensorValue was created from a set of variable length (dense) arrays, we can invert this process to turn a VarLenTensorValue back into the original dense arrays.</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def __init__(self, tensor):\n    self._tensor = tensor\n    self._offset = 0\n</code></pre> Functions\u00b6"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VarLenTensorValue-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VarLenTensorValue.dense_rows","title":"dense_rows","text":"<pre><code>dense_rows()\n</code></pre> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>def dense_rows(self):\n    return self.DenseRowIterator(self)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.VarLenTensorValue.from_dense_rows","title":"from_dense_rows  <code>classmethod</code>","text":"<pre><code>from_dense_rows(\n    dense_rows: Iterable[ndarray],\n) -&gt; VarLenTensorValue\n</code></pre> <p>Converts a collection of variable length dense arrays into a tensor.</p> <p>dense_rows: A sequence of possibly variable length 1D arrays.</p> <p>A new VarLenTensorValue containing the sparse representation of the   vertically stacked dense rows. The dense_shape attribute on the result   will be (num_rows, max_row_len).</p> Source code in <code>tensorflow_model_analysis/api/types.py</code> <pre><code>@classmethod\ndef from_dense_rows(cls, dense_rows: Iterable[np.ndarray]) -&gt; \"VarLenTensorValue\":\n    \"\"\"Converts a collection of variable length dense arrays into a tensor.\n\n    Args:\n    ----\n      dense_rows: A sequence of possibly variable length 1D arrays.\n\n    Returns:\n    -------\n      A new VarLenTensorValue containing the sparse representation of the\n      vertically stacked dense rows. The dense_shape attribute on the result\n      will be (num_rows, max_row_len).\n    \"\"\"\n    rows = []\n    index_arrays = []\n    max_row_len = 0\n    num_rows = 0\n    for i, row in enumerate(dense_rows):\n        num_rows += 1\n        if row.size:\n            if row.ndim &lt;= 1:\n                # Add a dimension for unsized numpy array. This will solve the problem\n                # where scalar numpy arrays like np.array(None), np.array(0) can not\n                # be merged with other numpy arrays.\n                row = row.reshape(-1)\n                rows.append(row)\n            else:\n                raise ValueError(\n                    \"Each non-empty dense row should be 1D or scalar but\"\n                    f\" found row with shape {row.shape}.\"\n                )\n            index_arrays.append(np.array([[i, j] for j in range(len(row))]))\n        max_row_len = max(max_row_len, row.size)\n    if index_arrays:\n        values = np.concatenate(rows, axis=0)\n        indices = np.concatenate(index_arrays, axis=0)\n    else:\n        # empty case\n        values = np.array([])\n        indices = np.empty((0, 2))\n    dense_shape = np.array([num_rows, max_row_len])\n    return cls.__new__(cls, values=values, indices=indices, dense_shape=dense_shape)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis-functions","title":"Functions","text":""},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.BatchedInputsToExtracts","title":"BatchedInputsToExtracts","text":"<pre><code>BatchedInputsToExtracts(\n    batched_inputs: PCollection,\n) -&gt; PCollection\n</code></pre> <p>Converts Arrow RecordBatch inputs to Extracts.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(Union[bytes, pa.RecordBatch, types.Extracts])\n@beam.typehints.with_output_types(types.Extracts)\ndef BatchedInputsToExtracts(  # pylint: disable=invalid-name\n    batched_inputs: beam.pvalue.PCollection,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Converts Arrow RecordBatch inputs to Extracts.\"\"\"\n\n    def to_extracts(\n        x: Union[bytes, types.Extracts, pa.RecordBatch],\n    ) -&gt; types.Extracts:\n        result = {}\n        if isinstance(x, dict):\n            result.update(x)\n        else:\n            result[constants.ARROW_RECORD_BATCH_KEY] = x\n        return result\n\n    return batched_inputs | \"AddArrowRecordBatchKey\" &gt;&gt; beam.Map(to_extracts)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ExtractAndEvaluate","title":"ExtractAndEvaluate","text":"<pre><code>ExtractAndEvaluate(\n    extracts: PCollection,\n    extractors: List[Extractor],\n    evaluators: List[Evaluator],\n) -&gt; Evaluation\n</code></pre> <p>Performs Extractions and Evaluations in provided order.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(types.Extracts)\n@beam.typehints.with_output_types(Any)\ndef ExtractAndEvaluate(  # pylint: disable=invalid-name\n    extracts: beam.pvalue.PCollection,\n    extractors: List[extractor.Extractor],\n    evaluators: List[evaluator.Evaluator],\n) -&gt; evaluator.Evaluation:\n    \"\"\"Performs Extractions and Evaluations in provided order.\"\"\"\n    # evaluation[k] = list of values for k\n    evaluation = {}\n\n    def update(evaluation: Dict[str, Any], new_evaluation: Dict[str, Any]):\n        for k, v in new_evaluation.items():\n            if k not in evaluation:\n                evaluation[k] = []\n            evaluation[k].append(v)\n        return evaluation\n\n    _ = extracts | \"TrackInputBytes\" &gt;&gt; _TrackBytesProcessed()  # pylint: disable=no-value-for-parameter\n    # Run evaluators that run before extraction (i.e. that only require\n    # the incoming input extract added by ReadInputs)\n    for v in evaluators:\n        if not v.run_after:\n            update(evaluation, extracts | v.stage_name &gt;&gt; v.ptransform)\n    for x in extractors:\n        extracts = extracts | x.stage_name &gt;&gt; x.ptransform\n        for v in evaluators:\n            if v.run_after == x.stage_name:\n                update(evaluation, extracts | v.stage_name &gt;&gt; v.ptransform)\n    for v in evaluators:\n        if v.run_after == extractor.LAST_EXTRACTOR_STAGE_NAME:\n            update(evaluation, extracts | v.stage_name &gt;&gt; v.ptransform)\n\n    # Merge multi-valued keys if necessary.\n    result = {}\n    for k, v in evaluation.items():\n        if len(v) == 1:\n            result[k] = v[0]\n            continue\n\n        # Note that we assume that if a key is multivalued, its values are\n        # dictionaries with disjoint keys. The combined value will simply be the\n        # disjoint union of all the dictionaries.\n        result[k] = (\n            v\n            | \"FlattenEvaluationOutput(%s)\" % k &gt;&gt; beam.Flatten()\n            | \"CombineEvaluationOutput(%s)\" % k\n            &gt;&gt; beam.CombinePerKey(_CombineEvaluationDictionariesFn())\n        )\n\n    return result\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.ExtractEvaluateAndWriteResults","title":"ExtractEvaluateAndWriteResults","text":"<pre><code>ExtractEvaluateAndWriteResults(\n    examples: PCollection,\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    eval_config: Optional[EvalConfig] = None,\n    extractors: Optional[List[Extractor]] = None,\n    evaluators: Optional[List[Evaluator]] = None,\n    writers: Optional[List[Writer]] = None,\n    output_path: Optional[str] = None,\n    display_only_data_location: Optional[str] = None,\n    display_only_file_format: Optional[str] = None,\n    slice_spec: Optional[List[SingleSliceSpec]] = None,\n    write_config: Optional[bool] = True,\n    compute_confidence_intervals: Optional[bool] = False,\n    min_slice_size: int = 1,\n    random_seed_for_testing: Optional[int] = None,\n    tensor_adapter_config: Optional[\n        TensorAdapterConfig\n    ] = None,\n    schema: Optional[Schema] = None,\n    config_version: Optional[int] = None,\n) -&gt; Dict[str, PCollection]\n</code></pre> <p>PTransform for performing extraction, evaluation, and writing results.</p> <p>Users who want to construct their own Beam pipelines instead of using the lightweight run_model_analysis functions should use this PTransform.</p> <p>Example usage:</p> <pre><code>eval_config = tfma.EvalConfig(model_specs=[...], metrics_specs=[...],\n                              slicing_specs=[...])\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=model_location, eval_config=eval_config)\ntfx_io = tf_example_record.TFExampleRecord(\n    file_pattern=data_location,\n    raw_record_column_name=tfma.ARROW_INPUT_COLUMN)\nwith beam.Pipeline(runner=...) as p:\n  _ = (p\n       | 'ReadData' &gt;&gt; tfx_io.BeamSource()\n       | 'ExtractEvaluateAndWriteResults' &gt;&gt;\n       tfma.ExtractEvaluateAndWriteResults(\n           eval_shared_model=eval_shared_model,\n           eval_config=eval_config,\n           ...))\nresult = tfma.load_eval_result(output_path=output_path)\ntfma.view.render_slicing_metrics(result)\n\nNOTE: If running with an EvalSavedModel (i.e. the ModelSpec has signature_name\n\"eval\"), then instead of using the tfxio.BeamSource() code use the following\nbeam.io.ReadFromTFRecord(data_location)\n</code></pre> <p>Note that the exact serialization format is an internal implementation detail and subject to change. Users should only use the TFMA functions to write and read the results.</p> <p>examples: PCollection of input examples or Arrow Record batches. Examples     can be any format the model accepts (e.g. string containing CSV row,     TensorFlow.Example, etc). If the examples are in the form of a dict it     will be assumed that input is already in the form of tfma.Extracts with     examples stored under tfma.INPUT_KEY (any other keys will be passed along     unchanged to downstream extractors and evaluators).   eval_shared_model: Optional shared model (single-model evaluation) or list     of shared models (multi-model evaluation). Only required if needed by     default extractors, evaluators, or writers and for display purposes of the     model path.   eval_config: Eval config.   extractors: Optional list of Extractors to apply to Extracts. Typically     these will be added by calling the default_extractors function. If no     extractors are provided, default_extractors (non-materialized) will be     used.   evaluators: Optional list of Evaluators for evaluating Extracts. Typically     these will be added by calling the default_evaluators function. If no     evaluators are provided, default_evaluators will be used.   writers: Optional list of Writers for writing Evaluation output. Typically     these will be added by calling the default_writers function. If no writers     are provided, default_writers will be used.   output_path: Path to output results to (config file, metrics, plots, etc).   display_only_data_location: Optional path indicating where the examples were     read from. This is used only for display purposes - data will not actually     be read from this path.   display_only_file_format: Optional format of the examples. This is used only     for display purposes.   slice_spec: Deprecated (use EvalConfig).   write_config: Deprecated (use EvalConfig).   compute_confidence_intervals: Deprecated (use EvalConfig).   min_slice_size: Deprecated (use EvalConfig).   random_seed_for_testing: Provide for deterministic tests only.   tensor_adapter_config: Tensor adapter config which specifies how to obtain     tensors from the Arrow RecordBatch. If None, an attempt will be made to     create the tensors using default TensorRepresentations.   schema: A schema to use for customizing evaluators.   config_version: Optional config version for this evaluation. This should not     be explicitly set by users. It is only intended to be used in cases where     the provided eval_config was generated internally, and thus not a reliable     indicator of user intent.</p> <p>ValueError: If EvalConfig invalid or matching Extractor not found for an     Evaluator.</p> <p>A dict of writer results keyed by the writer stage name.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>@beam.ptransform_fn\ndef ExtractEvaluateAndWriteResults(  # pylint: disable=invalid-name\n    examples: beam.PCollection,\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    extractors: Optional[List[extractor.Extractor]] = None,\n    evaluators: Optional[List[evaluator.Evaluator]] = None,\n    writers: Optional[List[writer.Writer]] = None,\n    output_path: Optional[str] = None,\n    display_only_data_location: Optional[str] = None,\n    display_only_file_format: Optional[str] = None,\n    slice_spec: Optional[List[slicer.SingleSliceSpec]] = None,\n    write_config: Optional[bool] = True,\n    compute_confidence_intervals: Optional[bool] = False,\n    min_slice_size: int = 1,\n    random_seed_for_testing: Optional[int] = None,\n    tensor_adapter_config: Optional[tensor_adapter.TensorAdapterConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    config_version: Optional[int] = None,\n) -&gt; Dict[str, beam.PCollection]:\n    \"\"\"PTransform for performing extraction, evaluation, and writing results.\n\n    Users who want to construct their own Beam pipelines instead of using the\n    lightweight run_model_analysis functions should use this PTransform.\n\n    Example usage:\n\n    ```python\n    eval_config = tfma.EvalConfig(model_specs=[...], metrics_specs=[...],\n                                  slicing_specs=[...])\n    eval_shared_model = tfma.default_eval_shared_model(\n        eval_saved_model_path=model_location, eval_config=eval_config)\n    tfx_io = tf_example_record.TFExampleRecord(\n        file_pattern=data_location,\n        raw_record_column_name=tfma.ARROW_INPUT_COLUMN)\n    with beam.Pipeline(runner=...) as p:\n      _ = (p\n           | 'ReadData' &gt;&gt; tfx_io.BeamSource()\n           | 'ExtractEvaluateAndWriteResults' &gt;&gt;\n           tfma.ExtractEvaluateAndWriteResults(\n               eval_shared_model=eval_shared_model,\n               eval_config=eval_config,\n               ...))\n    result = tfma.load_eval_result(output_path=output_path)\n    tfma.view.render_slicing_metrics(result)\n\n    NOTE: If running with an EvalSavedModel (i.e. the ModelSpec has signature_name\n    \"eval\"), then instead of using the tfxio.BeamSource() code use the following\n    beam.io.ReadFromTFRecord(data_location)\n    ```\n\n    Note that the exact serialization format is an internal implementation detail\n    and subject to change. Users should only use the TFMA functions to write and\n    read the results.\n\n    Args:\n    ----\n      examples: PCollection of input examples or Arrow Record batches. Examples\n        can be any format the model accepts (e.g. string containing CSV row,\n        TensorFlow.Example, etc). If the examples are in the form of a dict it\n        will be assumed that input is already in the form of tfma.Extracts with\n        examples stored under tfma.INPUT_KEY (any other keys will be passed along\n        unchanged to downstream extractors and evaluators).\n      eval_shared_model: Optional shared model (single-model evaluation) or list\n        of shared models (multi-model evaluation). Only required if needed by\n        default extractors, evaluators, or writers and for display purposes of the\n        model path.\n      eval_config: Eval config.\n      extractors: Optional list of Extractors to apply to Extracts. Typically\n        these will be added by calling the default_extractors function. If no\n        extractors are provided, default_extractors (non-materialized) will be\n        used.\n      evaluators: Optional list of Evaluators for evaluating Extracts. Typically\n        these will be added by calling the default_evaluators function. If no\n        evaluators are provided, default_evaluators will be used.\n      writers: Optional list of Writers for writing Evaluation output. Typically\n        these will be added by calling the default_writers function. If no writers\n        are provided, default_writers will be used.\n      output_path: Path to output results to (config file, metrics, plots, etc).\n      display_only_data_location: Optional path indicating where the examples were\n        read from. This is used only for display purposes - data will not actually\n        be read from this path.\n      display_only_file_format: Optional format of the examples. This is used only\n        for display purposes.\n      slice_spec: Deprecated (use EvalConfig).\n      write_config: Deprecated (use EvalConfig).\n      compute_confidence_intervals: Deprecated (use EvalConfig).\n      min_slice_size: Deprecated (use EvalConfig).\n      random_seed_for_testing: Provide for deterministic tests only.\n      tensor_adapter_config: Tensor adapter config which specifies how to obtain\n        tensors from the Arrow RecordBatch. If None, an attempt will be made to\n        create the tensors using default TensorRepresentations.\n      schema: A schema to use for customizing evaluators.\n      config_version: Optional config version for this evaluation. This should not\n        be explicitly set by users. It is only intended to be used in cases where\n        the provided eval_config was generated internally, and thus not a reliable\n        indicator of user intent.\n\n    Raises:\n    ------\n      ValueError: If EvalConfig invalid or matching Extractor not found for an\n        Evaluator.\n\n    Returns:\n    -------\n      A dict of writer results keyed by the writer stage name.\n    \"\"\"\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n\n    if eval_config is None:\n        config_version = 1 if config_version is None else config_version\n        eval_config = _default_eval_config(\n            eval_shared_models,\n            slice_spec,\n            write_config,\n            compute_confidence_intervals,\n            min_slice_size,\n        )\n    else:\n        config_version = 2 if config_version is None else config_version\n        eval_config = _update_eval_config_with_defaults(eval_config, eval_shared_model)\n    config_util.verify_eval_config(eval_config)\n\n    if not extractors:\n        extractors = default_extractors(\n            eval_config=eval_config,\n            eval_shared_model=eval_shared_model,\n            tensor_adapter_config=tensor_adapter_config,\n            config_version=config_version,\n        )\n\n    if not evaluators:\n        evaluators = default_evaluators(\n            eval_config=eval_config,\n            eval_shared_model=eval_shared_model,\n            random_seed_for_testing=random_seed_for_testing,\n            schema=schema,\n            config_version=config_version,\n        )\n\n    for v in evaluators:\n        evaluator.verify_evaluator(v, extractors)\n\n    if not writers:\n        writers = default_writers(\n            output_path=output_path,\n            eval_shared_model=eval_shared_model,\n            eval_config=eval_config,\n            display_only_data_location=display_only_data_location,\n            display_only_data_file_format=display_only_file_format,\n        )\n\n    # pylint: disable=no-value-for-parameter\n    if is_batched_input(eval_shared_model, eval_config, config_version):\n        extracts = examples | \"BatchedInputsToExtracts\" &gt;&gt; BatchedInputsToExtracts()\n    else:\n        extracts = examples | \"InputsToExtracts\" &gt;&gt; InputsToExtracts()\n\n    return (\n        extracts\n        | \"ExtractAndEvaluate\"\n        &gt;&gt; ExtractAndEvaluate(extractors=extractors, evaluators=evaluators)\n        | \"WriteResults\" &gt;&gt; WriteResults(writers=writers)\n    )\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.InputsToExtracts","title":"InputsToExtracts","text":"<pre><code>InputsToExtracts(inputs: PCollection) -&gt; PCollection\n</code></pre> <p>Converts serialized inputs (e.g. examples) to Extracts if not already.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>@beam.ptransform_fn\n# TODO(b/156538355): Find out why str is also required instead of just bytes\n#   after adding types.Extracts.\n@beam.typehints.with_input_types(Union[bytes, str, types.Extracts])\n@beam.typehints.with_output_types(types.Extracts)\ndef InputsToExtracts(  # pylint: disable=invalid-name\n    inputs: beam.pvalue.PCollection,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Converts serialized inputs (e.g. examples) to Extracts if not already.\"\"\"\n\n    def to_extracts(x: Union[bytes, str, types.Extracts]) -&gt; types.Extracts:\n        result = {}\n        if isinstance(x, dict):\n            result.update(x)\n        else:\n            result[constants.INPUT_KEY] = x\n        return result\n\n    return inputs | \"AddInputKey\" &gt;&gt; beam.Map(to_extracts)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.Validate","title":"Validate","text":"<pre><code>Validate(\n    extracts: PCollection,\n    alternatives: Dict[str, PTransform],\n    validators: List[Validator],\n) -&gt; Validation\n</code></pre> <p>Performs validation of alternative evaluations.</p> <p>extracts: PCollection of extracts.   alternatives: Dict of PTransforms (Extracts -&gt; Evaluation) whose output will     be compared for validation purposes (e.g. 'baseline' vs 'candidate').   validators: List of validators for validating the output from running the     alternatives. The Validation outputs produced by the validators will be     merged into a single output. If there are overlapping output keys, later     outputs will replace earlier outputs sharing the same key.</p> <p>Validation dict.</p> Source code in <code>tensorflow_model_analysis/api/verifier_lib.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(types.Extracts)\n@beam.typehints.with_output_types(Any)\ndef Validate(  # pylint: disable=invalid-name\n    extracts: beam.pvalue.PCollection,\n    alternatives: Dict[str, beam.PTransform],\n    validators: List[validator.Validator],\n) -&gt; validator.Validation:\n    \"\"\"Performs validation of alternative evaluations.\n\n    Args:\n    ----\n      extracts: PCollection of extracts.\n      alternatives: Dict of PTransforms (Extracts -&gt; Evaluation) whose output will\n        be compared for validation purposes (e.g. 'baseline' vs 'candidate').\n      validators: List of validators for validating the output from running the\n        alternatives. The Validation outputs produced by the validators will be\n        merged into a single output. If there are overlapping output keys, later\n        outputs will replace earlier outputs sharing the same key.\n\n    Returns:\n    -------\n      Validation dict.\n    \"\"\"\n    evaluations = {}\n    for key in alternatives:\n        evaluations[key] = extracts | \"Evaluate(%s)\" % key &gt;&gt; alternatives[key]\n\n    validation = {}\n    for v in validators:\n        validation.update(evaluations | v.stage_name &gt;&gt; v.ptransform)\n    return validation\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.WriteResults","title":"WriteResults","text":"<pre><code>WriteResults(\n    evaluation_or_validation: Union[Evaluation, Validation],\n    writers: List[Writer],\n) -&gt; Dict[str, PCollection]\n</code></pre> <p>Writes Evaluation or Validation results using given writers.</p> <p>evaluation_or_validation: Evaluation or Validation output.   writers: Writes to use for writing out output.</p> <p>ValueError: If Evaluation or Validation is empty.</p> <p>A dict of writer results keyed by the writer stage name.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>@beam.ptransform_fn\n# TODO(b/157600974): Add input typehint.\ndef WriteResults(  # pylint: disable=invalid-name\n    evaluation_or_validation: Union[evaluator.Evaluation, validator.Validation],\n    writers: List[writer.Writer],\n) -&gt; Dict[str, beam.PCollection]:\n    \"\"\"Writes Evaluation or Validation results using given writers.\n\n    Args:\n    ----\n      evaluation_or_validation: Evaluation or Validation output.\n      writers: Writes to use for writing out output.\n\n    Raises:\n    ------\n      ValueError: If Evaluation or Validation is empty.\n\n    Returns:\n    -------\n      A dict of writer results keyed by the writer stage name.\n    \"\"\"\n    if not evaluation_or_validation:\n        raise ValueError(\"Evaluations and Validations cannot be empty\")\n    result = {}\n    for w in writers:\n        result[w.stage_name] = evaluation_or_validation | w.stage_name &gt;&gt; w.ptransform\n    return result\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.analyze_raw_data","title":"analyze_raw_data","text":"<pre><code>analyze_raw_data(\n    data: DataFrame,\n    eval_config: Optional[EvalConfig] = None,\n    output_path: Optional[str] = None,\n    extractors: Optional[List[Extractor]] = None,\n    evaluators: Optional[List[Evaluator]] = None,\n    writers: Optional[List[Writer]] = None,\n    add_metric_callbacks: Optional[\n        List[AddMetricsCallbackType]\n    ] = None,\n) -&gt; EvalResult\n</code></pre> <p>Runs TensorFlow model analysis on a pandas.DataFrame.</p> <p>This function allows you to use TFMA with Pandas DataFrames. The dataframe must include a 'predicted' column for the predicted label and a 'label' column for the actual label.</p> <p>In addition to a DataFrame, this function requires an eval_config, a <code>tfma.EvalConfig</code> object containing various configuration parameters (see config.proto for a comprehensive list)...</p> <ul> <li>the metrics to compute</li> <li>the slices to compute metrics on</li> <li>the DataFrame's column names for example labels and predictions ('label'   and 'prediction' by default)</li> <li>confidence interval options</li> </ul> <p>This function returns a <code>tfma.EvalResult</code>, which contains TFMA's computed metrics and can be used to generate plots with <code>tfma.view.render_slicing_metrics</code>.</p> <p>Example usage:</p> <pre><code>model_specs = [\n  tfma.ModelSpec(\n      prediction_key='prediction',\n      label_key='label')\n]\nmetrics_specs = [\n    tfma.MetricsSpec(metrics=[\n      tfma.MetricConfig(class_name='Accuracy'),\n      tfma.MetricConfig(class_name='ExampleCount')\n    ])\n]\nslicing_specs = [\n    tfma.SlicingSpec(),  # the empty slice represents overall dataset\n    tfma.SlicingSpec(feature_keys=['language'])\n]\neval_config = tfma.EvalConfig(\n    model_specs=model_specs,\n    metrics_specs=metrics_specs,\n    slicing_specs=slicing_specs)\nresult = tfma.analyze_raw_data(df, eval_config)\ntfma.view.render_slicing_metrics(result)\n\n# Example with Fairness Indicators\nfrom tensorflow_model_analysis.addons.fairness.post_export_metrics import\nfairness_indicators\nfrom tensorflow_model_analysis.addons.fairness.view import widget_view\nadd_metrics_callbacks = [\n    tfma.post_export_metrics.fairness_indicators(thresholds=[0.25, 0.5, 0.75])\n]\nresult = tfma.analyze_raw_data(\n    data=df,\n    metrics_specs=metrics_specs,\n    slicing_specs=slicing_specs,\n    add_metric_callbacks=add_metrics_callbacks\n)\nwidget_view.render_fairness_indicator(result)\n</code></pre> <p>data: A pandas.DataFrame, where rows correspond to examples and columns     correspond to features. One column must indicate a row's predicted label,     and one column must indicate a row's actual label.   eval_config: A <code>tfma.EvalConfig</code>, which contains various configuration     parameters including metrics, slices, and label/prediction column names.   output_path: Path to write EvalResult to.   extractors: Optional list of Extractors to apply to Extracts. Typically     these will be added by calling the default_extractors function. If no     extractors are provided, default_extractors (non-materialized) will be     used.   evaluators: Optional list of Evaluators for evaluating Extracts. Typically     these will be added by calling the default_evaluators function. If no     evaluators are provided, default_evaluators will be used.   writers: Optional list of Writers for writing Evaluation output. Typically     these will be added by calling the default_writers function. If no writers     are provided, default_writers with <code>add_metric_callbacks</code> will be used.   add_metric_callbacks: Optional list of metric callbacks (if used).</p> <p>A tfma.EvalResult to extract metrics or generate visualizations from.</p> <p>KeyError: If the prediction or label columns are not found within the     DataFrame.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def analyze_raw_data(\n    data: pd.DataFrame,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    output_path: Optional[str] = None,\n    extractors: Optional[List[extractor.Extractor]] = None,\n    evaluators: Optional[List[evaluator.Evaluator]] = None,\n    writers: Optional[List[writer.Writer]] = None,\n    add_metric_callbacks: Optional[List[types.AddMetricsCallbackType]] = None,\n) -&gt; view_types.EvalResult:\n    \"\"\"Runs TensorFlow model analysis on a pandas.DataFrame.\n\n    This function allows you to use TFMA with Pandas DataFrames. The dataframe\n    must include a 'predicted' column for the predicted label and a 'label' column\n    for the actual label.\n\n    In addition to a DataFrame, this function requires an eval_config, a\n    `tfma.EvalConfig` object containing various configuration parameters (see\n    [config.proto](https://github.com/tensorflow/model-analysis/blob/master/tensorflow_model_analysis/proto/config.proto)\n    for a comprehensive list)...\n\n    * the metrics to compute\n    * the slices to compute metrics on\n    * the DataFrame's column names for example labels and predictions ('label'\n      and 'prediction' by default)\n    * confidence interval options\n\n    This function returns a `tfma.EvalResult`, which contains TFMA's computed\n    metrics and can be used to generate plots with\n    `tfma.view.render_slicing_metrics`.\n\n    Example usage:\n\n    ```python\n    model_specs = [\n      tfma.ModelSpec(\n          prediction_key='prediction',\n          label_key='label')\n    ]\n    metrics_specs = [\n        tfma.MetricsSpec(metrics=[\n          tfma.MetricConfig(class_name='Accuracy'),\n          tfma.MetricConfig(class_name='ExampleCount')\n        ])\n    ]\n    slicing_specs = [\n        tfma.SlicingSpec(),  # the empty slice represents overall dataset\n        tfma.SlicingSpec(feature_keys=['language'])\n    ]\n    eval_config = tfma.EvalConfig(\n        model_specs=model_specs,\n        metrics_specs=metrics_specs,\n        slicing_specs=slicing_specs)\n    result = tfma.analyze_raw_data(df, eval_config)\n    tfma.view.render_slicing_metrics(result)\n\n    # Example with Fairness Indicators\n    from tensorflow_model_analysis.addons.fairness.post_export_metrics import\n    fairness_indicators\n    from tensorflow_model_analysis.addons.fairness.view import widget_view\n    add_metrics_callbacks = [\n        tfma.post_export_metrics.fairness_indicators(thresholds=[0.25, 0.5, 0.75])\n    ]\n    result = tfma.analyze_raw_data(\n        data=df,\n        metrics_specs=metrics_specs,\n        slicing_specs=slicing_specs,\n        add_metric_callbacks=add_metrics_callbacks\n    )\n    widget_view.render_fairness_indicator(result)\n    ```\n\n    Args:\n    ----\n      data: A pandas.DataFrame, where rows correspond to examples and columns\n        correspond to features. One column must indicate a row's predicted label,\n        and one column must indicate a row's actual label.\n      eval_config: A `tfma.EvalConfig`, which contains various configuration\n        parameters including metrics, slices, and label/prediction column names.\n      output_path: Path to write EvalResult to.\n      extractors: Optional list of Extractors to apply to Extracts. Typically\n        these will be added by calling the default_extractors function. If no\n        extractors are provided, default_extractors (non-materialized) will be\n        used.\n      evaluators: Optional list of Evaluators for evaluating Extracts. Typically\n        these will be added by calling the default_evaluators function. If no\n        evaluators are provided, default_evaluators will be used.\n      writers: Optional list of Writers for writing Evaluation output. Typically\n        these will be added by calling the default_writers function. If no writers\n        are provided, default_writers with `add_metric_callbacks` will be used.\n      add_metric_callbacks: Optional list of metric callbacks (if used).\n\n    Returns:\n    -------\n      A tfma.EvalResult to extract metrics or generate visualizations from.\n\n    Raises:\n    ------\n      KeyError: If the prediction or label columns are not found within the\n        DataFrame.\n    \"\"\"\n    for model_spec in eval_config.model_specs:  # pytype: disable=attribute-error\n        model_spec.prediction_key = model_spec.prediction_key or \"prediction\"\n        model_spec.label_key = model_spec.label_key or \"label\"\n        if model_spec.prediction_key not in data.columns:\n            raise KeyError(\n                \"The prediction_key column was not found. Looked for %s but found: %s\"\n                % (model_spec.prediction_key, list(data.columns))\n            )\n        if model_spec.label_key not in data.columns:\n            raise KeyError(\n                \"The label_key column was not found. Looked for %s but found: %s\"\n                % (model_spec.label_key, list(data.columns))\n            )\n\n    # TODO(b/153570803): Validity check / assertions for dataframe structure\n    if eval_config.slicing_specs is None:  # pytype: disable=attribute-error\n        eval_config.slicing_specs = [config_pb2.SlicingSpec(feature_keys=[\"\"])]\n    if output_path is None:\n        output_path = tempfile.mkdtemp()\n\n    arrow_data = table_util.CanonicalizeRecordBatch(pa.RecordBatch.from_pandas(data))\n    beam_data = beam.Create([arrow_data])\n\n    if not writers:\n        writers = default_writers(\n            output_path,\n            eval_config=eval_config,\n            add_metric_callbacks=add_metric_callbacks,\n        )\n\n    with beam.Pipeline() as p:\n        _ = (\n            p\n            | beam_data\n            | \"ExtractEvaluateAndWriteResults\"\n            &gt;&gt; ExtractEvaluateAndWriteResults(  # pylint: disable=no-value-for-parameter\n                extractors=extractors,\n                evaluators=evaluators,\n                writers=writers,\n                eval_config=eval_config,\n                output_path=output_path,\n            )\n        )\n\n    return load_eval_result(output_path)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.default_eval_shared_model","title":"default_eval_shared_model","text":"<pre><code>default_eval_shared_model(\n    eval_saved_model_path: str,\n    add_metrics_callbacks: Optional[\n        List[AddMetricsCallbackType]\n    ] = None,\n    include_default_metrics: Optional[bool] = True,\n    example_weight_key: Optional[\n        Union[str, Dict[str, str]]\n    ] = None,\n    additional_fetches: Optional[List[str]] = None,\n    blacklist_feature_fetches: Optional[List[str]] = None,\n    tags: Optional[List[str]] = None,\n    model_name: str = \"\",\n    eval_config: Optional[EvalConfig] = None,\n    custom_model_loader: Optional[ModelLoader] = None,\n    rubber_stamp: Optional[bool] = False,\n    resource_hints: Optional[Dict[str, Any]] = None,\n    backend_config: Optional[Any] = None,\n) -&gt; EvalSharedModel\n</code></pre> <p>Returns default EvalSharedModel.</p> <p>eval_saved_model_path: Path to EvalSavedModel.   add_metrics_callbacks: Optional list of callbacks for adding additional     metrics to the graph (see EvalSharedModel for more information on how to     configure additional metrics). Metrics for example count and example     weights will be added automatically. Only used if EvalSavedModel used.   include_default_metrics: DEPRECATED. Use     eval_config.options.include_default_metrics.   example_weight_key: DEPRECATED. Use     eval_config.model_specs.example_weight_key or     eval_config.model_specs.example_weight_keys.   additional_fetches: Optional prefixes of additional tensors stored in     signature_def.inputs that should be fetched at prediction time. The     \"features\" and \"labels\" tensors are handled automatically and should not     be included. Only used if EvalSavedModel used.   blacklist_feature_fetches: Optional list of tensor names in the features     dictionary which should be excluded from the fetches request. This is     useful in scenarios where features are large (e.g. images) and can lead to     excessive memory use if stored. Only used if EvalSavedModel used.   tags: Optional model tags (e.g. 'serve' for serving or 'eval' for     EvalSavedModel).   model_name: Optional name of the model being created (should match     ModelSpecs.name). The name should only be provided if multiple models are     being evaluated.   eval_config: Eval config.   custom_model_loader: Optional custom model loader for non-TF models.   rubber_stamp: True when this run is a first run without a baseline model     while a baseline is configured, the diff thresholds will be ignored.   resource_hints: The beam resource hints to apply to the PTransform which     runs inference for this model.   backend_config: Optional configuration of backend running model inference     with some prediction extractors.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def default_eval_shared_model(\n    eval_saved_model_path: str,\n    add_metrics_callbacks: Optional[List[types.AddMetricsCallbackType]] = None,\n    include_default_metrics: Optional[bool] = True,\n    example_weight_key: Optional[Union[str, Dict[str, str]]] = None,\n    additional_fetches: Optional[List[str]] = None,\n    blacklist_feature_fetches: Optional[List[str]] = None,\n    tags: Optional[List[str]] = None,\n    model_name: str = \"\",\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    custom_model_loader: Optional[types.ModelLoader] = None,\n    rubber_stamp: Optional[bool] = False,\n    resource_hints: Optional[Dict[str, Any]] = None,\n    backend_config: Optional[Any] = None,\n) -&gt; types.EvalSharedModel:\n    \"\"\"Returns default EvalSharedModel.\n\n    Args:\n    ----\n      eval_saved_model_path: Path to EvalSavedModel.\n      add_metrics_callbacks: Optional list of callbacks for adding additional\n        metrics to the graph (see EvalSharedModel for more information on how to\n        configure additional metrics). Metrics for example count and example\n        weights will be added automatically. Only used if EvalSavedModel used.\n      include_default_metrics: DEPRECATED. Use\n        eval_config.options.include_default_metrics.\n      example_weight_key: DEPRECATED. Use\n        eval_config.model_specs.example_weight_key or\n        eval_config.model_specs.example_weight_keys.\n      additional_fetches: Optional prefixes of additional tensors stored in\n        signature_def.inputs that should be fetched at prediction time. The\n        \"features\" and \"labels\" tensors are handled automatically and should not\n        be included. Only used if EvalSavedModel used.\n      blacklist_feature_fetches: Optional list of tensor names in the features\n        dictionary which should be excluded from the fetches request. This is\n        useful in scenarios where features are large (e.g. images) and can lead to\n        excessive memory use if stored. Only used if EvalSavedModel used.\n      tags: Optional model tags (e.g. 'serve' for serving or 'eval' for\n        EvalSavedModel).\n      model_name: Optional name of the model being created (should match\n        ModelSpecs.name). The name should only be provided if multiple models are\n        being evaluated.\n      eval_config: Eval config.\n      custom_model_loader: Optional custom model loader for non-TF models.\n      rubber_stamp: True when this run is a first run without a baseline model\n        while a baseline is configured, the diff thresholds will be ignored.\n      resource_hints: The beam resource hints to apply to the PTransform which\n        runs inference for this model.\n      backend_config: Optional configuration of backend running model inference\n        with *some* prediction extractors.\n    \"\"\"\n    if not eval_config:\n        # Default to tfma eval model unless eval\n        is_baseline = False\n        if tags and _LEGACY_EVAL_TAG in tags:\n            model_type = constants.TFMA_EVAL\n        elif tags and tf.saved_model.SERVING in tags:\n            model_type = constants.TF_ESTIMATOR\n        else:\n            model_type = constants.TFMA_EVAL\n        if tags is None:\n            tags = [_LEGACY_EVAL_TAG]\n    else:\n        model_spec = model_util.get_model_spec(eval_config, model_name)\n        if not model_spec:\n            raise ValueError(\n                f\"ModelSpec for model name {model_name} not found in EvalConfig: \"\n                f\"config={eval_config}\"\n            )\n        is_baseline = model_spec.is_baseline\n        model_type = model_util.get_model_type(model_spec, eval_saved_model_path, tags)\n        if tags is None:\n            # Default to serving unless tfma_eval is used.\n            if model_type == constants.TFMA_EVAL:\n                tags = [_LEGACY_EVAL_TAG]\n            else:\n                tags = [tf.saved_model.SERVING]\n        if model_spec.example_weight_key or model_spec.example_weight_keys:\n            example_weight_key = (\n                model_spec.example_weight_key or model_spec.example_weight_keys\n            )\n        if eval_config.options.HasField(\"include_default_metrics\"):\n            include_default_metrics = eval_config.options.include_default_metrics.value\n\n    model_loader = custom_model_loader\n    if not model_loader and model_type in constants.VALID_TF_MODEL_TYPES:\n        model_loader = types.ModelLoader(\n            construct_fn=model_util.model_construct_fn(\n                eval_saved_model_path=eval_saved_model_path,\n                add_metrics_callbacks=add_metrics_callbacks,\n                include_default_metrics=include_default_metrics,\n                additional_fetches=additional_fetches,\n                blacklist_feature_fetches=blacklist_feature_fetches,\n                model_type=model_type,\n                tags=tags,\n            ),\n            tags=tags,\n        )\n\n    return types.EvalSharedModel(\n        model_name=model_name,\n        model_type=model_type,\n        model_path=eval_saved_model_path,\n        add_metrics_callbacks=add_metrics_callbacks,\n        include_default_metrics=include_default_metrics,\n        example_weight_key=example_weight_key,\n        additional_fetches=additional_fetches,\n        model_loader=model_loader,\n        rubber_stamp=rubber_stamp,\n        is_baseline=is_baseline,\n        resource_hints=resource_hints,\n        backend_config=backend_config,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.default_evaluators","title":"default_evaluators","text":"<pre><code>default_evaluators(\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    eval_config: Optional[EvalConfig] = None,\n    schema: Optional[Schema] = None,\n    compute_confidence_intervals: Optional[bool] = False,\n    min_slice_size: int = 1,\n    serialize: bool = False,\n    random_seed_for_testing: Optional[int] = None,\n    config_version: Optional[int] = None,\n) -&gt; List[Evaluator]\n</code></pre> <p>Returns the default evaluators for use in ExtractAndEvaluate.</p> <p>eval_shared_model: Optional shared model (single-model evaluation) or list     of shared models (multi-model evaluation). Only required if there are     metrics to be computed in-graph using the model.   eval_config: Eval config.   schema: A schema to use for customizing default evaluators.   compute_confidence_intervals: Deprecated (use eval_config).   min_slice_size: Deprecated (use eval_config).   serialize: Deprecated.   random_seed_for_testing: Provide for deterministic tests only.   config_version: Optional config version for this evaluation. This should not     be explicitly set by users. It is only intended to be used in cases where     the provided eval_config was generated internally, and thus not a reliable     indicator of user intent.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def default_evaluators(  # pylint: disable=invalid-name\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    compute_confidence_intervals: Optional[bool] = False,\n    min_slice_size: int = 1,\n    serialize: bool = False,\n    random_seed_for_testing: Optional[int] = None,\n    config_version: Optional[int] = None,\n) -&gt; List[evaluator.Evaluator]:\n    \"\"\"Returns the default evaluators for use in ExtractAndEvaluate.\n\n    Args:\n    ----\n      eval_shared_model: Optional shared model (single-model evaluation) or list\n        of shared models (multi-model evaluation). Only required if there are\n        metrics to be computed in-graph using the model.\n      eval_config: Eval config.\n      schema: A schema to use for customizing default evaluators.\n      compute_confidence_intervals: Deprecated (use eval_config).\n      min_slice_size: Deprecated (use eval_config).\n      serialize: Deprecated.\n      random_seed_for_testing: Provide for deterministic tests only.\n      config_version: Optional config version for this evaluation. This should not\n        be explicitly set by users. It is only intended to be used in cases where\n        the provided eval_config was generated internally, and thus not a reliable\n        indicator of user intent.\n    \"\"\"\n    disabled_outputs = []\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n    if eval_config:\n        eval_config = _update_eval_config_with_defaults(eval_config, eval_shared_model)\n        disabled_outputs = eval_config.options.disabled_outputs.values\n        if _model_types(eval_shared_models) == {constants.TF_LITE} or _model_types(\n            eval_shared_models\n        ) == {constants.TF_JS}:\n            # no in-graph metrics present when tflite or tfjs is used.\n            if eval_shared_models:\n                eval_shared_models = [\n                    v._replace(include_default_metrics=False)\n                    for v in eval_shared_models\n                ]\n    if (\n        constants.METRICS_KEY in disabled_outputs\n        and constants.PLOTS_KEY in disabled_outputs\n        and constants.ATTRIBUTIONS_KEY in disabled_outputs\n    ):\n        return []\n\n    return [\n        metrics_plots_and_validations_evaluator.MetricsPlotsAndValidationsEvaluator(\n            eval_config=eval_config,\n            eval_shared_model=eval_shared_model,\n            schema=schema,\n            random_seed_for_testing=random_seed_for_testing,\n        )\n    ]\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.default_extractors","title":"default_extractors","text":"<pre><code>default_extractors(\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    eval_config: Optional[EvalConfig] = None,\n    slice_spec: Optional[List[SingleSliceSpec]] = None,\n    materialize: Optional[bool] = None,\n    tensor_adapter_config: Optional[\n        TensorAdapterConfig\n    ] = None,\n    custom_predict_extractor: Optional[Extractor] = None,\n    config_version: Optional[int] = None,\n) -&gt; List[Extractor]\n</code></pre> <p>Returns the default extractors for use in ExtractAndEvaluate.</p> <p>eval_shared_model: Shared model (single-model evaluation) or list of shared     models (multi-model evaluation). Required unless the predictions are     provided alongside of the features (i.e. model-agnostic evaluations).   eval_config: Eval config.   slice_spec: Deprecated (use EvalConfig).   materialize: True to have extractors create materialized output.   tensor_adapter_config: Tensor adapter config which specifies how to obtain     tensors from the Arrow RecordBatch. If None, an attempt will be made to     create the tensors using default TensorRepresentations.   custom_predict_extractor: Optional custom predict extractor for non-TF     models.   config_version: Optional config version for this evaluation. This should not     be explicitly set by users. It is only intended to be used in cases where     the provided eval_config was generated internally, and thus not a reliable     indicator of user intent.</p> <p>NotImplementedError: If eval_config contains mixed serving and eval models.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def default_extractors(  # pylint: disable=invalid-name\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    slice_spec: Optional[List[slicer.SingleSliceSpec]] = None,\n    materialize: Optional[bool] = None,\n    tensor_adapter_config: Optional[tensor_adapter.TensorAdapterConfig] = None,\n    custom_predict_extractor: Optional[extractor.Extractor] = None,\n    config_version: Optional[int] = None,\n) -&gt; List[extractor.Extractor]:\n    \"\"\"Returns the default extractors for use in ExtractAndEvaluate.\n\n    Args:\n    ----\n      eval_shared_model: Shared model (single-model evaluation) or list of shared\n        models (multi-model evaluation). Required unless the predictions are\n        provided alongside of the features (i.e. model-agnostic evaluations).\n      eval_config: Eval config.\n      slice_spec: Deprecated (use EvalConfig).\n      materialize: True to have extractors create materialized output.\n      tensor_adapter_config: Tensor adapter config which specifies how to obtain\n        tensors from the Arrow RecordBatch. If None, an attempt will be made to\n        create the tensors using default TensorRepresentations.\n      custom_predict_extractor: Optional custom predict extractor for non-TF\n        models.\n      config_version: Optional config version for this evaluation. This should not\n        be explicitly set by users. It is only intended to be used in cases where\n        the provided eval_config was generated internally, and thus not a reliable\n        indicator of user intent.\n\n    Raises:\n    ------\n      NotImplementedError: If eval_config contains mixed serving and eval models.\n    \"\"\"\n    if materialize is None:\n        # TODO(b/172969312): Once analysis table is supported, remove defaulting\n        #  to false unless 'analysis' is in disabled_outputs.\n        materialize = False\n    if slice_spec and eval_config:\n        raise ValueError(\"slice_spec is deprecated, only use eval_config\")\n\n    if eval_config is not None:\n        eval_config = _update_eval_config_with_defaults(eval_config, eval_shared_model)\n    tensor_representations = None\n    if tensor_adapter_config:\n        tensor_representations = tensor_adapter_config.tensor_representations\n\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n    slicing_extractors = []\n    if _has_sql_slices(eval_config):\n        slicing_extractors.append(\n            sql_slice_key_extractor.SqlSliceKeyExtractor(eval_config)\n        )\n    slicing_extractors.extend(\n        [\n            unbatch_extractor.UnbatchExtractor(),\n            slice_key_extractor.SliceKeyExtractor(\n                eval_config=eval_config, materialize=materialize\n            ),\n        ]\n    )\n\n    extract_features = features_extractor.FeaturesExtractor(\n        eval_config=eval_config, tensor_representations=tensor_representations\n    )\n    extract_labels = labels_extractor.LabelsExtractor(eval_config=eval_config)\n    extract_example_weights = example_weights_extractor.ExampleWeightsExtractor(\n        eval_config=eval_config\n    )\n    extract_materialized_predictions = (\n        materialized_predictions_extractor.MaterializedPredictionsExtractor(\n            eval_config=eval_config\n        )\n    )\n    if eval_shared_model:\n        model_types = _model_types(eval_shared_models)\n        logging.info(\"eval_shared_models have model_types: %s\", model_types)\n        assert model_types is not None\n        if (\n            not model_types.issubset(constants.VALID_TF_MODEL_TYPES)\n            and not custom_predict_extractor\n        ):\n            raise NotImplementedError(\n                \"either a custom_predict_extractor must be used or model type must \"\n                f\"be one of: {str(constants.VALID_TF_MODEL_TYPES)}. evalconfig={eval_config}\"\n            )\n\n        if model_types == {constants.MATERIALIZED_PREDICTION}:\n            return [\n                extract_features,\n                extract_labels,\n                extract_example_weights,\n                extract_materialized_predictions,\n            ] + slicing_extractors\n        elif model_types == {constants.TF_LITE}:\n            # TODO(b/163889779): Convert TFLite extractor to operate on batched\n            # extracts. Then we can remove the input extractor.\n            return [\n                extract_features,\n                transformed_features_extractor.TransformedFeaturesExtractor(\n                    eval_config=eval_config, eval_shared_model=eval_shared_model\n                ),\n                extract_labels,\n                extract_example_weights,\n                (\n                    custom_predict_extractor\n                    or tflite_predict_extractor.TFLitePredictExtractor(\n                        eval_config=eval_config, eval_shared_model=eval_shared_model\n                    )\n                ),\n            ] + slicing_extractors\n        elif constants.TF_LITE in model_types:\n            raise NotImplementedError(\n                \"support for mixing tf_lite and non-tf_lite models is not \"\n                f\"implemented: eval_config={eval_config}\"\n            )\n        elif model_types == {constants.TF_JS}:\n            return [\n                extract_features,\n                extract_labels,\n                extract_example_weights,\n                (\n                    custom_predict_extractor\n                    or tfjs_predict_extractor.TFJSPredictExtractor(\n                        eval_config=eval_config, eval_shared_model=eval_shared_model\n                    )\n                ),\n            ] + slicing_extractors\n        elif constants.TF_JS in model_types:\n            raise NotImplementedError(\n                \"support for mixing tf_js and non-tf_js models is not \"\n                f\"implemented: eval_config={eval_config}\"\n            )\n        else:\n            extractors = [extract_features]\n            if not custom_predict_extractor:\n                extractors.append(\n                    transformed_features_extractor.TransformedFeaturesExtractor(\n                        eval_config=eval_config, eval_shared_model=eval_shared_model\n                    )\n                )\n            extractors.extend(\n                [\n                    extract_labels,\n                    extract_example_weights,\n                    (\n                        custom_predict_extractor\n                        or predictions_extractor.PredictionsExtractor(\n                            eval_config=eval_config, eval_shared_model=eval_shared_model\n                        )\n                    ),\n                ]\n            )\n            extractors.extend(slicing_extractors)\n            return extractors\n    else:\n        return [\n            extract_features,\n            extract_labels,\n            extract_example_weights,\n            extract_materialized_predictions,\n        ] + slicing_extractors\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.default_writers","title":"default_writers","text":"<pre><code>default_writers(\n    output_path: Optional[str],\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    eval_config: Optional[EvalConfig] = None,\n    display_only_data_location: Optional[str] = None,\n    display_only_data_file_format: Optional[str] = None,\n    output_file_format: str = \"tfrecord\",\n    add_metric_callbacks: Optional[\n        List[AddMetricsCallbackType]\n    ] = None,\n) -&gt; List[Writer]\n</code></pre> <p>Returns the default writers for use in WriteResults.</p> <p>Note, sharding will be enabled by default if an output_file_format is provided. Filenames will be -SSSSS-of-NNNNN. where SSSSS is the shard number and NNNNN is the number of shards. <p>output_path: Output path.   eval_shared_model: Optional shared model (single-model evaluation) or list     of shared models (multi-model evaluation). Required unless the predictions     are provided alongside of the features (i.e. model-agnostic evaluations).   eval_config: Eval config for writing out config along with results. Also     used for to check for missing slices.   display_only_data_location: Optional path indicating where the examples were     read from. This is used only for display purposes - data will not actually     be read from this path.   display_only_data_file_format: Optional format of the input examples. This     is used only for display purposes.   output_file_format: File format to use when saving files. Currently only     'tfrecord' is supported.   add_metric_callbacks: Optional list of metric callbacks (if used).</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def default_writers(\n    output_path: Optional[str],\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    display_only_data_location: Optional[str] = None,\n    display_only_data_file_format: Optional[str] = None,\n    output_file_format: str = \"tfrecord\",\n    add_metric_callbacks: Optional[List[types.AddMetricsCallbackType]] = None,\n) -&gt; List[writer.Writer]:  # pylint: disable=invalid-name\n    \"\"\"Returns the default writers for use in WriteResults.\n\n    Note, sharding will be enabled by default if an output_file_format is\n    provided. Filenames will be &lt;output_path&gt;-SSSSS-of-NNNNN.&lt;output_file_format&gt;\n    where SSSSS is the shard number and NNNNN is the number of shards.\n\n    Args:\n    ----\n      output_path: Output path.\n      eval_shared_model: Optional shared model (single-model evaluation) or list\n        of shared models (multi-model evaluation). Required unless the predictions\n        are provided alongside of the features (i.e. model-agnostic evaluations).\n      eval_config: Eval config for writing out config along with results. Also\n        used for to check for missing slices.\n      display_only_data_location: Optional path indicating where the examples were\n        read from. This is used only for display purposes - data will not actually\n        be read from this path.\n      display_only_data_file_format: Optional format of the input examples. This\n        is used only for display purposes.\n      output_file_format: File format to use when saving files. Currently only\n        'tfrecord' is supported.\n      add_metric_callbacks: Optional list of metric callbacks (if used).\n    \"\"\"\n    writers = []\n\n    if not add_metric_callbacks:\n        add_metric_callbacks = []\n    # The add_metric_callbacks are used in the metrics and plots serialization\n    # code to post process the metric data by calling populate_stats_and_pop.\n    # While both the legacy (V1) and new (V2) evaluation implementations support\n    # EvalSavedModels using add_metric_callbacks, this particular code is only\n    # required for the legacy evaluation based on the MetricsAndPlotsEvaluator.\n    # The V2 MetricsAndPlotsEvaluator output requires no additional processing.\n    # Since the V1 code only supports a single EvalSharedModel, we only set the\n    # add_metrics_callbacks if a dict is not passed.\n    if (\n        eval_shared_model\n        and not isinstance(eval_shared_model, dict)\n        and not isinstance(eval_shared_model, list)\n    ):\n        add_metric_callbacks = eval_shared_model.add_metrics_callbacks\n\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n\n    if eval_config:\n        model_locations = {}\n        for v in eval_shared_models or [None]:\n            k = \"\" if v is None else v.model_name\n            model_locations[k] = (\n                \"&lt;unknown&gt;\" if v is None or v.model_path is None else v.model_path\n            )\n        writers.append(\n            eval_config_writer.EvalConfigWriter(\n                output_path,\n                eval_config=eval_config,\n                data_location=display_only_data_location,\n                data_file_format=display_only_data_file_format,\n                model_locations=model_locations,\n            )\n        )\n\n    output_paths = {\n        constants.METRICS_KEY: os.path.join(output_path, constants.METRICS_KEY),\n        constants.PLOTS_KEY: os.path.join(output_path, constants.PLOTS_KEY),\n        constants.ATTRIBUTIONS_KEY: os.path.join(\n            output_path, constants.ATTRIBUTIONS_KEY\n        ),\n        constants.VALIDATIONS_KEY: os.path.join(output_path, constants.VALIDATIONS_KEY),\n    }\n    writers.append(\n        metrics_plots_and_validations_writer.MetricsPlotsAndValidationsWriter(\n            output_paths=output_paths,\n            # Empty EvalConfig supported for backwards compatibility.\n            eval_config=eval_config or config_pb2.EvalConfig(),\n            add_metrics_callbacks=add_metric_callbacks,\n            output_file_format=output_file_format,\n            rubber_stamp=model_util.has_rubber_stamp(eval_shared_models),\n        )\n    )\n    return writers\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.is_batched_input","title":"is_batched_input","text":"<pre><code>is_batched_input(\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    eval_config: Optional[EvalConfig] = None,\n    config_version: Optional[int] = None,\n) -&gt; bool\n</code></pre> <p>Returns true if batched input should be used.</p> <p>We will keep supporting the legacy unbatched V1 PredictExtractor as it parses  the features and labels, and is the only solution currently that allows for  slicing on transformed features. Eventually we should have support for  transformed features via keras preprocessing layers.</p> <p>eval_shared_model: Shared model (single-model evaluation) or list of shared     models (multi-model evaluation). Required unless the predictions are     provided alongside of the features (i.e. model-agnostic evaluations).   eval_config: Eval config.   config_version: Optional config version for this evaluation. This should not     be explicitly set by users. It is only intended to be used in cases where     the provided eval_config was generated internally, and thus not a reliable     indicator of user intent.</p> <p>A boolean indicating if batched extractors should be used.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def is_batched_input(\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    config_version: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"Returns true if batched input should be used.\n\n     We will keep supporting the legacy unbatched V1 PredictExtractor as it parses\n     the features and labels, and is the only solution currently that allows for\n     slicing on transformed features. Eventually we should have support for\n     transformed features via keras preprocessing layers.\n\n    Args:\n    ----\n      eval_shared_model: Shared model (single-model evaluation) or list of shared\n        models (multi-model evaluation). Required unless the predictions are\n        provided alongside of the features (i.e. model-agnostic evaluations).\n      eval_config: Eval config.\n      config_version: Optional config version for this evaluation. This should not\n        be explicitly set by users. It is only intended to be used in cases where\n        the provided eval_config was generated internally, and thus not a reliable\n        indicator of user intent.\n\n    Returns:\n    -------\n      A boolean indicating if batched extractors should be used.\n    \"\"\"\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n    return not _is_legacy_eval(config_version, eval_shared_models, eval_config)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.is_legacy_estimator","title":"is_legacy_estimator","text":"<pre><code>is_legacy_estimator(\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n) -&gt; bool\n</code></pre> <p>Returns true if there is a legacy estimator.</p> <p>eval_shared_model: Shared model (single-model evaluation) or list of shared     models (multi-model evaluation). Required unless the predictions are     provided alongside of the features (i.e. model-agnostic evaluations).</p> <p>A boolean indicating if legacy predict extractor will be used.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def is_legacy_estimator(\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n) -&gt; bool:\n    \"\"\"Returns true if there is a legacy estimator.\n\n    Args:\n    ----\n      eval_shared_model: Shared model (single-model evaluation) or list of shared\n        models (multi-model evaluation). Required unless the predictions are\n        provided alongside of the features (i.e. model-agnostic evaluations).\n\n    Returns:\n    -------\n      A boolean indicating if legacy predict extractor will be used.\n    \"\"\"\n    eval_shared_models = model_util.verify_and_update_eval_shared_models(\n        eval_shared_model\n    )\n    model_types = _model_types(eval_shared_models)\n    return (\n        model_types is not None\n        and model_types == {constants.TFMA_EVAL}\n        and all(_LEGACY_EVAL_TAG in m.model_loader.tags for m in eval_shared_models)\n    )\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.load_attributions","title":"load_attributions","text":"<pre><code>load_attributions(\n    output_path: str, output_file_format: str = \"tfrecord\"\n) -&gt; Iterator[AttributionsForSlice]\n</code></pre> <p>Read and deserialize the AttributionsForSlice records.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def load_attributions(\n    output_path: str, output_file_format: str = \"tfrecord\"\n) -&gt; Iterator[AttributionsForSlice]:\n    \"\"\"Read and deserialize the AttributionsForSlice records.\"\"\"\n    for a in metrics_plots_and_validations_writer.load_and_deserialize_attributions(\n        output_path, output_file_format\n    ):\n        yield a\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.load_eval_result","title":"load_eval_result","text":"<pre><code>load_eval_result(\n    output_path: str,\n    output_file_format: Optional[str] = \"tfrecord\",\n    model_name: Optional[str] = None,\n) -&gt; EvalResult\n</code></pre> <p>Loads EvalResult object for use with the visualization functions.</p> <p>output_path: Output directory containing config, metrics, plots, etc.   output_file_format: Optional file extension to filter files by.   model_name: Optional model name. Required if multi-model evaluation was run.</p> <p>EvalResult object for use with the visualization functions.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def load_eval_result(\n    output_path: str,\n    output_file_format: Optional[str] = \"tfrecord\",\n    model_name: Optional[str] = None,\n) -&gt; view_types.EvalResult:\n    \"\"\"Loads EvalResult object for use with the visualization functions.\n\n    Args:\n    ----\n      output_path: Output directory containing config, metrics, plots, etc.\n      output_file_format: Optional file extension to filter files by.\n      model_name: Optional model name. Required if multi-model evaluation was run.\n\n    Returns:\n    -------\n      EvalResult object for use with the visualization functions.\n    \"\"\"\n    # Config, metrics, and plots files should all exist under the given output\n    # directory, but fairness plugin has a use-case where only the metrics are\n    # provided so we support all files as being optional (the EvalResult will have\n    # corresponding None values for files that are not present).\n    eval_config, data_location, file_format, model_locations = (\n        eval_config_writer.load_eval_run(output_path)\n    )\n    metrics_list = []\n    for p in metrics_plots_and_validations_writer.load_and_deserialize_metrics(\n        output_path, output_file_format\n    ):\n        metrics = view_util.convert_metrics_proto_to_dict(p, model_name=model_name)\n        if metrics is not None:\n            metrics_list.append(metrics)\n    plots_list = []\n    for p in metrics_plots_and_validations_writer.load_and_deserialize_plots(\n        output_path, output_file_format\n    ):\n        plots = view_util.convert_plots_proto_to_dict(p, model_name=model_name)\n        if plots is not None:\n            plots_list.append(plots)\n    attributions_list = []\n    for a in metrics_plots_and_validations_writer.load_and_deserialize_attributions(\n        output_path, output_file_format\n    ):\n        attributions = view_util.convert_attributions_proto_to_dict(\n            a, model_name=model_name\n        )\n        if attributions is not None:\n            attributions_list.append(attributions)\n    if not model_locations:\n        model_location = \"\"\n    elif model_name is None:\n        model_location = list(model_locations.values())[0]\n    else:\n        model_location = model_locations[model_name]\n    return view_types.EvalResult(  # pytype: disable=wrong-arg-types\n        slicing_metrics=metrics_list,\n        plots=plots_list,\n        attributions=attributions_list,\n        config=eval_config,\n        data_location=data_location,\n        file_format=file_format,\n        model_location=model_location,\n    )\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.load_eval_results","title":"load_eval_results","text":"<pre><code>load_eval_results(\n    output_paths: Union[str, List[str]],\n    output_file_format: Optional[str] = \"tfrecord\",\n    mode: str = MODEL_CENTRIC_MODE,\n    model_name: Optional[str] = None,\n) -&gt; EvalResults\n</code></pre> <p>Loads results for multiple models or multiple data sets.</p> <p>output_paths: A single path or list of output paths of completed tfma runs.   output_file_format: Optional file extension to filter files by.   mode: The mode of the evaluation. Currently, tfma.DATA_CENTRIC_MODE and     tfma.MODEL_CENTRIC_MODE are supported.   model_name: Filters to only return results for given model. If unset all     models are returned.</p> <p>An EvalResults containing the evaluation results serialized at output_paths.   This can be used to construct a time series view.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def load_eval_results(\n    output_paths: Union[str, List[str]],\n    output_file_format: Optional[str] = \"tfrecord\",\n    mode: str = constants.MODEL_CENTRIC_MODE,\n    model_name: Optional[str] = None,\n) -&gt; view_types.EvalResults:\n    \"\"\"Loads results for multiple models or multiple data sets.\n\n    Args:\n    ----\n      output_paths: A single path or list of output paths of completed tfma runs.\n      output_file_format: Optional file extension to filter files by.\n      mode: The mode of the evaluation. Currently, tfma.DATA_CENTRIC_MODE and\n        tfma.MODEL_CENTRIC_MODE are supported.\n      model_name: Filters to only return results for given model. If unset all\n        models are returned.\n\n    Returns:\n    -------\n      An EvalResults containing the evaluation results serialized at output_paths.\n      This can be used to construct a time series view.\n    \"\"\"\n    results = []\n    if not isinstance(output_paths, list):\n        output_paths = [output_paths]\n    for output_path in output_paths:\n        if model_name is None:\n            _, _, _, model_locations = eval_config_writer.load_eval_run(output_path)\n            model_names = list(model_locations)\n        else:\n            model_names = [model_name]\n        for model_name in model_names:\n            results.append(\n                load_eval_result(output_path, output_file_format, model_name=model_name)\n            )\n    return make_eval_results(results, mode)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.load_metrics","title":"load_metrics","text":"<pre><code>load_metrics(\n    output_path: str, output_file_format: str = \"tfrecord\"\n) -&gt; Iterator[MetricsForSlice]\n</code></pre> <p>Read and deserialize the MetricsForSlice records.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def load_metrics(\n    output_path: str, output_file_format: str = \"tfrecord\"\n) -&gt; Iterator[MetricsForSlice]:\n    \"\"\"Read and deserialize the MetricsForSlice records.\"\"\"\n    for m in metrics_plots_and_validations_writer.load_and_deserialize_metrics(\n        output_path, output_file_format\n    ):\n        yield m\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.load_plots","title":"load_plots","text":"<pre><code>load_plots(\n    output_path: str, output_file_format: str = \"tfrecord\"\n) -&gt; Iterator[PlotsForSlice]\n</code></pre> <p>Read and deserialize the PlotsForSlice records.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def load_plots(\n    output_path: str, output_file_format: str = \"tfrecord\"\n) -&gt; Iterator[PlotsForSlice]:\n    \"\"\"Read and deserialize the PlotsForSlice records.\"\"\"\n    for p in metrics_plots_and_validations_writer.load_and_deserialize_plots(\n        output_path, output_file_format\n    ):\n        yield p\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.load_validation_result","title":"load_validation_result","text":"<pre><code>load_validation_result(\n    output_path: str, output_file_format: str = \"\"\n) -&gt; ValidationResult\n</code></pre> <p>Read and deserialize the ValidationResult.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def load_validation_result(\n    output_path: str, output_file_format: str = \"\"\n) -&gt; ValidationResult:\n    \"\"\"Read and deserialize the ValidationResult.\"\"\"\n    return metrics_plots_and_validations_writer.load_and_deserialize_validation_result(\n        output_path, output_file_format\n    )\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.make_eval_results","title":"make_eval_results","text":"<pre><code>make_eval_results(\n    results: List[EvalResult], mode: str\n) -&gt; EvalResults\n</code></pre> <p>Run model analysis for a single model on multiple data sets.</p> <p>results: A list of TFMA evaluation results.   mode: The mode of the evaluation. Currently, tfma.DATA_CENTRIC_MODE and     tfma.MODEL_CENTRIC_MODE are supported.</p> <p>An <code>tfma.view.EvalResults</code> object containing all evaluation results. This   can be used to construct a time series view.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def make_eval_results(\n    results: List[view_types.EvalResult], mode: str\n) -&gt; view_types.EvalResults:\n    \"\"\"Run model analysis for a single model on multiple data sets.\n\n    Args:\n    ----\n      results: A list of TFMA evaluation results.\n      mode: The mode of the evaluation. Currently, tfma.DATA_CENTRIC_MODE and\n        tfma.MODEL_CENTRIC_MODE are supported.\n\n    Returns:\n    -------\n      An `tfma.view.EvalResults` object containing all evaluation results. This\n      can be used to construct a time series view.\n    \"\"\"\n    return view_types.EvalResults(results, mode)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.multiple_data_analysis","title":"multiple_data_analysis","text":"<pre><code>multiple_data_analysis(\n    model_location: str, data_locations: List[str], **kwargs\n) -&gt; EvalResults\n</code></pre> <p>Run model analysis for a single model on multiple data sets.</p> <p>model_location: The location of the exported eval saved model.   data_locations: A list of data set locations.   **kwargs: The args used for evaluation. See tfma.run_model_analysis() for     details.</p> <p>A tfma.EvalResults containing all the evaluation results with the same order   as data_locations.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def multiple_data_analysis(\n    model_location: str, data_locations: List[str], **kwargs\n) -&gt; view_types.EvalResults:\n    \"\"\"Run model analysis for a single model on multiple data sets.\n\n    Args:\n    ----\n      model_location: The location of the exported eval saved model.\n      data_locations: A list of data set locations.\n      **kwargs: The args used for evaluation. See tfma.run_model_analysis() for\n        details.\n\n    Returns:\n    -------\n      A tfma.EvalResults containing all the evaluation results with the same order\n      as data_locations.\n    \"\"\"\n    results = []\n    for d in data_locations:\n        results.append(single_model_analysis(model_location, d, **kwargs))\n    return view_types.EvalResults(results, constants.DATA_CENTRIC_MODE)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.multiple_model_analysis","title":"multiple_model_analysis","text":"<pre><code>multiple_model_analysis(\n    model_locations: List[str], data_location: str, **kwargs\n) -&gt; EvalResults\n</code></pre> <p>Run model analysis for multiple models on the same data set.</p> <p>model_locations: A list of paths to the export eval saved model.   data_location: The location of the data files.   **kwargs: The args used for evaluation. See tfma.single_model_analysis() for     details.</p> <p>A tfma.EvalResults containing all the evaluation results with the same order   as model_locations.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def multiple_model_analysis(\n    model_locations: List[str], data_location: str, **kwargs\n) -&gt; view_types.EvalResults:\n    \"\"\"Run model analysis for multiple models on the same data set.\n\n    Args:\n    ----\n      model_locations: A list of paths to the export eval saved model.\n      data_location: The location of the data files.\n      **kwargs: The args used for evaluation. See tfma.single_model_analysis() for\n        details.\n\n    Returns:\n    -------\n      A tfma.EvalResults containing all the evaluation results with the same order\n      as model_locations.\n    \"\"\"\n    results = []\n    for m in model_locations:\n        results.append(single_model_analysis(m, data_location, **kwargs))\n    return view_types.EvalResults(results, constants.MODEL_CENTRIC_MODE)\n</code></pre>"},{"location":"api_docs/python/tfma/#tensorflow_model_analysis.run_model_analysis","title":"run_model_analysis","text":"<pre><code>run_model_analysis(\n    eval_shared_model: Optional[\n        MaybeMultipleEvalSharedModels\n    ] = None,\n    eval_config: Optional[EvalConfig] = None,\n    data_location: str = \"\",\n    file_format: str = \"tfrecords\",\n    output_path: Optional[str] = None,\n    extractors: Optional[List[Extractor]] = None,\n    evaluators: Optional[List[Evaluator]] = None,\n    writers: Optional[List[Writer]] = None,\n    pipeline_options: Optional[Any] = None,\n    slice_spec: Optional[List[SingleSliceSpec]] = None,\n    write_config: Optional[bool] = True,\n    compute_confidence_intervals: Optional[bool] = False,\n    min_slice_size: int = 1,\n    random_seed_for_testing: Optional[int] = None,\n    schema: Optional[Schema] = None,\n) -&gt; Union[EvalResult, EvalResults]\n</code></pre> <p>Runs TensorFlow model analysis.</p> <p>It runs a Beam pipeline to compute the slicing metrics exported in TensorFlow Eval SavedModel and returns the results.</p> <p>This is a simplified API for users who want to quickly get something running locally. Users who wish to create their own Beam pipelines can use the Evaluate PTransform instead.</p> <p>eval_shared_model: Optional shared model (single-model evaluation) or list     of shared models (multi-model evaluation). Only required if needed by     default extractors, evaluators, or writers.   eval_config: Eval config.   data_location: The location of the data files.   file_format: The file format of the data, can be either 'text' or     'tfrecords' for now. By default, 'tfrecords' will be used.   output_path: The directory to output metrics and results to. If None, we use     a temporary directory.   extractors: Optional list of Extractors to apply to Extracts. Typically     these will be added by calling the default_extractors function. If no     extractors are provided, default_extractors (non-materialized) will be     used.   evaluators: Optional list of Evaluators for evaluating Extracts. Typically     these will be added by calling the default_evaluators function. If no     evaluators are provided, default_evaluators will be used.   writers: Optional list of Writers for writing Evaluation output. Typically     these will be added by calling the default_writers function. If no writers     are provided, default_writers will be used.   pipeline_options: Optional arguments to run the Pipeline, for instance     whether to run directly.   slice_spec: Deprecated (use EvalConfig).   write_config: Deprecated (use EvalConfig).   compute_confidence_intervals: Deprecated (use EvalConfig).   min_slice_size: Deprecated (use EvalConfig).   random_seed_for_testing: Provide for deterministic tests only.   schema: Optional tf.Metadata schema of the input data.</p> <p>An EvalResult that can be used with the TFMA visualization functions.</p> <p>ValueError: If the file_format is unknown to us.</p> Source code in <code>tensorflow_model_analysis/api/model_eval_lib.py</code> <pre><code>def run_model_analysis(\n    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels] = None,\n    eval_config: Optional[config_pb2.EvalConfig] = None,\n    data_location: str = \"\",\n    file_format: str = \"tfrecords\",\n    output_path: Optional[str] = None,\n    extractors: Optional[List[extractor.Extractor]] = None,\n    evaluators: Optional[List[evaluator.Evaluator]] = None,\n    writers: Optional[List[writer.Writer]] = None,\n    pipeline_options: Optional[Any] = None,\n    slice_spec: Optional[List[slicer.SingleSliceSpec]] = None,\n    write_config: Optional[bool] = True,\n    compute_confidence_intervals: Optional[bool] = False,\n    min_slice_size: int = 1,\n    random_seed_for_testing: Optional[int] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n) -&gt; Union[view_types.EvalResult, view_types.EvalResults]:\n    \"\"\"Runs TensorFlow model analysis.\n\n    It runs a Beam pipeline to compute the slicing metrics exported in TensorFlow\n    Eval SavedModel and returns the results.\n\n    This is a simplified API for users who want to quickly get something running\n    locally. Users who wish to create their own Beam pipelines can use the\n    Evaluate PTransform instead.\n\n    Args:\n    ----\n      eval_shared_model: Optional shared model (single-model evaluation) or list\n        of shared models (multi-model evaluation). Only required if needed by\n        default extractors, evaluators, or writers.\n      eval_config: Eval config.\n      data_location: The location of the data files.\n      file_format: The file format of the data, can be either 'text' or\n        'tfrecords' for now. By default, 'tfrecords' will be used.\n      output_path: The directory to output metrics and results to. If None, we use\n        a temporary directory.\n      extractors: Optional list of Extractors to apply to Extracts. Typically\n        these will be added by calling the default_extractors function. If no\n        extractors are provided, default_extractors (non-materialized) will be\n        used.\n      evaluators: Optional list of Evaluators for evaluating Extracts. Typically\n        these will be added by calling the default_evaluators function. If no\n        evaluators are provided, default_evaluators will be used.\n      writers: Optional list of Writers for writing Evaluation output. Typically\n        these will be added by calling the default_writers function. If no writers\n        are provided, default_writers will be used.\n      pipeline_options: Optional arguments to run the Pipeline, for instance\n        whether to run directly.\n      slice_spec: Deprecated (use EvalConfig).\n      write_config: Deprecated (use EvalConfig).\n      compute_confidence_intervals: Deprecated (use EvalConfig).\n      min_slice_size: Deprecated (use EvalConfig).\n      random_seed_for_testing: Provide for deterministic tests only.\n      schema: Optional tf.Metadata schema of the input data.\n\n    Returns:\n    -------\n      An EvalResult that can be used with the TFMA visualization functions.\n\n    Raises:\n    ------\n      ValueError: If the file_format is unknown to us.\n    \"\"\"\n    _assert_tensorflow_version()\n\n    if output_path is None:\n        output_path = tempfile.mkdtemp()\n    if not tf.io.gfile.exists(output_path):\n        tf.io.gfile.makedirs(output_path)\n\n    if eval_config is None:\n        config_version = 1\n        eval_shared_models = model_util.verify_and_update_eval_shared_models(\n            eval_shared_model\n        )\n        eval_config = _default_eval_config(\n            eval_shared_models,\n            slice_spec,\n            write_config,\n            compute_confidence_intervals,\n            min_slice_size,\n        )\n    else:\n        config_version = 2\n        eval_config = _update_eval_config_with_defaults(eval_config, eval_shared_model)\n\n    tensor_adapter_config = None\n    with beam.Pipeline(options=pipeline_options) as p:\n        if file_format == \"tfrecords\":\n            if is_batched_input(eval_shared_model, eval_config, config_version):\n                if is_legacy_estimator(eval_shared_model):\n                    tfxio = raw_tf_record.RawTfRecordTFXIO(\n                        file_pattern=data_location,\n                        raw_record_column_name=constants.ARROW_INPUT_COLUMN,\n                        telemetry_descriptors=[\"StandaloneTFMA\"],\n                    )\n                else:\n                    tfxio = tf_example_record.TFExampleRecord(\n                        file_pattern=data_location,\n                        schema=schema,\n                        raw_record_column_name=constants.ARROW_INPUT_COLUMN,\n                        telemetry_descriptors=[\"StandaloneTFMA\"],\n                    )\n                    if schema is not None:\n                        tensor_adapter_config = tensor_adapter.TensorAdapterConfig(\n                            arrow_schema=tfxio.ArrowSchema(),\n                            tensor_representations=tfxio.TensorRepresentations(),\n                        )\n                data = p | \"ReadFromTFRecordToArrow\" &gt;&gt; tfxio.BeamSource()\n            else:\n                data = p | \"ReadFromTFRecord\" &gt;&gt; beam.io.ReadFromTFRecord(\n                    file_pattern=data_location,\n                    compression_type=beam.io.filesystem.CompressionTypes.AUTO,\n                )\n        elif file_format == \"text\":\n            tfxio = raw_tf_record.RawBeamRecordTFXIO(\n                physical_format=\"csv\",\n                raw_record_column_name=constants.ARROW_INPUT_COLUMN,\n                telemetry_descriptors=[\"StandaloneTFMA\"],\n            )\n            data = (\n                p\n                | \"ReadFromText\"\n                &gt;&gt; beam.io.textio.ReadFromText(\n                    data_location, coder=beam.coders.BytesCoder()\n                )\n                | \"ConvertToArrow\" &gt;&gt; tfxio.BeamSource()\n            )\n        else:\n            raise ValueError(f\"unknown file_format: {file_format}\")\n\n        # pylint: disable=no-value-for-parameter\n        _ = data | \"ExtractEvaluateAndWriteResults\" &gt;&gt; ExtractEvaluateAndWriteResults(\n            eval_config=eval_config,\n            eval_shared_model=eval_shared_model,\n            display_only_data_location=data_location,\n            display_only_file_format=file_format,\n            output_path=output_path,\n            extractors=extractors,\n            evaluators=evaluators,\n            writers=writers,\n            random_seed_for_testing=random_seed_for_testing,\n            tensor_adapter_config=tensor_adapter_config,\n            schema=schema,\n            config_version=config_version,\n        )\n        # pylint: enable=no-value-for-parameter\n\n    if len(eval_config.model_specs) &lt;= 1:\n        return load_eval_result(output_path)\n    else:\n        results = []\n        for spec in eval_config.model_specs:\n            results.append(load_eval_result(output_path, model_name=spec.name))\n        return view_types.EvalResults(results, constants.MODEL_CENTRIC_MODE)\n</code></pre>"},{"location":"api_docs/python/deprecated/","title":"All symbols in TensorFlow Model Analysis","text":"<ul> <li><code>tfma</code></li> <li><code>tfma.EvalConfig</code></li> <li><code>tfma.EvalResult</code></li> <li><code>tfma.EvalSharedModel</code></li> <li><code>tfma.ExtractAndEvaluate</code></li> <li><code>tfma.ExtractEvaluateAndWriteResults</code></li> <li><code>tfma.FeaturesPredictionsLabels</code></li> <li><code>tfma.InputsToExtracts</code></li> <li><code>tfma.MaterializedColumn</code></li> <li><code>tfma.Validate</code></li> <li><code>tfma.WriteResults</code></li> <li><code>tfma.compound_key</code></li> <li><code>tfma.constants</code></li> <li><code>tfma.create_keys_key</code></li> <li><code>tfma.create_values_key</code></li> <li><code>tfma.default_eval_shared_model</code></li> <li><code>tfma.default_evaluators</code></li> <li><code>tfma.default_extractors</code></li> <li><code>tfma.default_writers</code></li> <li><code>tfma.evaluators</code></li> <li><code>tfma.evaluators.AnalysisTableEvaluator</code></li> <li><code>tfma.evaluators.Evaluator</code></li> <li><code>tfma.evaluators.MetricsAndPlotsEvaluator</code></li> <li><code>tfma.evaluators.verify_evaluator</code></li> <li><code>tfma.export</code></li> <li><code>tfma.export.build_parsing_eval_input_receiver_fn</code></li> <li><code>tfma.exporter</code></li> <li><code>tfma.exporter.FinalExporter</code></li> <li><code>tfma.exporter.LatestExporter</code></li> <li><code>tfma.extractors</code></li> <li><code>tfma.extractors.Extractor</code></li> <li><code>tfma.extractors.FeatureExtractor</code></li> <li><code>tfma.extractors.Filter</code></li> <li><code>tfma.extractors.PredictExtractor</code></li> <li><code>tfma.extractors.SliceKeyExtractor</code></li> <li><code>tfma.load_eval_result</code></li> <li><code>tfma.load_eval_results</code></li> <li><code>tfma.make_eval_results</code></li> <li><code>tfma.multiple_data_analysis</code></li> <li><code>tfma.multiple_model_analysis</code></li> <li><code>tfma.post_export_metrics</code></li> <li><code>tfma.post_export_metrics.auc</code></li> <li><code>tfma.post_export_metrics.auc_plots</code></li> <li><code>tfma.post_export_metrics.calibration_plot_and_prediction_histogram</code></li> <li><code>tfma.post_export_metrics.confusion_matrix_at_thresholds</code></li> <li><code>tfma.post_export_metrics.example_count</code></li> <li><code>tfma.post_export_metrics.example_weight</code></li> <li><code>tfma.post_export_metrics.mean_absolute_error</code></li> <li><code>tfma.post_export_metrics.precision_at_k</code></li> <li><code>tfma.post_export_metrics.recall_at_k</code></li> <li><code>tfma.post_export_metrics.squared_pearson_correlation</code></li> <li><code>tfma.run_model_analysis</code></li> <li><code>tfma.types</code></li> <li><code>tfma.types.EvalSharedModel</code></li> <li><code>tfma.types.FeaturesPredictionsLabels</code></li> <li><code>tfma.types.MaterializedColumn</code></li> <li><code>tfma.types.ValueWithConfidenceInterval</code></li> <li><code>tfma.types.is_tensor</code></li> <li><code>tfma.unique_key</code></li> <li><code>tfma.validators</code></li> <li><code>tfma.validators.Validator</code></li> <li><code>tfma.version</code></li> <li><code>tfma.view</code></li> <li><code>tfma.view.render_plot</code></li> <li><code>tfma.view.render_slicing_metrics</code></li> <li><code>tfma.view.render_time_series</code></li> <li><code>tfma.writers</code></li> <li><code>tfma.writers.Write</code></li> <li><code>tfma.writers.Writer</code></li> </ul>"},{"location":"api_docs/python/deprecated/tfma/","title":"Tfma","text":""},{"location":"api_docs/python/deprecated/tfma/#module-tfma","title":"Module: tfma","text":"<p>Defined in <code>__init__.py</code>.</p> <p>Init module for TensorFlow Model Analysis.</p>"},{"location":"api_docs/python/deprecated/tfma/#modules","title":"Modules","text":"<p><code>constants</code> module: Constants used in TensorFlow Model Analysis.</p> <p><code>evaluators</code> module: Init module for TensorFlow Model Analysis evaluators.</p> <p><code>export</code> module: Library for exporting the EvalSavedModel.</p> <p><code>exporter</code> module: <code>Exporter</code> class represents different flavors of model export.</p> <p><code>extractors</code> module: Init module for TensorFlow Model Analysis extractors.</p> <p><code>post_export_metrics</code> module: Library containing helpers for adding post export metrics for evaluation.</p> <p><code>types</code> module: Types.</p> <p><code>validators</code> module: Init module for TensorFlow Model Analysis validators.</p> <p><code>version</code> module: Contains the version string for this release of TFMA.</p> <p><code>view</code> module: Initializes TFMA's view rendering api.</p> <p><code>writers</code> module: Init module for TensorFlow Model Analysis writers.</p>"},{"location":"api_docs/python/deprecated/tfma/#classes","title":"Classes","text":"<p><code>class EvalConfig</code>: Config used for extraction and evaluation.</p> <p><code>class EvalResult</code>: EvalResult(slicing_metrics, plots, config)</p> <p><code>class EvalSharedModel</code>: Shared model used during extraction and evaluation.</p> <p><code>class FeaturesPredictionsLabels</code>: FeaturesPredictionsLabels(input_ref, features, predictions, labels)</p> <p><code>class MaterializedColumn</code>: MaterializedColumn(name, value)</p>"},{"location":"api_docs/python/deprecated/tfma/#functions","title":"Functions","text":"<p><code>ExtractAndEvaluate(...)</code>: Performs Extractions and Evaluations in provided order.</p> <p><code>ExtractEvaluateAndWriteResults(...)</code>: PTransform for performing extraction, evaluation, and writing results.</p> <p><code>InputsToExtracts(...)</code>: Converts serialized inputs (e.g. examples) to Extracts.</p> <p><code>Validate(...)</code>: Performs validation of alternative evaluations.</p> <p><code>WriteResults(...)</code>: Writes Evaluation or Validation results using given writers.</p> <p><code>compound_key(...)</code>: Returns a compound key based on a list of keys.</p> <p><code>create_keys_key(...)</code>: Creates secondary key representing the sparse keys associated with key.</p> <p><code>create_values_key(...)</code>: Creates secondary key representing sparse values associated with key.</p> <p><code>default_eval_shared_model(...)</code>: Returns default EvalSharedModel.</p> <p><code>default_evaluators(...)</code>: Returns the default evaluators for use in ExtractAndEvaluate.</p> <p><code>default_extractors(...)</code>: Returns the default extractors for use in ExtractAndEvaluate.</p> <p><code>default_writers(...)</code>: Returns the default writers for use in WriteResults.</p> <p><code>load_eval_result(...)</code>: Creates an EvalResult object for use with the visualization functions.</p> <p><code>load_eval_results(...)</code>: Run model analysis for a single model on multiple data sets.</p> <p><code>make_eval_results(...)</code>: Run model analysis for a single model on multiple data sets.</p> <p><code>multiple_data_analysis(...)</code>: Run model analysis for a single model on multiple data sets.</p> <p><code>multiple_model_analysis(...)</code>: Run model analysis for multiple models on the same data set.</p> <p><code>run_model_analysis(...)</code>: Runs TensorFlow model analysis.</p> <p><code>unique_key(...)</code>: Returns a unique key given a list of current keys.</p>"},{"location":"api_docs/python/deprecated/tfma/#other-members","title":"Other Members","text":""},{"location":"api_docs/python/deprecated/tfma/#ANALYSIS_KEY","title":"<code>ANALYSIS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#ATTRIBUTIONS_KEY","title":"<code>ATTRIBUTIONS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#BASELINE_KEY","title":"<code>BASELINE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#BASELINE_SCORE_KEY","title":"<code>BASELINE_SCORE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#CANDIDATE_KEY","title":"<code>CANDIDATE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#DATA_CENTRIC_MODE","title":"<code>DATA_CENTRIC_MODE</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#EXAMPLE_SCORE_KEY","title":"<code>EXAMPLE_SCORE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#FEATURES_KEY","title":"<code>FEATURES_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#FEATURES_PREDICTIONS_LABELS_KEY","title":"<code>FEATURES_PREDICTIONS_LABELS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#INPUT_KEY","title":"<code>INPUT_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#LABELS_KEY","title":"<code>LABELS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#METRICS_KEY","title":"<code>METRICS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#MODEL_CENTRIC_MODE","title":"<code>MODEL_CENTRIC_MODE</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#PLOTS_KEY","title":"<code>PLOTS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#PREDICTIONS_KEY","title":"<code>PREDICTIONS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/#VERSION","title":"<code>VERSION</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/","title":"EvalConfig","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#tfmaevalconfig","title":"tfma.EvalConfig","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#class-evalconfig","title":"Class <code>EvalConfig</code>","text":"<p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Config used for extraction and evaluation.</p>"},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    cls,\n    model_location,\n    data_location=None,\n    slice_spec=None,\n    example_weight_metric_key=None,\n    num_bootstrap_samples=1\n)\n</code></pre> <p>Create new instance of EvalConfig(model_location, data_location, slice_spec, example_weight_metric_key, num_bootstrap_samples)</p>"},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#model_location","title":"<code>model_location</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#data_location","title":"<code>data_location</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#slice_spec","title":"<code>slice_spec</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#example_weight_metric_key","title":"<code>example_weight_metric_key</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalConfig/#num_bootstrap_samples","title":"<code>num_bootstrap_samples</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalResult/","title":"EvalResult","text":""},{"location":"api_docs/python/deprecated/tfma/EvalResult/#tfmaevalresult","title":"tfma.EvalResult","text":""},{"location":"api_docs/python/deprecated/tfma/EvalResult/#class-evalresult","title":"Class <code>EvalResult</code>","text":"<p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>EvalResult(slicing_metrics, plots, config)</p>"},{"location":"api_docs/python/deprecated/tfma/EvalResult/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    _cls,\n    slicing_metrics,\n    plots,\n    config\n)\n</code></pre> <p>Create new instance of EvalResult(slicing_metrics, plots, config)</p>"},{"location":"api_docs/python/deprecated/tfma/EvalResult/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/EvalResult/#slicing_metrics","title":"<code>slicing_metrics</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalResult/#plots","title":"<code>plots</code>","text":""},{"location":"api_docs/python/deprecated/tfma/EvalResult/#config","title":"<code>config</code>","text":""},{"location":"api_docs/python/deprecated/tfma/ExtractAndEvaluate/","title":"ExtractAndEvaluate","text":""},{"location":"api_docs/python/deprecated/tfma/ExtractAndEvaluate/#tfmaextractandevaluate","title":"tfma.ExtractAndEvaluate","text":"<pre><code>tfma.ExtractAndEvaluate(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Performs Extractions and Evaluations in provided order.</p>"},{"location":"api_docs/python/deprecated/tfma/ExtractEvaluateAndWriteResults/","title":"ExtractEvaluateAndWriteResults","text":""},{"location":"api_docs/python/deprecated/tfma/ExtractEvaluateAndWriteResults/#tfmaextractevaluateandwriteresults","title":"tfma.ExtractEvaluateAndWriteResults","text":"<pre><code>tfma.ExtractEvaluateAndWriteResults(\n    *args,\n    **kwargs\n)\n</code></pre> <p>PTransform for performing extraction, evaluation, and writing results.</p> <p>Users who want to construct their own Beam pipelines instead of using the lightweight run_model_analysis functions should use this PTransform.</p> <p>Example usage: eval_shared_model = tfma.default_eval_shared_model( eval_saved_model_path=model_location, add_metrics_callbacks=[...]) with beam.Pipeline(runner=...) as p: _ = (p | 'ReadData' &gt;&gt; beam.io.ReadFromTFRecord(data_location) | 'ExtractEvaluateAndWriteResults' &gt;&gt; tfma.ExtractEvaluateAndWriteResults( eval_shared_model=eval_shared_model, output_path=output_path, display_only_data_location=data_location, slice_spec=slice_spec, ...)) result = tfma.load_eval_result(output_path=output_path) tfma.view.render_slicing_metrics(result)</p> <p>Note that the exact serialization format is an internal implementation detail and subject to change. Users should only use the TFMA functions to write and read the results.</p>"},{"location":"api_docs/python/deprecated/tfma/ExtractEvaluateAndWriteResults/#args","title":"Args:","text":"<ul> <li><code>examples</code>: PCollection of input examples. Can be any format the     model accepts (e.g. string containing CSV row, TensorFlow.Example, etc).</li> <li><code>eval_shared_model</code>: Shared model parameters for EvalSavedModel     including any additional metrics (see EvalSharedModel for more information     on how to configure additional metrics).</li> <li><code>output_path</code>: Path to output metrics and plots results.</li> <li><code>display_only_data_location</code>: Optional path indicating where the     examples were read from. This is used only for display purposes - data will     not actually be read from this path.</li> <li><code>slice_spec</code>: Optional list of SingleSliceSpec specifying the slices     to slice the data into. If None, defaults to the overall slice.</li> <li><code>desired_batch_size</code>: Optional batch size for batching in Predict and     Aggregate.</li> <li><code>extractors</code>: Optional list of Extractors to apply to Extracts.     Typically these will be added by calling the default_extractors function. If     no extractors are provided, default_extractors (non-materialized) will be     used.</li> <li><code>evaluators</code>: Optional list of Evaluators for evaluating Extracts.     Typically these will be added by calling the default_evaluators function. If     no evaluators are provided, default_evaluators will be used.</li> <li><code>writers</code>: Optional list of Writers for writing Evaluation output.     Typically these will be added by calling the default_writers function. If no     writers are provided, default_writers will be used.</li> <li><code>write_config</code>: True to write the config along with the results.</li> <li><code>num_bootstrap_samples</code>: Optional, set to at least 20 in order to     calculate metrics with confidence intervals.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/ExtractEvaluateAndWriteResults/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If matching Extractor not found for an Evaluator.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/ExtractEvaluateAndWriteResults/#returns","title":"Returns:","text":"<p>PDone.</p>"},{"location":"api_docs/python/deprecated/tfma/InputsToExtracts/","title":"InputsToExtracts","text":""},{"location":"api_docs/python/deprecated/tfma/InputsToExtracts/#tfmainputstoextracts","title":"tfma.InputsToExtracts","text":"<pre><code>tfma.InputsToExtracts(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Converts serialized inputs (e.g. examples) to Extracts.</p>"},{"location":"api_docs/python/deprecated/tfma/Validate/","title":"Validate","text":""},{"location":"api_docs/python/deprecated/tfma/Validate/#tfmavalidate","title":"tfma.Validate","text":"<pre><code>tfma.Validate(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Performs validation of alternative evaluations.</p>"},{"location":"api_docs/python/deprecated/tfma/Validate/#args","title":"Args:","text":"<ul> <li><code>extracts</code>: PCollection of extracts.</li> <li><code>alternatives</code>: Dict of PTransforms (Extracts -&gt; Evaluation) whose     output will be compared for validation purposes (e.g. 'baseline' vs     'candidate').</li> <li><code>validators</code>: List of validators for validating the output from     running the alternatives. The Validation outputs produced by the validators     will be merged into a single output. If there are overlapping output keys,     later outputs will replace earlier outputs sharing the same key.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/Validate/#returns","title":"Returns:","text":"<p>Validation dict.</p>"},{"location":"api_docs/python/deprecated/tfma/WriteResults/","title":"WriteResults","text":""},{"location":"api_docs/python/deprecated/tfma/WriteResults/#tfmawriteresults","title":"tfma.WriteResults","text":"<pre><code>tfma.WriteResults(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Writes Evaluation or Validation results using given writers.</p>"},{"location":"api_docs/python/deprecated/tfma/WriteResults/#args","title":"Args:","text":"<ul> <li><code>evaluation_or_validation</code>: Evaluation or Validation output.</li> <li><code>writers</code>: Writes to use for writing out output.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/WriteResults/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If Evaluation or Validation is empty.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/WriteResults/#returns","title":"Returns:","text":"<p>beam.pvalue.PDone.</p>"},{"location":"api_docs/python/deprecated/tfma/compound_key/","title":"Compound key","text":""},{"location":"api_docs/python/deprecated/tfma/compound_key/#tfmacompound_key","title":"tfma.compound_key","text":"<pre><code>tfma.compound_key(\n    keys,\n    separator=KEY_SEPARATOR\n)\n</code></pre> <p>Defined in <code>util.py</code>.</p> <p>Returns a compound key based on a list of keys.</p>"},{"location":"api_docs/python/deprecated/tfma/compound_key/#args","title":"Args:","text":"<ul> <li><code>keys</code>: Keys used to make up compound key.</li> <li><code>separator</code>: Separator between keys. To ensure the keys can be parsed     out of any compound key created, any use of a separator within a key will be     replaced by two separators.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/constants/","title":"Constants","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#module-tfmaconstants","title":"Module: tfma.constants","text":"<p>Defined in <code>constants.py</code>.</p> <p>Constants used in TensorFlow Model Analysis.</p>"},{"location":"api_docs/python/deprecated/tfma/constants/#other-members","title":"Other Members","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#ANALYSIS_KEY","title":"<code>ANALYSIS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#ATTRIBUTIONS_KEY","title":"<code>ATTRIBUTIONS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#BASELINE_KEY","title":"<code>BASELINE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#BASELINE_SCORE_KEY","title":"<code>BASELINE_SCORE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#CANDIDATE_KEY","title":"<code>CANDIDATE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#DATA_CENTRIC_MODE","title":"<code>DATA_CENTRIC_MODE</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#EXAMPLE_SCORE_KEY","title":"<code>EXAMPLE_SCORE_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#FEATURES_KEY","title":"<code>FEATURES_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#FEATURES_PREDICTIONS_LABELS_KEY","title":"<code>FEATURES_PREDICTIONS_LABELS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#INPUT_KEY","title":"<code>INPUT_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#LABELS_KEY","title":"<code>LABELS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#METRICS_KEY","title":"<code>METRICS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#METRICS_NAMESPACE","title":"<code>METRICS_NAMESPACE</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#MODEL_CENTRIC_MODE","title":"<code>MODEL_CENTRIC_MODE</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#PLACEHOLDER","title":"<code>PLACEHOLDER</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#PLOTS_KEY","title":"<code>PLOTS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#PREDICTIONS_KEY","title":"<code>PREDICTIONS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#SLICE_KEYS_KEY","title":"<code>SLICE_KEYS_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#SLICE_KEY_TYPES_KEY","title":"<code>SLICE_KEY_TYPES_KEY</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#SPARSE_PLACEHOLDER","title":"<code>SPARSE_PLACEHOLDER</code>","text":""},{"location":"api_docs/python/deprecated/tfma/constants/#UNKNOWN_EVAL_MODE","title":"<code>UNKNOWN_EVAL_MODE</code>","text":""},{"location":"api_docs/python/deprecated/tfma/create_keys_key/","title":"Create keys key","text":""},{"location":"api_docs/python/deprecated/tfma/create_keys_key/#tfmacreate_keys_key","title":"tfma.create_keys_key","text":"<pre><code>tfma.create_keys_key(key)\n</code></pre> <p>Defined in <code>util.py</code>.</p> <p>Creates secondary key representing the sparse keys associated with key.</p>"},{"location":"api_docs/python/deprecated/tfma/create_values_key/","title":"Create values key","text":""},{"location":"api_docs/python/deprecated/tfma/create_values_key/#tfmacreate_values_key","title":"tfma.create_values_key","text":"<pre><code>tfma.create_values_key(key)\n</code></pre> <p>Defined in <code>util.py</code>.</p> <p>Creates secondary key representing sparse values associated with key.</p>"},{"location":"api_docs/python/deprecated/tfma/default_eval_shared_model/","title":"Default eval shared model","text":""},{"location":"api_docs/python/deprecated/tfma/default_eval_shared_model/#tfmadefault_eval_shared_model","title":"tfma.default_eval_shared_model","text":"<pre><code>tfma.default_eval_shared_model(\n    eval_saved_model_path,\n    add_metrics_callbacks=None,\n    include_default_metrics=True,\n    example_weight_key=None,\n    additional_fetches=None\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Returns default EvalSharedModel.</p>"},{"location":"api_docs/python/deprecated/tfma/default_eval_shared_model/#args","title":"Args:","text":"<ul> <li><code>eval_saved_model_path</code>: Path to EvalSavedModel.</li> <li><code>add_metrics_callbacks</code>: Optional list of callbacks for adding     additional metrics to the graph (see EvalSharedModel for more information on     how to configure additional metrics). Metrics for example counts and example     weight will be added automatically.</li> <li><code>include_default_metrics</code>: True to include the default metrics that     are part of the saved model graph during evaluation.</li> <li><code>example_weight_key</code>: Deprecated.</li> <li><code>additional_fetches</code>: Prefixes of additional tensors stored in     signature_def.inputs that should be fetched at prediction time. The     \"features\" and \"labels\" tensors are handled automatically and should not be     included.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/default_evaluators/","title":"Default evaluators","text":""},{"location":"api_docs/python/deprecated/tfma/default_evaluators/#tfmadefault_evaluators","title":"tfma.default_evaluators","text":"<pre><code>tfma.default_evaluators(\n    eval_shared_model,\n    desired_batch_size=None,\n    num_bootstrap_samples=None\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Returns the default evaluators for use in ExtractAndEvaluate.</p>"},{"location":"api_docs/python/deprecated/tfma/default_evaluators/#args","title":"Args:","text":"<ul> <li><code>eval_shared_model</code>: Shared model parameters for EvalSavedModel.</li> <li><code>desired_batch_size</code>: Optional batch size for batching in Aggregate.</li> <li><code>num_bootstrap_samples</code>: Number of bootstrap samples to draw. If more     than 1, confidence intervals will be computed for metrics. Suggested value     is at least 20.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/default_extractors/","title":"Default extractors","text":""},{"location":"api_docs/python/deprecated/tfma/default_extractors/#tfmadefault_extractors","title":"tfma.default_extractors","text":"<pre><code>tfma.default_extractors(\n    eval_shared_model,\n    slice_spec=None,\n    desired_batch_size=None,\n    materialize=True\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Returns the default extractors for use in ExtractAndEvaluate.</p>"},{"location":"api_docs/python/deprecated/tfma/default_extractors/#args","title":"Args:","text":"<ul> <li><code>eval_shared_model</code>: Shared model parameters for EvalSavedModel.</li> <li><code>slice_spec</code>: Optional list of SingleSliceSpec specifying the slices     to slice the data into. If None, defaults to the overall slice.</li> <li><code>desired_batch_size</code>: Optional batch size for batching in Aggregate.</li> <li><code>materialize</code>: True to have extractors create materialized output.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/default_writers/","title":"Default writers","text":""},{"location":"api_docs/python/deprecated/tfma/default_writers/#tfmadefault_writers","title":"tfma.default_writers","text":"<pre><code>tfma.default_writers(output_path)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Returns the default writers for use in WriteResults.</p>"},{"location":"api_docs/python/deprecated/tfma/default_writers/#args","title":"Args:","text":"<ul> <li><code>output_path</code>: Path to store results files under.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/evaluators/","title":"Evaluators","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/#module-tfmaevaluators","title":"Module: tfma.evaluators","text":"<p>Defined in <code>evaluators/__init__.py</code>.</p> <p>Init module for TensorFlow Model Analysis evaluators.</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/#classes","title":"Classes","text":"<p><code>class Evaluator</code>: Evaluator(stage_name, run_after, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/#functions","title":"Functions","text":"<p><code>AnalysisTableEvaluator(...)</code>: Creates an Evaluator for returning Extracts data for analysis.</p> <p><code>MetricsAndPlotsEvaluator(...)</code>: Creates an Evaluator for evaluating metrics and plots.</p> <p><code>verify_evaluator(...)</code>: Verifies evaluator is matched with an extractor.</p>"},{"location":"api_docs/python/deprecated/tfma/export/","title":"Export","text":""},{"location":"api_docs/python/deprecated/tfma/export/#module-tfmaexport","title":"Module: tfma.export","text":"<p>Defined in <code>eval_saved_model/export.py</code>.</p> <p>Library for exporting the EvalSavedModel.</p>"},{"location":"api_docs/python/deprecated/tfma/export/#functions","title":"Functions","text":"<p><code>build_parsing_eval_input_receiver_fn(...)</code>: Build a eval_input_receiver_fn expecting fed tf.Examples.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/","title":"Exporter","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/#module-tfmaexporter","title":"Module: tfma.exporter","text":"<p>Defined in <code>eval_saved_model/exporter.py</code>.</p> <p><code>Exporter</code> class represents different flavors of model export.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/#classes","title":"Classes","text":"<p><code>class FinalExporter</code>: This class exports the EvalSavedModel in the end.</p> <p><code>class LatestExporter</code>: This class regularly exports the EvalSavedModel.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/","title":"Extractors","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/#module-tfmaextractors","title":"Module: tfma.extractors","text":"<p>Defined in <code>extractors/__init__.py</code>.</p> <p>Init module for TensorFlow Model Analysis extractors.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/#classes","title":"Classes","text":"<p><code>class Extractor</code>: Extractor(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/#functions","title":"Functions","text":"<p><code>FeatureExtractor(...)</code></p> <p><code>Filter(...)</code>: Filters extracts to include/exclude specified keys.</p> <p><code>PredictExtractor(...)</code>: Creates an Extractor for TFMAPredict.</p> <p><code>SliceKeyExtractor(...)</code>: Creates an extractor for extracting slice keys.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/#other-members","title":"Other Members","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/#FEATURE_EXTRACTOR_STAGE_NAME","title":"<code>FEATURE_EXTRACTOR_STAGE_NAME</code>","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/#LAST_EXTRACTOR_STAGE_NAME","title":"<code>LAST_EXTRACTOR_STAGE_NAME</code>","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/#PREDICT_EXTRACTOR_STAGE_NAME","title":"<code>PREDICT_EXTRACTOR_STAGE_NAME</code>","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/#SLICE_KEY_EXTRACTOR_STAGE_NAME","title":"<code>SLICE_KEY_EXTRACTOR_STAGE_NAME</code>","text":""},{"location":"api_docs/python/deprecated/tfma/load_eval_result/","title":"Load eval result","text":""},{"location":"api_docs/python/deprecated/tfma/load_eval_result/#tfmaload_eval_result","title":"tfma.load_eval_result","text":"<pre><code>tfma.load_eval_result(output_path)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Creates an EvalResult object for use with the visualization functions.</p>"},{"location":"api_docs/python/deprecated/tfma/load_eval_results/","title":"Load eval results","text":""},{"location":"api_docs/python/deprecated/tfma/load_eval_results/#tfmaload_eval_results","title":"tfma.load_eval_results","text":"<pre><code>tfma.load_eval_results(\n    output_paths,\n    mode\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Run model analysis for a single model on multiple data sets.</p>"},{"location":"api_docs/python/deprecated/tfma/load_eval_results/#args","title":"Args:","text":"<ul> <li><code>output_paths</code>: A list of output paths of completed tfma runs.</li> <li><code>mode</code>: The mode of the evaluation. Currently, tfma.DATA_CENTRIC_MODE     and tfma.MODEL_CENTRIC_MODE are supported.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/load_eval_results/#returns","title":"Returns:","text":"<p>An EvalResults containing the evaluation results serialized at output_paths. This can be used to construct a time series view.</p>"},{"location":"api_docs/python/deprecated/tfma/make_eval_results/","title":"Make eval results","text":""},{"location":"api_docs/python/deprecated/tfma/make_eval_results/#tfmamake_eval_results","title":"tfma.make_eval_results","text":"<pre><code>tfma.make_eval_results(\n    results,\n    mode\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Run model analysis for a single model on multiple data sets.</p>"},{"location":"api_docs/python/deprecated/tfma/make_eval_results/#args","title":"Args:","text":"<ul> <li><code>results</code>: A list of TFMA evaluation results.</li> <li><code>mode</code>: The mode of the evaluation. Currently, tfma.DATA_CENTRIC_MODE     and tfma.MODEL_CENTRIC_MODE are supported.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/make_eval_results/#returns","title":"Returns:","text":"<p>An EvalResults containing all evaluation results. This can be used to construct a time series view.</p>"},{"location":"api_docs/python/deprecated/tfma/multiple_data_analysis/","title":"Multiple data analysis","text":""},{"location":"api_docs/python/deprecated/tfma/multiple_data_analysis/#tfmamultiple_data_analysis","title":"tfma.multiple_data_analysis","text":"<pre><code>tfma.multiple_data_analysis(\n    model_location,\n    data_locations,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Run model analysis for a single model on multiple data sets.</p>"},{"location":"api_docs/python/deprecated/tfma/multiple_data_analysis/#args","title":"Args:","text":"<ul> <li><code>model_location</code>: The location of the exported eval saved model.</li> <li><code>data_locations</code>: A list of data set locations.</li> <li><code>**kwargs</code>: The args used for evaluation. See     tfma.run_model_analysis() for details.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/multiple_data_analysis/#returns","title":"Returns:","text":"<p>A tfma.EvalResults containing all the evaluation results with the same order as data_locations.</p>"},{"location":"api_docs/python/deprecated/tfma/multiple_model_analysis/","title":"Multiple model analysis","text":""},{"location":"api_docs/python/deprecated/tfma/multiple_model_analysis/#tfmamultiple_model_analysis","title":"tfma.multiple_model_analysis","text":"<pre><code>tfma.multiple_model_analysis(\n    model_locations,\n    data_location,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Run model analysis for multiple models on the same data set.</p>"},{"location":"api_docs/python/deprecated/tfma/multiple_model_analysis/#args","title":"Args:","text":"<ul> <li><code>model_locations</code>: A list of paths to the export eval saved model.</li> <li><code>data_location</code>: The location of the data files.</li> <li><code>**kwargs</code>: The args used for evaluation. See     tfma.run_model_analysis() for details.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/multiple_model_analysis/#returns","title":"Returns:","text":"<p>A tfma.EvalResults containing all the evaluation results with the same order as model_locations.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/","title":"Post export metrics","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/#module-tfmapost_export_metrics","title":"Module: tfma.post_export_metrics","text":"<p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>Library containing helpers for adding post export metrics for evaluation.</p> <p>These post export metrics can be included in the add_post_export_metrics parameter of Evaluate to compute them.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/#functions","title":"Functions","text":"<p><code>auc(...)</code>: This is the function that the user calls.</p> <p><code>auc_plots(...)</code>: This is the function that the user calls.</p> <p><code>calibration_plot_and_prediction_histogram(...)</code>: This is the function that the user calls.</p> <p><code>confusion_matrix_at_thresholds(...)</code>: This is the function that the user calls.</p> <p><code>example_count(...)</code>: This is the function that the user calls.</p> <p><code>example_weight(...)</code>: This is the function that the user calls.</p> <p><code>mean_absolute_error(...)</code>: This is the function that the user calls.</p> <p><code>precision_at_k(...)</code>: This is the function that the user calls.</p> <p><code>recall_at_k(...)</code>: This is the function that the user calls.</p> <p><code>squared_pearson_correlation(...)</code>: This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/#other-members","title":"Other Members","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/#DEFAULT_KEY_PREFERENCE","title":"<code>DEFAULT_KEY_PREFERENCE</code>","text":""},{"location":"api_docs/python/deprecated/tfma/run_model_analysis/","title":"Run model analysis","text":""},{"location":"api_docs/python/deprecated/tfma/run_model_analysis/#tfmarun_model_analysis","title":"tfma.run_model_analysis","text":"<pre><code>tfma.run_model_analysis(\n    eval_shared_model,\n    data_location,\n    file_format='tfrecords',\n    slice_spec=None,\n    output_path=None,\n    extractors=None,\n    evaluators=None,\n    writers=None,\n    write_config=True,\n    pipeline_options=None,\n    num_bootstrap_samples=1\n)\n</code></pre> <p>Defined in <code>api/model_eval_lib.py</code>.</p> <p>Runs TensorFlow model analysis.</p> <p>It runs a Beam pipeline to compute the slicing metrics exported in TensorFlow Eval SavedModel and returns the results.</p> <p>This is a simplified API for users who want to quickly get something running locally. Users who wish to create their own Beam pipelines can use the Evaluate PTransform instead.</p>"},{"location":"api_docs/python/deprecated/tfma/run_model_analysis/#args","title":"Args:","text":"<ul> <li><code>eval_shared_model</code>: Shared model parameters for EvalSavedModel     including any additional metrics (see EvalSharedModel for more information     on how to configure additional metrics).</li> <li><code>data_location</code>: The location of the data files.</li> <li><code>file_format</code>: The file format of the data, can be either 'text' or     'tfrecords' for now. By default, 'tfrecords' will be used.</li> <li><code>slice_spec</code>: A list of tfma.slicer.SingleSliceSpec. Each spec     represents a way to slice the data. If None, defaults to the overall slice.     Example usages: # TODO(xinzha): add more use cases once they are supported     in frontend.<ul> <li>tfma.SingleSiceSpec(): no slice, metrics are computed on overall data.</li> <li>tfma.SingleSiceSpec(columns=['country']): slice based on features in     column \"country\". We might get metrics for slice \"country:us\",     \"country:jp\", and etc in results.</li> <li>tfma.SingleSiceSpec(features=[('country', 'us')]): metrics are computed     on slice \"country:us\".</li> </ul> </li> <li><code>output_path</code>: The directory to output metrics and results to. If     None, we use a temporary directory.</li> <li><code>extractors</code>: Optional list of Extractors to apply to Extracts.     Typically these will be added by calling the default_extractors function. If     no extractors are provided, default_extractors (non-materialized) will be     used.</li> <li><code>evaluators</code>: Optional list of Evaluators for evaluating Extracts.     Typically these will be added by calling the default_evaluators function. If     no evaluators are provided, default_evaluators will be used.</li> <li><code>writers</code>: Optional list of Writers for writing Evaluation output.     Typically these will be added by calling the default_writers function. If no     writers are provided, default_writers will be used.</li> <li><code>write_config</code>: True to write the config along with the results.</li> <li><code>pipeline_options</code>: Optional arguments to run the Pipeline, for     instance whether to run directly.</li> <li><code>num_bootstrap_samples</code>: Optional, set to at least 20 in order to     calculate metrics with confidence intervals.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/run_model_analysis/#returns","title":"Returns:","text":"<p>An EvalResult that can be used with the TFMA visualization functions.</p>"},{"location":"api_docs/python/deprecated/tfma/run_model_analysis/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If the file_format is unknown to us.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/types/","title":"Types","text":""},{"location":"api_docs/python/deprecated/tfma/types/#module-tfmatypes","title":"Module: tfma.types","text":"<p>Defined in <code>types.py</code>.</p> <p>Types.</p>"},{"location":"api_docs/python/deprecated/tfma/types/#classes","title":"Classes","text":"<p><code>class EvalSharedModel</code>: Shared model used during extraction and evaluation.</p> <p><code>class FeaturesPredictionsLabels</code>: FeaturesPredictionsLabels(input_ref, features, predictions, labels)</p> <p><code>class MaterializedColumn</code>: MaterializedColumn(name, value)</p> <p><code>class ValueWithConfidenceInterval</code>: Represents a value with mean, upper, and lower bound.</p>"},{"location":"api_docs/python/deprecated/tfma/types/#functions","title":"Functions","text":"<p><code>is_tensor(...)</code></p>"},{"location":"api_docs/python/deprecated/tfma/unique_key/","title":"Unique key","text":""},{"location":"api_docs/python/deprecated/tfma/unique_key/#tfmaunique_key","title":"tfma.unique_key","text":"<pre><code>tfma.unique_key(\n    key,\n    current_keys,\n    update_keys=False\n)\n</code></pre> <p>Defined in <code>util.py</code>.</p> <p>Returns a unique key given a list of current keys.</p> <p>If the key exists in current_keys then a new key with _1, _2, ..., etc appended will be returned, otherwise the key will be returned as passed.</p>"},{"location":"api_docs/python/deprecated/tfma/unique_key/#args","title":"Args:","text":"<ul> <li><code>key</code>: desired key name.</li> <li><code>current_keys</code>: List of current key names.</li> <li><code>update_keys</code>: True to append the new key to current_keys.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/validators/","title":"Validators","text":""},{"location":"api_docs/python/deprecated/tfma/validators/#module-tfmavalidators","title":"Module: tfma.validators","text":"<p>Defined in <code>validators/__init__.py</code>.</p> <p>Init module for TensorFlow Model Analysis validators.</p>"},{"location":"api_docs/python/deprecated/tfma/validators/#classes","title":"Classes","text":"<p><code>class Validator</code>: Validator(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/version/","title":"Version","text":""},{"location":"api_docs/python/deprecated/tfma/version/#module-tfmaversion","title":"Module: tfma.version","text":"<p>Defined in <code>version.py</code>.</p> <p>Contains the version string for this release of TFMA.</p>"},{"location":"api_docs/python/deprecated/tfma/version/#other-members","title":"Other Members","text":""},{"location":"api_docs/python/deprecated/tfma/version/#VERSION","title":"<code>VERSION</code>","text":""},{"location":"api_docs/python/deprecated/tfma/view/","title":"View","text":""},{"location":"api_docs/python/deprecated/tfma/view/#module-tfmaview","title":"Module: tfma.view","text":"<p>Defined in <code>view/__init__.py</code>.</p> <p>Initializes TFMA's view rendering api.</p>"},{"location":"api_docs/python/deprecated/tfma/view/#functions","title":"Functions","text":"<p><code>render_plot(...)</code>: Renders the plot view as widget.</p> <p><code>render_slicing_metrics(...)</code>: Renders the slicing metrics view as widget.</p> <p><code>render_time_series(...)</code>: Renders the time series view as widget.</p>"},{"location":"api_docs/python/deprecated/tfma/writers/","title":"Writers","text":""},{"location":"api_docs/python/deprecated/tfma/writers/#module-tfmawriters","title":"Module: tfma.writers","text":"<p>Defined in <code>writers/__init__.py</code>.</p> <p>Init module for TensorFlow Model Analysis writers.</p>"},{"location":"api_docs/python/deprecated/tfma/writers/#classes","title":"Classes","text":"<p><code>class Writer</code>: Writer(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/writers/#functions","title":"Functions","text":"<p><code>Write(...)</code>: Writes given Evaluation or Validation data using given writer PTransform.</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/AnalysisTableEvaluator/","title":"AnalysisTableEvaluator","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/AnalysisTableEvaluator/#tfmaevaluatorsanalysistableevaluator","title":"tfma.evaluators.AnalysisTableEvaluator","text":"<pre><code>tfma.evaluators.AnalysisTableEvaluator(\n    key=constants.ANALYSIS_KEY,\n    run_after=extractor.LAST_EXTRACTOR_STAGE_NAME,\n    include=None,\n    exclude=None\n)\n</code></pre> <p>Defined in <code>evaluators/analysis_table_evaluator.py</code>.</p> <p>Creates an Evaluator for returning Extracts data for analysis.</p> <p>If both include and exclude are None then tfma.INPUT_KEY extracts will be excluded by default.</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/AnalysisTableEvaluator/#args","title":"Args:","text":"<ul> <li><code>key</code>: Name to use for key in Evaluation output.</li> <li><code>run_after</code>: Extractor to run after (None means before any     extractors).</li> <li><code>include</code>: Keys of extracts to include in output. Keys starting with     '_' are automatically filtered out at write time.</li> <li><code>exclude</code>: Keys of extracts to exclude from output.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/evaluators/AnalysisTableEvaluator/#returns","title":"Returns:","text":"<p>Evaluator for collecting analysis data. The output is stored under the key 'analysis'.</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/AnalysisTableEvaluator/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If both include and exclude are used.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/","title":"Evaluator","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/#tfmaevaluatorsevaluator","title":"tfma.evaluators.Evaluator","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/#class-evaluator","title":"Class <code>Evaluator</code>","text":"<p>Defined in <code>evaluators/evaluator.py</code>.</p> <p>Evaluator(stage_name, run_after, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    _cls,\n    stage_name,\n    run_after,\n    ptransform\n)\n</code></pre> <p>Create new instance of Evaluator(stage_name, run_after, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/#stage_name","title":"<code>stage_name</code>","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/#run_after","title":"<code>run_after</code>","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/Evaluator/#ptransform","title":"<code>ptransform</code>","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/MetricsAndPlotsEvaluator/","title":"MetricsAndPlotsEvaluator","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/MetricsAndPlotsEvaluator/#tfmaevaluatorsmetricsandplotsevaluator","title":"tfma.evaluators.MetricsAndPlotsEvaluator","text":"<pre><code>tfma.evaluators.MetricsAndPlotsEvaluator(\n    eval_shared_model,\n    desired_batch_size=None,\n    metrics_key=constants.METRICS_KEY,\n    plots_key=constants.PLOTS_KEY,\n    run_after=slice_key_extractor.SLICE_KEY_EXTRACTOR_STAGE_NAME,\n    num_bootstrap_samples=1\n)\n</code></pre> <p>Defined in <code>evaluators/metrics_and_plots_evaluator.py</code>.</p> <p>Creates an Evaluator for evaluating metrics and plots.</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/MetricsAndPlotsEvaluator/#args","title":"Args:","text":"<ul> <li><code>eval_shared_model</code>: Shared model parameters for EvalSavedModel.</li> <li><code>desired_batch_size</code>: Optional batch size for batching in Aggregate.</li> <li><code>metrics_key</code>: Name to use for metrics key in Evaluation output.</li> <li><code>plots_key</code>: Name to use for plots key in Evaluation output.</li> <li><code>run_after</code>: Extractor to run after (None means before any     extractors).</li> <li><code>num_bootstrap_samples</code>: Number of bootstrap samples to draw. If more     than 1, confidence intervals will be computed for metrics. Suggested value     is at least 20.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/evaluators/MetricsAndPlotsEvaluator/#returns","title":"Returns:","text":"<p>Evaluator for evaluating metrics and plots. The output will be stored under 'metrics' and 'plots' keys.</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/verify_evaluator/","title":"Verify evaluator","text":""},{"location":"api_docs/python/deprecated/tfma/evaluators/verify_evaluator/#tfmaevaluatorsverify_evaluator","title":"tfma.evaluators.verify_evaluator","text":"<pre><code>tfma.evaluators.verify_evaluator(\n    evaluator,\n    extractors\n)\n</code></pre> <p>Defined in <code>evaluators/evaluator.py</code>.</p> <p>Verifies evaluator is matched with an extractor.</p>"},{"location":"api_docs/python/deprecated/tfma/evaluators/verify_evaluator/#args","title":"Args:","text":"<ul> <li><code>evaluator</code>: Evaluator to verify.</li> <li><code>extractors</code>: Extractors to use in verification.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/evaluators/verify_evaluator/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If an Extractor cannot be found for the Evaluator.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/export/build_parsing_eval_input_receiver_fn/","title":"Build parsing eval input receiver fn","text":""},{"location":"api_docs/python/deprecated/tfma/export/build_parsing_eval_input_receiver_fn/#tfmaexportbuild_parsing_eval_input_receiver_fn","title":"tfma.export.build_parsing_eval_input_receiver_fn","text":"<pre><code>tfma.export.build_parsing_eval_input_receiver_fn(\n    feature_spec,\n    label_key\n)\n</code></pre> <p>Defined in <code>eval_saved_model/export.py</code>.</p> <p>Build a eval_input_receiver_fn expecting fed tf.Examples.</p> <p>Creates a eval_input_receiver_fn that expects a serialized tf.Example fed into a string placeholder. The function parses the tf.Example according to the provided feature_spec, and returns all parsed Tensors as features.</p>"},{"location":"api_docs/python/deprecated/tfma/export/build_parsing_eval_input_receiver_fn/#args","title":"Args:","text":"<ul> <li><code>feature_spec</code>: A dict of string to     <code>VarLenFeature</code>/<code>FixedLenFeature</code>.</li> <li><code>label_key</code>: The key for the label column in the feature_spec. Note     that the label must be part of the feature_spec. If None, does not pass a     label to the EvalInputReceiver (note that label_key must be None and not     simply the empty string for this case).</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/export/build_parsing_eval_input_receiver_fn/#returns","title":"Returns:","text":"<p>A eval_input_receiver_fn suitable for use with TensorFlow model analysis.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/","title":"FinalExporter","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#tfmaexporterfinalexporter","title":"tfma.exporter.FinalExporter","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#class-finalexporter","title":"Class <code>FinalExporter</code>","text":"<p>Defined in <code>eval_saved_model/exporter.py</code>.</p> <p>This class exports the EvalSavedModel in the end.</p> <p>This class performs a single export in the end of training.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#__init__","title":"<code>__init__</code>","text":"<pre><code>__init__(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Wrapped function.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#name","title":"<code>name</code>","text":"<p>Directory name.</p> <p>A directory name under the export base directory where exports of this type are written. Should not be <code>None</code> nor empty.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#methods","title":"Methods","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#export","title":"<code>export</code>","text":"<pre><code>export(\n    estimator,\n    export_path,\n    checkpoint_path,\n    eval_result,\n    is_the_final_export\n)\n</code></pre> <p>Exports the given <code>Estimator</code> to a specific format.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#args","title":"Args:","text":"<ul> <li><code>estimator</code>: the <code>Estimator</code> to export.</li> <li><code>export_path</code>: A string containing a directory where to write the     export.</li> <li><code>checkpoint_path</code>: The checkpoint path to export.</li> <li><code>eval_result</code>: The output of <code>Estimator.evaluate</code> on this checkpoint.</li> <li><code>is_the_final_export</code>: This boolean is True when this is an export in     the end of training. It is False for the intermediate exports during the     training. When passing <code>Exporter</code> to <code>tf.estimator.train_and_evaluate</code> <code>is_the_final_export</code> is always False if <code>TrainSpec.max_steps</code> is <code>None</code>.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/exporter/FinalExporter/#returns","title":"Returns:","text":"<p>The string path to the exported directory or <code>None</code> if export is skipped.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/","title":"LatestExporter","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#tfmaexporterlatestexporter","title":"tfma.exporter.LatestExporter","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#class-latestexporter","title":"Class <code>LatestExporter</code>","text":"<p>Defined in <code>eval_saved_model/exporter.py</code>.</p> <p>This class regularly exports the EvalSavedModel.</p> <p>In addition to exporting, this class also garbage collects stale exports.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#__init__","title":"<code>__init__</code>","text":"<pre><code>__init__(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Wrapped function.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#name","title":"<code>name</code>","text":"<p>Directory name.</p> <p>A directory name under the export base directory where exports of this type are written. Should not be <code>None</code> nor empty.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#methods","title":"Methods","text":""},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#export","title":"<code>export</code>","text":"<pre><code>export(\n    estimator,\n    export_path,\n    checkpoint_path,\n    eval_result,\n    is_the_final_export\n)\n</code></pre> <p>Exports the given <code>Estimator</code> to a specific format.</p>"},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#args","title":"Args:","text":"<ul> <li><code>estimator</code>: the <code>Estimator</code> to export.</li> <li><code>export_path</code>: A string containing a directory where to write the     export.</li> <li><code>checkpoint_path</code>: The checkpoint path to export.</li> <li><code>eval_result</code>: The output of <code>Estimator.evaluate</code> on this checkpoint.</li> <li><code>is_the_final_export</code>: This boolean is True when this is an export in     the end of training. It is False for the intermediate exports during the     training. When passing <code>Exporter</code> to <code>tf.estimator.train_and_evaluate</code> <code>is_the_final_export</code> is always False if <code>TrainSpec.max_steps</code> is <code>None</code>.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/exporter/LatestExporter/#returns","title":"Returns:","text":"<p>The string path to the exported directory or <code>None</code> if export is skipped.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/Extractor/","title":"Extractor","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/Extractor/#tfmaextractorsextractor","title":"tfma.extractors.Extractor","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/Extractor/#class-extractor","title":"Class <code>Extractor</code>","text":"<p>Defined in <code>extractors/extractor.py</code>.</p> <p>Extractor(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/Extractor/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    _cls,\n    stage_name,\n    ptransform\n)\n</code></pre> <p>Create new instance of Extractor(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/Extractor/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/Extractor/#stage_name","title":"<code>stage_name</code>","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/Extractor/#ptransform","title":"<code>ptransform</code>","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/FeatureExtractor/","title":"FeatureExtractor","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/FeatureExtractor/#tfmaextractorsfeatureextractor","title":"tfma.extractors.FeatureExtractor","text":"<pre><code>tfma.extractors.FeatureExtractor(\n    additional_extracts=None,\n    excludes=None,\n    extract_source=constants.FEATURES_PREDICTIONS_LABELS_KEY\n)\n</code></pre> <p>Defined in <code>extractors/feature_extractor.py</code>.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/Filter/","title":"Filter","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/Filter/#tfmaextractorsfilter","title":"tfma.extractors.Filter","text":"<pre><code>tfma.extractors.Filter(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Filters extracts to include/exclude specified keys.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/Filter/#args","title":"Args:","text":"<ul> <li><code>extracts</code>: PCollection of extracts.</li> <li><code>include</code>: Keys to include in output.</li> <li><code>exclude</code>: Keys to exclude from output.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/extractors/Filter/#returns","title":"Returns:","text":"<p>Filtered PCollection of Extracts.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/Filter/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If both include and exclude are used.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/extractors/PredictExtractor/","title":"PredictExtractor","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/PredictExtractor/#tfmaextractorspredictextractor","title":"tfma.extractors.PredictExtractor","text":"<pre><code>tfma.extractors.PredictExtractor(\n    eval_shared_model,\n    desired_batch_size=None,\n    materialize=True\n)\n</code></pre> <p>Defined in <code>extractors/predict_extractor.py</code>.</p> <p>Creates an Extractor for TFMAPredict.</p> <p>The extractor's PTransform loads and runs the eval_saved_model against every example yielding a copy of the Extracts input with an additional extract of type FeaturesPredictionsLabels keyed by tfma.FEATURES_PREDICTIONS_LABELS_KEY.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/PredictExtractor/#args","title":"Args:","text":"<ul> <li><code>eval_shared_model</code>: Shared model parameters for EvalSavedModel.</li> <li><code>desired_batch_size</code>: Optional batch size for batching in Aggregate.</li> <li><code>materialize</code>: True to call the FeatureExtractor to add     MaterializedColumn entries for the features, predictions, and labels.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/extractors/PredictExtractor/#returns","title":"Returns:","text":"<p>Extractor for extracting features, predictions, labels, and other tensors during predict.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/SliceKeyExtractor/","title":"SliceKeyExtractor","text":""},{"location":"api_docs/python/deprecated/tfma/extractors/SliceKeyExtractor/#tfmaextractorsslicekeyextractor","title":"tfma.extractors.SliceKeyExtractor","text":"<pre><code>tfma.extractors.SliceKeyExtractor(\n    slice_spec=None,\n    materialize=True\n)\n</code></pre> <p>Defined in <code>extractors/slice_key_extractor.py</code>.</p> <p>Creates an extractor for extracting slice keys.</p> <p>The incoming Extracts must contain a FeaturesPredictionsLabels extract keyed by tfma.FEATURES_PREDICTIONS_LABELS_KEY. Typically this will be obtained by calling the PredictExtractor.</p> <p>The extractor's PTransform yields a copy of the Extracts input with an additional extract pointing at the list of SliceKeyType values keyed by tfma.SLICE_KEY_TYPES_KEY. If materialize is True then a materialized version of the slice keys will be added under the key tfma.SLICE_KEYS_KEY.</p>"},{"location":"api_docs/python/deprecated/tfma/extractors/SliceKeyExtractor/#args","title":"Args:","text":"<ul> <li><code>slice_spec</code>: Optional list of SingleSliceSpec specifying the slices     to slice the data into. If None, defaults to the overall slice.</li> <li><code>materialize</code>: True to add MaterializedColumn entries for the slice     keys.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/extractors/SliceKeyExtractor/#returns","title":"Returns:","text":"<p>Extractor for slice keys.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/auc/","title":"Auc","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/auc/#tfmapost_export_metricsauc","title":"tfma.post_export_metrics.auc","text":"<pre><code>tfma.post_export_metrics.auc(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/auc_plots/","title":"Auc plots","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/auc_plots/#tfmapost_export_metricsauc_plots","title":"tfma.post_export_metrics.auc_plots","text":"<pre><code>tfma.post_export_metrics.auc_plots(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/calibration_plot_and_prediction_histogram/","title":"Calibration plot and prediction histogram","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/calibration_plot_and_prediction_histogram/#tfmapost_export_metricscalibration_plot_and_prediction_histogram","title":"tfma.post_export_metrics.calibration_plot_and_prediction_histogram","text":"<pre><code>tfma.post_export_metrics.calibration_plot_and_prediction_histogram(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/confusion_matrix_at_thresholds/","title":"Confusion matrix at thresholds","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/confusion_matrix_at_thresholds/#tfmapost_export_metricsconfusion_matrix_at_thresholds","title":"tfma.post_export_metrics.confusion_matrix_at_thresholds","text":"<pre><code>tfma.post_export_metrics.confusion_matrix_at_thresholds(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/example_count/","title":"Example count","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/example_count/#tfmapost_export_metricsexample_count","title":"tfma.post_export_metrics.example_count","text":"<pre><code>tfma.post_export_metrics.example_count(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/example_weight/","title":"Example weight","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/example_weight/#tfmapost_export_metricsexample_weight","title":"tfma.post_export_metrics.example_weight","text":"<pre><code>tfma.post_export_metrics.example_weight(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/mean_absolute_error/","title":"Mean absolute error","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/mean_absolute_error/#tfmapost_export_metricsmean_absolute_error","title":"tfma.post_export_metrics.mean_absolute_error","text":"<pre><code>tfma.post_export_metrics.mean_absolute_error(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/precision_at_k/","title":"Precision at k","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/precision_at_k/#tfmapost_export_metricsprecision_at_k","title":"tfma.post_export_metrics.precision_at_k","text":"<pre><code>tfma.post_export_metrics.precision_at_k(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/recall_at_k/","title":"Recall at k","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/recall_at_k/#tfmapost_export_metricsrecall_at_k","title":"tfma.post_export_metrics.recall_at_k","text":"<pre><code>tfma.post_export_metrics.recall_at_k(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/squared_pearson_correlation/","title":"Squared pearson correlation","text":""},{"location":"api_docs/python/deprecated/tfma/post_export_metrics/squared_pearson_correlation/#tfmapost_export_metricssquared_pearson_correlation","title":"tfma.post_export_metrics.squared_pearson_correlation","text":"<pre><code>tfma.post_export_metrics.squared_pearson_correlation(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Defined in <code>post_export_metrics/post_export_metrics.py</code>.</p> <p>This is the function that the user calls.</p>"},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/","title":"EvalSharedModel","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#tfmatypesevalsharedmodel","title":"tfma.types.EvalSharedModel","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#class-evalsharedmodel","title":"Class <code>EvalSharedModel</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#aliases","title":"Aliases:","text":"<ul> <li>Class <code>tfma.EvalSharedModel</code></li> <li>Class <code>tfma.types.EvalSharedModel</code></li> </ul> <p>Defined in <code>types.py</code>.</p> <p>Shared model used during extraction and evaluation.</p>"},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#attributes","title":"Attributes:","text":"<ul> <li><code>model_path</code>: Path to EvalSavedModel (containing the saved_model.pb     file).</li> <li><code>add_metrics_callbacks</code>: Optional list of callbacks for adding     additional metrics to the graph. The names of the metrics added by the     callbacks should not conflict with existing metrics. See below for more     details about what each callback should do. The callbacks are only used     during evaluation.</li> <li><code>include_default_metrics</code>: True to include the default metrics that     are part of the saved model graph during evaluation.</li> <li><code>example_weight_key</code>: Deprecated.</li> <li><code>additional_fetches</code>: Prefixes of additional tensors stored in     signature_def.inputs that should be fetched at prediction time. The     \"features\" and \"labels\" tensors are handled automatically and should not be     included in this list.</li> <li><code>shared_handle</code>: Optional handle to a shared.Shared object for     sharing the in-memory model within / between stages.</li> <li><code>construct_fn</code>: A callable which creates a construct function to set     up the tensorflow graph. Callable takes a beam.metrics distribution to track     graph construction time.</li> </ul> <p>More details on add_metrics_callbacks:</p> <p>Each add_metrics_callback should have the following prototype: def add_metrics_callback(features_dict, predictions_dict, labels_dict):</p> <p>Note that features_dict, predictions_dict and labels_dict are not necessarily dictionaries - they might also be Tensors, depending on what the model's eval_input_receiver_fn returns.</p> <p>It should create and return a metric_ops dictionary, such that metric_ops['metric_name'] = (value_op, update_op), just as in the Trainer.</p> <p>Short example:</p> <p>def add_metrics_callback(features_dict, predictions_dict, labels): metrics_ops = {} metric_ops['mean_label'] = tf.metrics.mean(labels) metric_ops['mean_probability'] = tf.metrics.mean(tf.slice( predictions_dict['probabilities'], [0, 1], [2, 1])) return metric_ops</p>"},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    cls,\n    model_path=None,\n    add_metrics_callbacks=None,\n    include_default_metrics=True,\n    example_weight_key=None,\n    additional_fetches=None,\n    shared_handle=None,\n    construct_fn=None\n)\n</code></pre> <p>Create new instance of EvalSharedModel(model_path, add_metrics_callbacks, include_default_metrics, example_weight_key, additional_fetches, shared_handle, construct_fn)</p>"},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#model_path","title":"<code>model_path</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#add_metrics_callbacks","title":"<code>add_metrics_callbacks</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#include_default_metrics","title":"<code>include_default_metrics</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#example_weight_key","title":"<code>example_weight_key</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#additional_fetches","title":"<code>additional_fetches</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#shared_handle","title":"<code>shared_handle</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/EvalSharedModel/#construct_fn","title":"<code>construct_fn</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/","title":"FeaturesPredictionsLabels","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#tfmatypesfeaturespredictionslabels","title":"tfma.types.FeaturesPredictionsLabels","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#class-featurespredictionslabels","title":"Class <code>FeaturesPredictionsLabels</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#aliases","title":"Aliases:","text":"<ul> <li>Class <code>tfma.FeaturesPredictionsLabels</code></li> <li>Class <code>tfma.types.FeaturesPredictionsLabels</code></li> </ul> <p>Defined in <code>types.py</code>.</p> <p>FeaturesPredictionsLabels(input_ref, features, predictions, labels)</p>"},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    _cls,\n    input_ref,\n    features,\n    predictions,\n    labels\n)\n</code></pre> <p>Create new instance of FeaturesPredictionsLabels(input_ref, features, predictions, labels)</p>"},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#input_ref","title":"<code>input_ref</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#features","title":"<code>features</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#predictions","title":"<code>predictions</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/FeaturesPredictionsLabels/#labels","title":"<code>labels</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/","title":"MaterializedColumn","text":""},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/#tfmatypesmaterializedcolumn","title":"tfma.types.MaterializedColumn","text":""},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/#class-materializedcolumn","title":"Class <code>MaterializedColumn</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/#aliases","title":"Aliases:","text":"<ul> <li>Class <code>tfma.MaterializedColumn</code></li> <li>Class <code>tfma.types.MaterializedColumn</code></li> </ul> <p>Defined in <code>types.py</code>.</p> <p>MaterializedColumn(name, value)</p>"},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    _cls,\n    name,\n    value\n)\n</code></pre> <p>Create new instance of MaterializedColumn(name, value)</p>"},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/#name","title":"<code>name</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/MaterializedColumn/#value","title":"<code>value</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/","title":"ValueWithConfidenceInterval","text":""},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#tfmatypesvaluewithconfidenceinterval","title":"tfma.types.ValueWithConfidenceInterval","text":""},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#class-valuewithconfidenceinterval","title":"Class <code>ValueWithConfidenceInterval</code>","text":"<p>Defined in <code>types.py</code>.</p> <p>Represents a value with mean, upper, and lower bound.</p>"},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    cls,\n    value,\n    lower_bound=None,\n    upper_bound=None,\n    unsampled_value=None\n)\n</code></pre> <p>Create new instance of ValueWithConfidenceInterval(value, lower_bound, upper_bound, unsampled_value)</p>"},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#value","title":"<code>value</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#lower_bound","title":"<code>lower_bound</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#upper_bound","title":"<code>upper_bound</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/ValueWithConfidenceInterval/#unsampled_value","title":"<code>unsampled_value</code>","text":""},{"location":"api_docs/python/deprecated/tfma/types/is_tensor/","title":"Is tensor","text":""},{"location":"api_docs/python/deprecated/tfma/types/is_tensor/#tfmatypesis_tensor","title":"tfma.types.is_tensor","text":"<pre><code>tfma.types.is_tensor(obj)\n</code></pre> <p>Defined in <code>types.py</code>.</p>"},{"location":"api_docs/python/deprecated/tfma/validators/Validator/","title":"Validator","text":""},{"location":"api_docs/python/deprecated/tfma/validators/Validator/#tfmavalidatorsvalidator","title":"tfma.validators.Validator","text":""},{"location":"api_docs/python/deprecated/tfma/validators/Validator/#class-validator","title":"Class <code>Validator</code>","text":"<p>Defined in <code>validators/validator.py</code>.</p> <p>Validator(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/validators/Validator/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    _cls,\n    stage_name,\n    ptransform\n)\n</code></pre> <p>Create new instance of Validator(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/validators/Validator/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/validators/Validator/#stage_name","title":"<code>stage_name</code>","text":""},{"location":"api_docs/python/deprecated/tfma/validators/Validator/#ptransform","title":"<code>ptransform</code>","text":""},{"location":"api_docs/python/deprecated/tfma/view/render_plot/","title":"Render plot","text":""},{"location":"api_docs/python/deprecated/tfma/view/render_plot/#tfmaviewrender_plot","title":"tfma.view.render_plot","text":"<pre><code>tfma.view.render_plot(\n    result,\n    slicing_spec=None,\n    label=None\n)\n</code></pre> <p>Defined in <code>view/widget_view.py</code>.</p> <p>Renders the plot view as widget.</p>"},{"location":"api_docs/python/deprecated/tfma/view/render_plot/#args","title":"Args:","text":"<ul> <li><code>result</code>: An tfma.EvalResult.</li> <li><code>slicing_spec</code>: The slicing spec to identify the slice. Show overall     if unset.</li> <li><code>label</code>: A partial label used to match a set of plots in the results.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/view/render_plot/#returns","title":"Returns:","text":"<p>A PlotViewer object if in Jupyter notebook; None if in Colab.</p>"},{"location":"api_docs/python/deprecated/tfma/view/render_slicing_metrics/","title":"Render slicing metrics","text":""},{"location":"api_docs/python/deprecated/tfma/view/render_slicing_metrics/#tfmaviewrender_slicing_metrics","title":"tfma.view.render_slicing_metrics","text":"<pre><code>tfma.view.render_slicing_metrics(\n    result,\n    slicing_column=None,\n    slicing_spec=None,\n    weighted_example_column=None\n)\n</code></pre> <p>Defined in <code>view/widget_view.py</code>.</p> <p>Renders the slicing metrics view as widget.</p>"},{"location":"api_docs/python/deprecated/tfma/view/render_slicing_metrics/#args","title":"Args:","text":"<ul> <li><code>result</code>: An tfma.EvalResult.</li> <li><code>slicing_column</code>: The column to slice on.</li> <li><code>slicing_spec</code>: The slicing spec to filter results. If neither column     nor spec is set, show overall.</li> <li><code>weighted_example_column</code>: Override for the weighted example column.     This can be used when different weights are applied in different aprts of     the model (eg: multi-head).</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/view/render_slicing_metrics/#returns","title":"Returns:","text":"<p>A SlicingMetricsViewer object if in Jupyter notebook; None if in Colab.</p>"},{"location":"api_docs/python/deprecated/tfma/view/render_time_series/","title":"Render time series","text":""},{"location":"api_docs/python/deprecated/tfma/view/render_time_series/#tfmaviewrender_time_series","title":"tfma.view.render_time_series","text":"<pre><code>tfma.view.render_time_series(\n    results,\n    slice_spec=None,\n    display_full_path=False\n)\n</code></pre> <p>Defined in <code>view/widget_view.py</code>.</p> <p>Renders the time series view as widget.</p>"},{"location":"api_docs/python/deprecated/tfma/view/render_time_series/#args","title":"Args:","text":"<ul> <li><code>results</code>: An tfma.EvalResults.</li> <li><code>slice_spec</code>: A slicing spec determining the slice to show time     series on. Show overall if not set.</li> <li><code>display_full_path</code>: Whether to display the full path to model / data     in the visualization or just show file name.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/view/render_time_series/#returns","title":"Returns:","text":"<p>A TimeSeriesViewer object if in Jupyter notebook; None if in Colab.</p>"},{"location":"api_docs/python/deprecated/tfma/writers/Write/","title":"Write","text":""},{"location":"api_docs/python/deprecated/tfma/writers/Write/#tfmawriterswrite","title":"tfma.writers.Write","text":"<pre><code>tfma.writers.Write(\n    *args,\n    **kwargs\n)\n</code></pre> <p>Writes given Evaluation or Validation data using given writer PTransform.</p>"},{"location":"api_docs/python/deprecated/tfma/writers/Write/#args","title":"Args:","text":"<ul> <li><code>evaluation_or_validation</code>: Evaluation or Validation data.</li> <li><code>key</code>: Key for Evaluation or Validation output to write. It is valid     for the key to not exist in the dict (in which case the write is a no-op).</li> <li><code>ptransform</code>: PTransform to use for writing.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/writers/Write/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If Evaluation or Validation is empty. The key does not     need to exist in the Evaluation or Validation, but the dict must not be     empty.</li> </ul>"},{"location":"api_docs/python/deprecated/tfma/writers/Write/#returns","title":"Returns:","text":"<p>beam.pvalue.PDone.</p>"},{"location":"api_docs/python/deprecated/tfma/writers/Writer/","title":"Writer","text":""},{"location":"api_docs/python/deprecated/tfma/writers/Writer/#tfmawriterswriter","title":"tfma.writers.Writer","text":""},{"location":"api_docs/python/deprecated/tfma/writers/Writer/#class-writer","title":"Class <code>Writer</code>","text":"<p>Defined in <code>writers/writer.py</code>.</p> <p>Writer(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/writers/Writer/#__new__","title":"<code>__new__</code>","text":"<pre><code>@staticmethod\n__new__(\n    _cls,\n    stage_name,\n    ptransform\n)\n</code></pre> <p>Create new instance of Writer(stage_name, ptransform)</p>"},{"location":"api_docs/python/deprecated/tfma/writers/Writer/#properties","title":"Properties","text":""},{"location":"api_docs/python/deprecated/tfma/writers/Writer/#stage_name","title":"<code>stage_name</code>","text":""},{"location":"api_docs/python/deprecated/tfma/writers/Writer/#ptransform","title":"<code>ptransform</code>","text":""}]}